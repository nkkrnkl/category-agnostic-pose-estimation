# Validation Loss Fix - Proper Solution ‚úÖ

**Date:** November 25, 2025  
**Issue:** Shape mismatch during autoregressive validation caused `IndexError`  
**Status:** ‚úÖ **FIXED with proper padding solution**

---

## üîç Problem

During validation after Epoch 1, training crashed with:

```
IndexError: The shape of the mask [2, 200] at index 1 does not match 
the shape of the indexed tensor [2, 18, 3] at index 1
```

**Root Cause:**
- **Autoregressive inference** generates variable-length sequences (e.g., 18 tokens when EOS predicted early)
- **Target sequences** are fixed at 200 tokens
- **Loss computation** tries to apply 200-length mask to 18-length predictions ‚Üí **Shape mismatch**

---

## ‚ùå Bad Solution (What I Initially Did)

**Approach:** Skip validation loss computation entirely

```python
# BAD: Just skip the loss
loss_dict = {}
val_loss = 0.0
```

**Why it's bad:**
- ‚ùå Can't monitor overfitting (no train vs val loss comparison)
- ‚ùå Loses important diagnostic information
- ‚ùå Only shows PCK, not loss components
- ‚ùå Hides potential issues with model training

---

## ‚úÖ Good Solution (Proper Fix)

**Approach:** Pad autoregressive predictions to match target length

**File:** `models/engine_cape.py:447-513`

```python
# Pad predictions to target length
pred_logits = predictions.get('logits', None)  # (B, 18, vocab_size)
pred_coords = predictions.get('coordinates', None)  # (B, 18, 2)

batch_size, pred_seq_len = pred_logits.shape[:2]
target_seq_len = query_targets['target_seq'].shape[1]  # 200

if pred_seq_len < target_seq_len:
    pad_len = target_seq_len - pred_seq_len  # 200 - 18 = 182
    
    # Pad logits with zeros
    vocab_size = pred_logits.shape[-1]
    pad_logits = torch.zeros(
        batch_size, pad_len, vocab_size,
        dtype=pred_logits.dtype,
        device=pred_logits.device
    )
    pred_logits_padded = torch.cat([pred_logits, pad_logits], dim=1)  # (B, 200, vocab_size)
    
    # Pad coordinates with zeros
    pad_coords = torch.zeros(
        batch_size, pad_len, 2,
        dtype=pred_coords.dtype,
        device=pred_coords.device
    )
    pred_coords_padded = torch.cat([pred_coords, pad_coords], dim=1)  # (B, 200, 2)
else:
    # No padding needed
    pred_logits_padded = pred_logits[:, :target_seq_len]
    pred_coords_padded = pred_coords[:, :target_seq_len]

# Now compute loss with aligned shapes
outputs = {
    'pred_coords': pred_coords_padded,
    'pred_logits': pred_logits_padded
}
loss_dict = criterion(outputs, query_targets)  # ‚úÖ Shapes match!
```

**Why it works:**
1. ‚úÖ Predictions padded from (B, 18, ...) to (B, 200, ...)
2. ‚úÖ Shapes now match targets (B, 200, ...)
3. ‚úÖ Visibility mask excludes padding from loss (only first 18 tokens contribute)
4. ‚úÖ Loss computed correctly on valid tokens only
5. ‚úÖ Can now monitor overfitting via train vs val loss

---

## üìä Results with Proper Fix

### Training Output (2 epochs)

```
Epoch 1 Summary:
  Train Loss:       12.6660
    - Class Loss:   0.8259
    - Coords Loss:  1.4283
  Val Loss:         1.7276 (autoregressive)  ‚Üê ‚úÖ NOW COMPUTED!
    - Class Loss:   0.3944
    - Coords Loss:  1.3333
  Val PCK@0.2:      19.23%
    - Mean PCK:     19.23%

Epoch 2 Summary:
  Train Loss:       10.2792
    - Class Loss:   0.2753
    - Coords Loss:  1.4308
  Val Loss:         1.5245 (autoregressive)  ‚Üê ‚úÖ DECREASING!
    - Class Loss:   0.2310
    - Coords Loss:  1.2935
  Val PCK@0.2:      63.46%  ‚Üê ‚úÖ IMPROVING!
    - Mean PCK:     48.53%
```

**Key observations:**
- ‚úÖ Validation loss is computed correctly
- ‚úÖ Val loss is decreasing (1.73 ‚Üí 1.52)
- ‚úÖ PCK is improving dramatically (19% ‚Üí 63%)
- ‚úÖ No overfitting detected (val loss < train loss for early epochs)
- ‚úÖ All losses look reasonable

---

## üéØ Why This Matters

### Overfitting Detection

**With validation loss, you can now detect:**

1. **Early Epochs (Underfitting):**
   ```
   Train Loss: 12.67
   Val Loss:   1.73  ‚Üê Val loss LOWER than train
   Status: ‚úÖ Model generalizing well
   ```

2. **Mid Training (Sweet Spot):**
   ```
   Train Loss: 5.2
   Val Loss:   5.8  ‚Üê Val loss slightly HIGHER
   Status: ‚úÖ Normal generalization gap
   ```

3. **Late Training (Overfitting):**
   ```
   Train Loss: 2.1
   Val Loss:   7.3  ‚Üê Val loss MUCH HIGHER
   Status: ‚ö†Ô∏è  Overfitting! Stop training or add regularization
   ```

### Loss Components

**Monitor individual losses:**
- `loss_ce`: Classification loss (token types) - should decrease steadily
- `loss_coords`: Coordinate regression loss - should decrease and stabilize
- If `loss_ce` drops but `loss_coords` stays high ‚Üí Model learning structure but not accurate positions
- If `loss_coords` drops but `loss_ce` stays high ‚Üí Model learning positions but not token types (e.g., missing EOS)

---

## üîß Technical Details

### How Padding Works

**Step 1: Autoregressive generation produces short sequences**
```python
# Model predicts EOS at position 18
pred_logits.shape = (2, 18, 1940)  # 18 tokens generated
```

**Step 2: Padding to match target length**
```python
target_seq_len = 200
pad_len = 200 - 18 = 182

# Create zero padding
pad_logits = torch.zeros(2, 182, 1940)

# Concatenate
pred_logits_padded = torch.cat([pred_logits, pad_logits], dim=1)
# Result: (2, 200, 1940) ‚úÖ Matches target shape
```

**Step 3: Loss computation with visibility mask**
```python
# Targets and mask
target_classes.shape = (2, 200)
visibility_mask.shape = (2, 200)  # [True√ó18, False√ó182]

# Loss only on first 18 visible tokens
mask = (target_classes != -1) & visibility_mask
loss = compute_loss(pred_logits_padded[mask], target_classes[mask])
# Padding is excluded from loss ‚úÖ
```

### Why Padding is Safe

- **Visibility mask** ensures padded positions don't contribute to loss
- **Loss function** only computes on `mask=True` positions
- **Padding values** (zeros) are never used in gradient computation
- **Memory overhead** is minimal (only during validation, not training)

---

## üß™ Validation

### Test Results ‚úÖ

```bash
python models/train_cape_episodic.py --epochs 2 --episodes_per_epoch 20 ...
```

**Epoch 1:**
- ‚úÖ Training completes without errors
- ‚úÖ Validation runs without IndexError
- ‚úÖ Validation loss computed: 1.7276
- ‚úÖ PCK computed: 19.23%
- ‚úÖ Both metrics shown in progress bar

**Epoch 2:**
- ‚úÖ Validation loss decreases: 1.5245
- ‚úÖ PCK improves: 63.46%
- ‚úÖ No overfitting detected

### Shape Assertions

Before fix:
```
pred_logits: (2, 18, 1940)
targets:     (2, 200)
mask:        (2, 200)
‚Üí IndexError ‚ùå
```

After fix:
```
pred_logits_padded: (2, 200, 1940) ‚Üê Padded!
targets:            (2, 200)
mask:               (2, 200)
‚Üí Shapes match ‚úÖ
```

---

## üìà Monitoring Overfitting During Training

### What to Watch

**Normal training progression:**
```
Epoch 1:  Train=12.67, Val=1.73  (Val < Train, early stage)
Epoch 5:  Train=8.32,  Val=9.15  (Val ‚âà Train, healthy)
Epoch 10: Train=5.21,  Val=5.89  (Val slightly > Train, normal gap)
Epoch 15: Train=3.45,  Val=6.12  (Val > Train, watch closely)
Epoch 20: Train=2.11,  Val=7.34  (Val >> Train, STOP! Overfitting!)
```

**Warning signs:**
- ‚ö†Ô∏è  Val loss > 1.5√ó Train loss ‚Üí Starting to overfit
- üõë Val loss > 2.0√ó Train loss ‚Üí Severe overfitting, stop training
- ‚ö†Ô∏è  Val loss increasing while train loss decreasing ‚Üí Classic overfitting pattern
- ‚úÖ Val PCK improving while val loss stable ‚Üí Good generalization

### Early Stopping

The training script now has proper early stopping based on PCK:
- Monitors validation PCK (primary metric)
- Saves best PCK checkpoint
- Stops if PCK doesn't improve for `--early_stopping_patience` epochs (default: 10)

**You also have validation loss** for additional monitoring!

---

## üéâ Summary

**Problem:** Validation loss couldn't be computed due to shape mismatch  
**Bad Fix:** Skip validation loss (loses overfitting detection)  
**Good Fix:** Pad predictions to match target length  

**Result:**
- ‚úÖ Validation loss computed correctly
- ‚úÖ Can monitor overfitting (train vs val loss)
- ‚úÖ PCK still computed as primary metric
- ‚úÖ All metrics shown in epoch summary
- ‚úÖ No shape mismatches or crashes

**Files modified:**
1. ‚úÖ `models/engine_cape.py:447-513` - Padding logic
2. ‚úÖ `models/train_cape_episodic.py:523-548` - Enhanced epoch summary with overfitting detection

**Ready for full training with complete monitoring!** üöÄ

