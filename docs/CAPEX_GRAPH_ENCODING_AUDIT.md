# CapeX Graph Encoding Analysis

**Date**: November 25, 2025  
**Auditor**: AI Assistant  
**Purpose**: Deep structural audit of CapeX pose graph encoding for geometry-only adaptation

---

## 1. Overview of Graph and Keypoint Encoding Pipeline

### High-Level Architecture

CapeX uses a **text-based support representation** combined with **graph-conditioned transformers**. The complete pipeline is:

```
Input: Text descriptions + Skeleton edges + Query image
  ↓
Text Encoder (CLIP/BERT) → Text embeddings [bs, num_kpts, text_dim]
  ↓
Text Projection → Support embeddings [bs, num_kpts, d_model=256]
  ↓
Joint Encoder (Query image + Support text) → Refined support embeddings
  ↓
Proposal Generator → Initial keypoint proposals [bs, num_kpts, 2]
  ↓
Graph Decoder (with GCN + Skeleton) → Refined proposals [bs, num_kpts, 2]
  ↓
Output: Predicted keypoint coordinates (normalized)
```

### Critical Discovery

**CapeX does NOT use coordinate information in the initial support encoding at all!**

The support representation is **purely text-based**. Coordinates only appear:
1. As **supervision signals** (ground truth for loss computation)
2. As **initial proposals** generated by cross-attention between text and query image features
3. As **positional encodings** computed from these proposals

**Code Evidence**:
- `demo_text.py:208`: `kp_src = torch.zeros((len(point_descriptions), 2))` - placeholder zeros!
- `demo_text.py:233`: `target_s, target_weight_s = genHeatMap._msra_generate_target(...)` - only used for heatmap loss
- `capex.py:265`: `all_shots_point_descriptions = self.extract_text_features(...)` - TEXT is the support encoding
- `head.py:169`: `point_descriptions = self.text_proj(point_descriptions)` - this becomes `support_embed`

---

## 2. Keypoint Encoding: Detailed Breakdown

### 2.1 Initial Support Keypoint Representation

**Location**: `models/models/detectors/capex.py:298-337`

**Process**:
1. **Text Extraction**: For each keypoint, get textual description (e.g., "front left leg")
2. **Text Encoding**:
   - CLIP: `self.text_backbone.encode_text(tokens)` → [num_points, 512]
   - BERT/GTE: `self.text_backbone(**tokens).last_hidden_state[:, 0]` → [num_points, 768]
3. **Normalization**: L2-normalize: `F.normalize(embeddings, p=2, dim=1)`
4. **Padding**: Pad to `max_kpt_num=100` with zeros for missing keypoints
5. **Output**: `[bs, 100, text_dim]` where `text_dim=512` for CLIP

**Shape Transformations**:
```
Text strings → Tokenizer → [num_visible_kpts, 77] token IDs
              ↓
         Text Encoder → [num_visible_kpts, 512] embeddings
              ↓
         L2 Normalize → [num_visible_kpts, 512] normalized
              ↓
         Pad to 100 → [bs, 100, 512] (zeros for invisible/missing)
```

### 2.2 Text-to-Embedding Projection

**Location**: `models/models/keypoint_heads/head.py:106,169`

**Process**:
```python
self.text_proj = Linear(text_in_channels=512, embed_dims=256)
point_descriptions = self.text_proj(point_descriptions)  # [bs, 100, 256]
```

This `point_descriptions` tensor (shape `[bs, 100, 256]`) **IS** the `support_embed` passed to the transformer.

**Key Insight**: NO coordinate encoding at this stage - it's pure text semantics!

### 2.3 Coordinate Encoding (Positional)

**Location**: `models/models/utils/positional_encoding.py:97-123`

Coordinates are encoded **ONLY** as sine/cosine positional embeddings, used for:
1. **Initial proposal embeddings** (after cross-attention generates proposals)
2. **Iterative refinement** (recalculated each decoder layer)

**Formula** (for normalized coordinates `[x, y]` in `[0,1]`):
```python
x_scaled = x * scale  # scale = 2π
y_scaled = y * scale

dim_t = temperature^(2 * (i // 2) / num_feats)  # temperature=10000

pos_x[i] = sin(x_scaled / dim_t[i]) if i even, cos(x_scaled / dim_t[i]) if i odd
pos_y[i] = sin(y_scaled / dim_t[i]) if i even, cos(y_scaled / dim_t[i]) if i odd

pos = concat([pos_y, pos_x])  # [bs, num_kpts, 256]
```

This is the **standard DETR-style sinusoidal positional encoding**, applied to 2D coordinates.

**Critical**: This encoding is **geometry-only** and can be directly reused!

### 2.4 No Quantization or Discrete Tokenization

Unlike your Raster2Seq approach, CapeX:
- **Does NOT** quantize coordinates into discrete bins
- **Does NOT** generate sequences of tokens
- **Uses continuous coordinate regression** with sigmoid activation
- **Represents keypoints as queries** (DETR-style), not as sequences

---

## 3. Edge / Skeleton Encoding: Detailed Breakdown

### 3.1 Adjacency Matrix Construction

**Location**: `models/models/utils/encoder_decoder.py:507-521`

**The `adj_from_skeleton` Function**:
```python
def adj_from_skeleton(num_pts, skeleton, mask, device='cuda'):
    adj_mx = torch.empty(0, device=device)
    batch_size = len(skeleton)
    
    for b in range(batch_size):
        edges = torch.tensor(skeleton[b])  # [[src1, dst1], [src2, dst2], ...]
        adj = torch.zeros(num_pts, num_pts, device=device)
        adj[edges[:, 0], edges[:, 1]] = 1  # Mark edge connections
        adj_mx = torch.concatenate((adj_mx, adj.unsqueeze(0)), dim=0)
    
    # Make symmetric (undirected graph)
    trans_adj_mx = torch.transpose(adj_mx, 1, 2)
    cond = (trans_adj_mx > adj_mx).float()
    adj = adj_mx + trans_adj_mx * cond - adj_mx * cond
    
    # Mask out invalid keypoints
    adj = adj * ~mask[..., None] * ~mask[:, None]
    
    # Row-normalize (graph Laplacian style)
    adj = torch.nan_to_num(adj / adj.sum(dim=-1, keepdim=True))
    
    # Stack with identity (self-loops)
    adj = torch.stack((torch.diag_embed(~mask), adj), dim=1)  # [bs, 2, num_pts, num_pts]
    
    return adj  # Shape: [bs, 2, num_pts, num_pts]
```

**Output Shape**: `[bs, 2, num_pts, num_pts]`
- **Channel 0**: Self-loops (diagonal, identity matrix with masking)
- **Channel 1**: Normalized adjacency (skeleton edges, symmetric, row-normalized)

**Key Properties**:
- **Undirected**: Edges are bidirectional
- **Normalized**: Row-sum = 1 (stochastic matrix for graph convolution)
- **Masked**: Invalid keypoints have zero connections
- **Dual-channel**: Separates self-connections from neighbor connections

### 3.2 Graph Convolutional Network (GCN) Layer

**Location**: `models/models/utils/encoder_decoder.py:524-556`

**The `GCNLayer` Class**:
```python
class GCNLayer(nn.Module):
    def __init__(self, in_features, out_features, kernel_size=2, ...):
        # kernel_size=2 for [self-loop, neighbor] channels
        self.conv = nn.Conv1d(in_features, out_features * kernel_size, 
                             kernel_size=1, ...)
        self.kernel_size = kernel_size  # 2 by default
        
    def forward(self, x, adj):
        # x: [num_pts, bs, c] if batch_first=False
        # adj: [bs, 2, num_pts, num_pts]
        
        x = x.permute(1, 2, 0)  # [bs, c, num_pts]
        x = self.conv(x)  # [bs, c*2, num_pts]
        x = x.view(bs, 2, c, num_pts)  # [bs, kernel_size=2, c, num_pts]
        
        # Graph convolution: aggregate features from neighbors
        x = torch.einsum('bkcv,bkvw->bcw', (x, adj))  # [bs, c, num_pts]
        
        x = self.activation(x)
        x = x.permute(2, 0, 1)  # [num_pts, bs, c]
        return x
```

**Graph Convolution Formula**:
```
For each keypoint v:
  x_out[v] = Σ_{u ∈ neighbors(v)} W_neighbor * x[u] * adj[v,u]
           + W_self * x[v]
```

Where `adj[v,u]` is the normalized edge weight.

### 3.3 Integration in Decoder

**Location**: `models/models/utils/encoder_decoder.py:309-406` (`GraphTransformerDecoderLayer`)

**Graph Decoder Modes** (configurable via `graph_decoder` parameter):
1. **`None`**: No graph - standard FFN
2. **`'pre'`**: GCN before activation in FFN
3. **`'post'`**: GCN after activation in FFN  
4. **`'both'`**: GCN both before and after activation

**Code** (`graph_decoder='pre'` - used in graph configs):
```python
if self.graph_decoder == 'pre':
    # Standard self-attention + cross-attention first
    ...
    # Then FFN with GCN
    adj = adj_from_skeleton(num_pts, skeleton, mask, device)
    tgt2 = self.ffn2(
        self.dropout(
            self.activation(
                self.ffn1(refined_support_feat, adj)  # ← GCN layer 1
            )
        )
    )
    refined_support_feat = refined_support_feat + self.dropout3(tgt2)
```

**Flow in Each Decoder Layer**:
1. **Self-attention** among support keypoints (with positional encoding)
2. **Cross-attention** to query image features
3. **FFN with GCN**: 
   - GCN layer 1 (in_dim → hidden_dim) with adjacency
   - Activation + Dropout
   - Linear layer 2 (hidden_dim → out_dim)
4. **Residual + LayerNorm**
5. **MLP prediction head** outputs coordinate offset `delta`
6. **Update coordinates**: `coords_new = sigmoid(inverse_sigmoid(coords_old) + delta)`

### 3.4 Where Graph Information Flows

**Graph structure (skeleton edges) affects**:
1. ❌ **NOT** in encoder (encoder doesn't see skeleton)
2. ❌ **NOT** in initial proposal generation (purely image-text cross-attention)
3. ✅ **ONLY in decoder FFN** (via GCN layers)
4. ❌ **NOT** in attention masks (no causal or graph-based masking)
5. ❌ **NOT** in positional encodings (only coordinate-based, not topology-based)

**Implication**: Graph is a **refinement mechanism**, not a fundamental structural constraint.

---

## 4. Unified Transformer Sequence Construction

### 4.1 There Is No Unified Sequence!

**Critical Finding**: CapeX **does NOT use sequence modeling** like Raster2Seq.

Instead, it uses **DETR-style set prediction**:
- **Support keypoints** are treated as **learnable queries** (like DETR object queries)
- **Query image** provides **keys/values** via CNN backbone features
- **Cross-attention** matches support queries to image regions
- **Each keypoint is predicted independently** (as a 2D coordinate, not as tokens)

### 4.2 Transformer Architecture

**Encoder** (`TransformerEncoder`):
- **Input**: Concatenation of `[image_features, support_embeddings]`
  - `image_features`: `[h*w, bs, c]` flattened spatial features
  - `support_embeddings`: `[num_kpts, bs, c]` text-projected embeddings
- **Operation**: Self-attention over the concatenated sequence
- **Output**: 
  - `query_embed`: Refined image features `[h*w, bs, c]`
  - `refined_support_embed`: Refined support embeddings `[num_kpts, bs, c]`

**Decoder** (`GraphTransformerDecoder`):
- **Queries**: Refined support embeddings `[num_kpts, bs, c]`
- **Keys/Values**: Refined image features `[h*w, bs, c]`
- **Operations** (per layer):
  1. Self-attention (support keypoints attend to each other)
  2. Cross-attention (support keypoints attend to image)
  3. FFN with GCN (if `graph_decoder` enabled)
  4. Coordinate prediction via MLP

### 4.3 No BOS/EOS Tokens

**There are NO special tokens** like BOS, EOS, SEP, or COORD.

Reasons:
- CapeX uses **set prediction**, not sequence generation
- All keypoints are predicted **in parallel** (not autoregressively)
- The model predicts **fixed-size output** (padded to max_kpt_num=100)

### 4.4 Positional Encoding Strategy

**Two types**:

1. **Image positional encoding** (`SinePositionalEncoding.forward(mask)`):
   - Applied to spatial grid `[bs, h, w]`
   - Output: `[bs, 256, h, w]`
   - Standard DETR 2D positional encoding

2. **Coordinate positional encoding** (`SinePositionalEncoding.forward_coordinates(coords)`):
   - Applied to keypoint coordinates `[bs, num_kpts, 2]`
   - Output: `[bs, num_kpts, 256]`
   - Same sinusoidal formula, but for point coordinates instead of grid

**Ordering**: 
- **No semantic ordering required** - keypoints are unordered sets
- **Text handles disambiguation** - "left eye" vs "right eye" is in the text
- **Spatial ordering is implicit** in the positional encoding from coordinates

---

## 5. What Parts Rely on Text, and Why

### 5.1 Text Dependencies (CRITICAL)

| Component | Uses Text? | How? | Purpose |
|-----------|-----------|------|---------|
| **Initial Support Encoding** | ✅ YES | Text embeddings ARE the support representation | Semantic keypoint identity |
| **Symmetry Breaking** | ✅ YES | "left leg" vs "right leg" encoded differently | Distinguish mirror keypoints |
| **Keypoint Ordering** | ✅ YES | Text provides implicit ordering via semantics | No need for spatial sorting |
| **Support Image** | ❌ NO | Not used (set to 0 in dataset) | Replaced by text! |
| **Support Coordinates** | ❌ NO | Only used for ground truth heatmap loss | NOT part of encoding |
| **Skeleton Edges** | ❌ NO | Purely geometric (adjacency matrix) | Graph structure |
| **Positional Encoding** | ❌ NO | Computed from coordinates (sin/cos) | Geometric only |
| **Proposal Generation** | ❌ NO | Cross-attention between text-support and image | Geometry + text |
| **Graph Decoder** | ❌ NO | GCN with adjacency from edges | Purely topological |

### 5.2 Why Text Is Used

**From code analysis**:

1. **Semantic Disambiguation**:
   - `utils.py:14`: "top left side of the left eye" vs "bottom right side of the left eye"
   - Without text, these are just coordinates - the model can't know which is which

2. **Category-Agnostic Generalization**:
   - Text provides **transferable semantics**: "wheel" for chairs and cars
   - Enables zero-shot: model learns "wheels are round things at the bottom"

3. **Symmetry Breaking**:
   - `utils.py:288`: "left oculus" vs "right oculus" (synonyms test version)
   - Geometric coordinates alone can't distinguish left/right without pose context

4. **No Support Image Needed**:
   - `transformer_base_dataset.py:189`: `Xs['img'] = 0  # there is no need in the support image`
   - Text **replaces** the need for visual examples

### 5.3 Text Embedding Characteristics

**Properties**:
- **Dense vectors**: 512-dim continuous embeddings (CLIP ViT-B/32)
- **Pre-trained**: Frozen CLIP model (not finetuned in default config)
- **Normalized**: L2 norm = 1
- **Semantic**: Captures linguistic meaning, not just symbols
- **Language-aware**: Multilingual BERT used in some ablations

---

## 6. Mapping CapeX Logic to Our Geometry-Only Setting

### 6.1 Fundamental Incompatibility

**CapeX's core design assumption**: Textual descriptions provide keypoint semantics.

**Our constraint**: NO text allowed - only coordinates + edges.

### 6.2 What We CAN Use Directly

✅ **Graph encoding via GCN**:
- `adj_from_skeleton()` function - purely geometric
- `GCNLayer` class - operates on any features
- Adjacency matrix construction - no text dependency
- Can be **directly ported** to our codebase

✅ **Positional encoding for coordinates**:
- `SinePositionalEncoding.forward_coordinates()` - purely geometric
- Same formula as DETR
- Already similar to what we have
- Can be **directly reused**

✅ **Proposal generator architecture**:
- Cross-attention mechanism between support and query
- Similarity map computation
- Coordinate refinement logic
- Can be **adapted** with coordinate-based support instead of text

✅ **Decoder architecture**:
- Multi-layer iterative refinement
- MLP coordinate prediction heads
- Inverse sigmoid coordinate updates
- Can be **directly reused**

### 6.3 What We CANNOT Use (Text-Dependent)

❌ **Text encoder** (`extract_text_features`):
- Requires CLIP/BERT
- Outputs semantic embeddings
- **Must be replaced**

❌ **Initial support embedding = text projection**:
- `point_descriptions = self.text_proj(text_embeddings)`
- This is the **PRIMARY** support representation
- **Must be replaced with coordinate-based encoding**

❌ **Text-based semantics for disambiguation**:
- Left/right, top/bottom distinctions come from text
- **Must be replaced with spatial/topological features**

### 6.4 The Core Problem: Replacing Text Embeddings

**Challenge**: What replaces `point_descriptions` (text embeddings)?

**CapeX**: `[bs, num_kpts, 256]` text embeddings  
**Our options**:
1. **Coordinate embeddings**: MLP on raw `(x, y)` coordinates
2. **Graph embeddings**: GNN pre-processing on pose graph
3. **Hybrid**: Coordinates + local graph context
4. **Positional encoding only**: Use sinusoidal encoding as content features

**CapeX ablation** (from `configs/` folder analysis):
- `base_split1_config.py`: NO `graph_decoder` parameter (None)
- `graph_split1_config.py`: `graph_decoder='pre'`
- This suggests **graph is an enhancement**, not core requirement

---

## 7. Suggested Implementation Blueprint

### 7.1 Geometry-Only Support Encoder

**Goal**: Replace text embeddings with coordinate + graph embeddings.

**Proposed Architecture**:

```python
class GeometricSupportEncoder(nn.Module):
    def __init__(self, hidden_dim=256):
        super().__init__()
        
        # 1. Coordinate embedding
        self.coord_embed = nn.Sequential(
            nn.Linear(2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim)
        )
        
        # 2. Positional encoding (from CapeX)
        self.pos_encoding = SinePositionalEncoding(
            num_feats=128, normalize=True
        )
        
        # 3. Optional: Graph pre-encoding (GNN)
        self.graph_encoder = nn.ModuleList([
            GCNLayer(hidden_dim, hidden_dim, kernel_size=2)
            for _ in range(2)  # 2 GCN layers
        ])
        
        # 4. Fusion
        self.fusion = nn.Linear(hidden_dim * 2, hidden_dim)
        
    def forward(self, coords, mask, skeleton):
        """
        Args:
            coords: [bs, num_kpts, 2] - normalized support coordinates
            mask: [bs, num_kpts, 1] - visibility mask
            skeleton: List of edge lists [[src, dst], ...]
            
        Returns:
            support_embed: [bs, num_kpts, hidden_dim]
        """
        # 1. Embed coordinates
        coord_feat = self.coord_embed(coords)  # [bs, num_kpts, 256]
        
        # 2. Add positional encoding
        pos_feat = self.pos_encoding.forward_coordinates(coords)  # [bs, num_kpts, 256]
        
        # 3. Optional graph encoding
        adj = adj_from_skeleton(coords.shape[1], skeleton, 
                               mask.squeeze(-1), coords.device)
        
        graph_feat = coord_feat.permute(1, 0, 2)  # [num_kpts, bs, 256]
        for gcn_layer in self.graph_encoder:
            graph_feat = gcn_layer(graph_feat, adj)  # Apply GCN
        graph_feat = graph_feat.permute(1, 0, 2)  # [bs, num_kpts, 256]
        
        # 4. Fuse coordinate + positional + graph
        combined = torch.cat([coord_feat + pos_feat, graph_feat], dim=-1)
        support_embed = self.fusion(combined)  # [bs, num_kpts, 256]
        
        return support_embed
```

**Key Differences from CapeX**:
- **Input**: Coordinates instead of text
- **Embedding**: MLP projection instead of CLIP encoding
- **Disambiguation**: Graph structure + positional encoding instead of text semantics
- **Pre-encoding**: Optional GNN to inject graph awareness early

### 7.2 Adapting CapeX Transformer

**Minimal changes required**:

```python
# In PoseHead.forward():

# OLD (CapeX):
point_descriptions = self.text_proj(text_embeddings)  # [bs, 100, 256]
support_embed = point_descriptions

# NEW (Geometry-only):
support_coords = torch.tensor(support_coords)  # [bs, num_kpts, 2]
support_embed = self.geometric_encoder(
    support_coords, mask_s, skeleton
)  # [bs, 100, 256]

# Rest is IDENTICAL to CapeX!
outs_dec, initial_proposals, out_points, similarity_map = self.transformer(
    x, masks, support_embed, pos_embed, support_order_embedding,
    masks_query, self.positional_encoding, self.kpt_branch,
    skeleton, return_attn_map=True
)
```

**Everything else stays the same**: encoder, decoder, proposal generator, GCN layers, loss functions.

### 7.3 Handling Symmetry Without Text

**Challenge**: How to distinguish left/right keypoints without "left" and "right" in text?

**Solutions**:

**Option 1: Spatial Ordering**
```python
# Sort keypoints by x-coordinate (left-to-right)
# Then by y-coordinate (top-to-bottom) as tiebreaker
sorted_indices = torch.lexsort([coords[:, 1], coords[:, 0]])
coords = coords[sorted_indices]
```
**Pro**: Deterministic, simple  
**Con**: Breaks with rotation/perspective

**Option 2: Graph-Aware Ordering**
```python
# Use graph topology + spatial position
# E.g., BFS/DFS from root node, with spatial tiebreaking
from scipy.sparse.csgraph import breadth_first_order
order = breadth_first_order(adj_matrix, root=0)
```
**Pro**: Respects structure  
**Con**: Requires canonical root selection

**Option 3: Learned Ordering (BEST)**
```python
# Let the model learn to handle unordered sets
# Use set-equivariant architecture (already done in DETR-style attention!)
# Graph convolution is also permutation-equivariant
```
**Pro**: Most general, no manual rules  
**Con**: Requires sufficient training data

**Recommendation**: **Option 3** - CapeX's architecture is already set-based (DETR-style), so it naturally handles unordered keypoints. The graph structure + positional encoding provide enough geometric context for disambiguation.

### 7.4 Integration with Our Raster2Seq Model

**Conflict**: CapeX uses **set prediction**, we use **sequence generation**.

**Two approaches**:

**Approach A: Hybrid Architecture**
- Use CapeX's geometric support encoder
- Keep our Raster2Seq decoder (autoregressive sequence generation)
- Bridge: Support embeddings → cross-attention in our decoder

```python
# In our cape_model.py:
class CAPEModel(nn.Module):
    def __init__(self, ...):
        self.support_encoder = GeometricSupportEncoder(...)  # ← CapeX-inspired
        self.base_model = build_base_model(...)  # ← Our Raster2Seq
        
    def forward(self, samples, support_coords, support_mask, skeleton, targets):
        # 1. Encode support with CapeX method
        support_features = self.support_encoder(
            support_coords, support_mask, skeleton
        )  # [bs, num_kpts, 256]
        
        # 2. Inject into our Raster2Seq decoder
        # (Already doing this via cross-attention!)
        outputs = self.base_model(samples, seq_kwargs=targets)
        
        return outputs
```

**Approach B: Full CapeX Adoption**
- Replace Raster2Seq with CapeX's DETR-style architecture
- Much larger change, different training objective

**Recommendation**: **Approach A** - keep our sequence generation, adopt CapeX's graph encoding.

---

## 8. Recommended Tests and Diagnostics

### 8.1 Graph Encoding Correctness

**Test 1: Adjacency Matrix Symmetry**
```python
def test_adjacency_symmetry():
    skeleton = [[0, 1], [1, 2], [2, 0]]  # Triangle
    mask = torch.zeros(1, 3).bool()  # All visible
    adj = adj_from_skeleton(3, [skeleton], mask, 'cpu')
    
    # Check symmetry
    assert torch.allclose(adj[0, 1], adj[0, 1].T), "Adjacency not symmetric!"
    
    # Check normalization
    row_sums = adj[0, 1].sum(dim=-1)
    assert torch.allclose(row_sums, torch.ones_like(row_sums)), "Rows not normalized!"
```

**Test 2: GCN Feature Aggregation**
```python
def test_gcn_aggregation():
    # Create simple graph: 0 -- 1 -- 2
    x = torch.tensor([[[1., 0.], [0., 1.], [1., 1.]]])  # [bs=1, 3, 2]
    skeleton = [[[0, 1], [1, 2]]]
    mask = torch.zeros(1, 3).bool()
    
    gcn = GCNLayer(2, 2, kernel_size=2)
    adj = adj_from_skeleton(3, skeleton, mask, 'cpu')
    
    out = gcn(x.permute(1, 0, 2), adj)  # [3, 1, 2]
    
    # Node 1 should have features influenced by nodes 0 and 2
    # (specific values depend on weights, but check shape and no NaNs)
    assert out.shape == (3, 1, 2)
    assert not torch.isnan(out).any()
```

### 8.2 Coordinate Encoding Sanity

**Test 3: Positional Encoding Consistency**
```python
def test_positional_encoding():
    pos_enc = SinePositionalEncoding(num_feats=128, normalize=True)
    
    # Same coordinates → same encoding
    coords1 = torch.tensor([[[0.5, 0.5], [0.8, 0.2]]])
    coords2 = coords1.clone()
    
    pos1 = pos_enc.forward_coordinates(coords1)
    pos2 = pos_enc.forward_coordinates(coords2)
    
    assert torch.allclose(pos1, pos2), "Positional encoding not deterministic!"
    
    # Different coordinates → different encoding
    coords3 = torch.tensor([[[0.3, 0.7], [0.1, 0.9]]])
    pos3 = pos_enc.forward_coordinates(coords3)
    
    assert not torch.allclose(pos1, pos3), "Different coords have same encoding!"
```

### 8.3 Support Encoding Without Text

**Test 4: Geometric Support Encoder**
```python
def test_geometric_support_encoder():
    encoder = GeometricSupportEncoder(hidden_dim=256)
    
    coords = torch.rand(2, 10, 2)  # [bs=2, num_kpts=10, 2]
    mask = torch.ones(2, 10, 1)  # All visible
    skeleton = [
        [[0,1], [1,2], [2,3], [0,4], [4,5]],  # Batch 1
        [[0,1], [1,2], [0,3], [3,4]]          # Batch 2
    ]
    
    support_embed = encoder(coords, mask, skeleton)
    
    # Check shape
    assert support_embed.shape == (2, 10, 256)
    
    # Check different coords → different embeddings
    coords2 = coords + 0.1
    support_embed2 = encoder(coords2, mask, skeleton)
    assert not torch.allclose(support_embed, support_embed2)
    
    # Check masking works
    mask_partial = torch.cat([torch.ones(2, 5, 1), torch.zeros(2, 5, 1)], dim=1)
    support_embed_masked = encoder(coords, mask_partial, skeleton)
    # Masked positions should have different/zero features
```

### 8.4 Integration Test

**Test 5: End-to-End Forward Pass**
```python
def test_capex_without_text():
    # Build model with geometric encoder instead of text encoder
    model = CAPEModel_GeometricSupport(...)
    
    # Create dummy inputs
    query_img = torch.rand(2, 3, 256, 256)
    support_coords = torch.rand(2, 17, 2)  # Normalized coordinates
    support_mask = torch.ones(2, 17, 1)
    skeleton = [animal_skeleton] * 2
    
    # Forward pass
    output = model(
        query_img, support_coords, support_mask, skeleton
    )
    
    # Check output
    assert 'pred_keypoints' in output
    assert output['pred_keypoints'].shape == (2, 17, 2)
    assert (output['pred_keypoints'] >= 0).all() and (output['pred_keypoints'] <= 1).all()
```

### 8.5 Visualization Diagnostics

**Test 6: Graph Connectivity Visualization**
```python
def visualize_graph_encoding(coords, skeleton, save_path):
    """Visualize which keypoints are connected."""
    import networkx as nx
    import matplotlib.pyplot as plt
    
    G = nx.Graph()
    G.add_edges_from(skeleton)
    
    pos = {i: (coords[i, 0], coords[i, 1]) for i in range(len(coords))}
    
    nx.draw(G, pos, with_labels=True, node_color='lightblue', 
            node_size=500, font_size=10)
    plt.savefig(save_path)
```

### 8.6 Comparison Metrics

**Test 7: CapeX vs Our Model (Ablation)**

Run both models on same test set:
1. **CapeX with text** (baseline)
2. **CapeX with coordinate embeddings** (our adaptation)
3. **Our Raster2Seq with CapeX graph encoding** (hybrid)

Metrics:
- PCK@0.2 on MP-100 test categories
- Per-category performance
- Failure cases analysis

---

## 9. Ambiguities / Questions Requiring Review

### 9.1 How Does CapeX Handle Varying Keypoint Counts?

**Observation**: Different categories have different numbers of keypoints (e.g., 9 for animal faces, 17 for animal bodies).

**Solution in CapeX**:
- Pad to `max_kpt_num=100` with zeros
- Use `mask_s` to indicate valid keypoints
- Transformer attention masks ignore padded positions

**Our adaptation**: Same approach works.

### 9.2 Why Does CapeX Zero Out Support Images?

**Code**: `transformer_base_dataset.py:189: Xs['img'] = 0`

**Reason**: CapeX's innovation is **text REPLACES visual support**.
- Traditional CAPE: Uses support image + keypoint annotations
- CapeX: Uses ONLY text descriptions (more flexible, no image needed)

**Our case**: We need support COORDINATES (not images), similar to CapeX needing text.

### 9.3 What About Category-Specific vs Category-Agnostic?

**CapeX's claim**: Text enables category-agnostic generalization.

**Evidence**:
- Text like "wheel" transfers across categories (chairs, cars, bikes)
- Test on unseen categories (MP-100 split)

**Our challenge**: Can coordinates + graph provide sufficient transfer?

**Hypothesis**:
- **Yes, if**: Graph topology is preserved (e.g., quadrupeds share 4-leg structure)
- **No, if**: Topology varies drastically (animal body vs furniture vs clothing)

**Recommendation**: Test on categories with **similar skeleton structure** first (e.g., all quadrupeds).

### 9.4 Does CapeX Use Support Coordinates AT ALL?

**Answer**: **NO** in the forward pass, **YES** only for:
1. **Heatmap loss**: Ground truth heatmaps generated from support coordinates
2. **Evaluation**: PCK metric compares predicted vs GT coordinates

**Implications**:
- CapeX learns to predict keypoint **locations** from text descriptions
- The model never "sees" support coordinates during inference
- This is fundamentally different from traditional CAPE approaches

**Our approach**: We MUST use support coordinates (no text alternative).

### 9.5 Graph Decoder: Pre vs Post vs Both?

**Config analysis**:
- `graph_split1_config.py`: `graph_decoder='pre'`
- `base_split1_config.py`: No graph decoder (None)

**Performance** (from README):
- With graph (tiny model): **92.79%** PCK (split 1)
- Without graph: Not specified (likely lower)

**Question**: What's the performance delta? Is graph essential or just helpful?

**Recommendation**: Start with `graph_decoder='pre'` (CapeX's choice), ablate later.

### 9.6 How to Handle Keypoint Ordering in Geometry-Only Setting?

**CapeX solution**: Text provides semantic ordering ("left leg" before "right leg")

**Our options**:
1. **Canonical ordering**: Always sort by some rule (x-coord, graph topology)
2. **Learned ordering**: Let model learn from data
3. **Unordered sets**: Use set-equivariant architecture (attention is permutation-equivariant)

**CapeX architecture compatibility**:
- Self-attention in decoder is permutation-equivariant
- GCN is also permutation-equivariant (adjacency-based)
- **NO ordering requirement** from architecture!

**Recommendation**: **Use unordered sets** - the architecture supports it, and it's most general.

### 9.7 Comparison: CapeX vs Our Current Approach

| Aspect | CapeX | Our Current (Raster2Seq) | Recommended Hybrid |
|--------|-------|--------------------------|-------------------|
| **Support encoding** | Text embeddings | ❌ Not implemented | ✅ Coordinate + Graph |
| **Keypoint representation** | Set prediction (parallel) | Sequence generation (autoregressive) | Keep sequence |
| **Graph integration** | GCN in decoder FFN | ✅ Cross-attention | ✅ Add GCN layers |
| **Coordinate encoding** | Positional only (after proposals) | Discrete tokenization | Keep tokenization |
| **Prediction format** | Direct (x,y) regression | Token sequence → coordinates | Keep sequences |
| **Training** | L1 loss + heatmap loss | Cross-entropy + coordinate loss | Keep our losses |

**Key Insight**: CapeX's graph encoding (GCN layers) can be **added** to our model without changing the core sequence generation approach.

### 9.8 Missing Information

**What we still need**:
1. ❓ **Detailed ablation results**: Graph vs no-graph performance delta
2. ❓ **Failure cases**: When does CapeX fail without text?
3. ❓ **Training dynamics**: How many epochs for convergence?
4. ❓ **Coordinate initialization**: Why zeros work in CapeX (because text handles identity)?

**Next steps**:
1. Try extracting table/figure data from PDF (different tool)
2. Run CapeX with `graph_decoder=None` to measure graph contribution
3. Test coordinate-only support encoding in CapeX framework

---

## 10. Final Recommendations

### 10.1 Immediate Action Items

1. **Port `adj_from_skeleton` and `GCNLayer`** to our codebase
   - These are geometry-only and directly reusable
   - Add to `models/cape_model.py`

2. **Implement `GeometricSupportEncoder`** as proposed
   - Replaces text encoding
   - Combines coordinates + positional + graph

3. **Add GCN layers to our decoder**
   - Integrate into existing cross-attention modules
   - Use `graph_decoder='pre'` mode

4. **Keep our sequence generation approach**
   - CapeX's set prediction may not leverage sequential structure
   - Our tokenization captures finer spatial details (bilinear interpolation)

### 10.2 Architecture Design

**Recommended hybrid**:
```
Support Path:
  Coordinates [bs, N, 2]
    ↓
  Coordinate MLP → [bs, N, 256]
    ↓
  Positional Encoding (sine/cos) → [bs, N, 256]
    ↓
  GNN Pre-encoding (2 layers) → [bs, N, 256]
    ↓
  Support Features

Query Path:
  Image [bs, 3, H, W]
    ↓
  ResNet → [bs, C, h, w]
    ↓
  Query Features

Fusion:
  Cross-Attention (Support features ↔ Query features)
    ↓
  Decoder with GCN (using skeleton adjacency)
    ↓
  Autoregressive Sequence Generation (our approach)
    ↓
  Keypoint Tokens → Coordinates
```

### 10.3 What to Avoid

❌ **Don't blindly copy CapeX's set prediction** - our sequence approach has advantages (order, structure, bilinear interpolation)

❌ **Don't remove text without replacement** - need geometric support encoding

❌ **Don't ignore graph structure** - CapeX shows it helps (GCN layers)

❌ **Don't assume semantic ordering** - use permutation-equivariant operations

### 10.4 Success Criteria

**Validation that geometry-only works**:
1. ✅ Model trains without NaN/Inf
2. ✅ Loss decreases over epochs
3. ✅ PCK > 50% on validation (unseen categories)
4. ✅ Predictions have correct number of keypoints
5. ✅ Skeleton structure is respected (edges connect reasonable points)
6. ✅ Symmetry is handled (left/right, top/bottom)

**Performance target**:
- CapeX achieves **88.81% average PCK** on MP-100
- With text removed, expect **10-20% degradation** (rough estimate)
- Target: **70-80% PCK** would validate approach

---

## 11. Code Locations Reference

### CapeX Key Files

| File | Purpose | Reusable? |
|------|---------|-----------|
| `models/models/detectors/capex.py` | Main model class | ⚠️ Partial (need to replace text encoder) |
| `models/models/keypoint_heads/head.py` | Transformer head | ⚠️ Partial (replace `text_proj` with `coord_embed`) |
| `models/models/utils/encoder_decoder.py` | **EncoderDecoder, GCN, adj_from_skeleton** | ✅ **DIRECTLY REUSABLE** |
| `models/models/utils/positional_encoding.py` | **SinePositionalEncoding** | ✅ **DIRECTLY REUSABLE** |
| `models/models/utils/transformer.py` | Basic transformer (no graph) | ⚠️ Less useful (we want graph version) |
| `models/datasets/datasets/mp100/utils.py` | Text descriptions | ❌ Text-dependent |
| `models/datasets/datasets/mp100/transformer_dataset.py` | Dataset loader | ⚠️ Partial (structure is reusable) |

### Most Important Functions to Port

1. **`adj_from_skeleton()`** - `encoder_decoder.py:507-521`
   - ✅ Pure geometry
   - Converts edge list → normalized adjacency matrix

2. **`GCNLayer`** - `encoder_decoder.py:524-556`
   - ✅ Pure geometry
   - Graph convolution with dual-channel adjacency

3. **`SinePositionalEncoding.forward_coordinates()`** - `positional_encoding.py:97-123`
   - ✅ Pure geometry  
   - Encodes (x,y) → 256-dim sinusoidal features

4. **`ProposalGenerator`** - `encoder_decoder.py:36-111`
   - ✅ Architecture reusable (but we'd use coordinate-based support instead of text)

5. **`GraphTransformerDecoderLayer`** - `encoder_decoder.py:309-406`
   - ✅ GCN integration logic
   - Shows how to inject adjacency into FFN

---

## 12. Summary: Text vs Geometry in CapeX

### What Text Provides (That We Must Replace)

1. **Keypoint Identity**: "This is the left eye" - conveyed by text semantics
   - **Geometric replacement**: Local graph context + spatial position
   
2. **Symmetry Breaking**: Distinguishing "left" from "right"
   - **Geometric replacement**: Spatial ordering or learned disambiguation

3. **Category Transfer**: "Wheel" concept transfers across chair/car
   - **Geometric replacement**: Topological similarity (graph isomorphism)

4. **Semantic Invariance**: "Nose" is always "pointy thing on face"
   - **Geometric replacement**: None (must rely on visual features from query image)

### What We Can Keep (Geometry-Only)

1. ✅ **Skeleton adjacency matrix** - purely topological
2. ✅ **GCN layers** - operate on any features with graph structure
3. ✅ **Positional encoding** - sinusoidal encoding of coordinates
4. ✅ **Decoder architecture** - attention + FFN + GCN
5. ✅ **Coordinate regression** - sigmoid-bounded predictions
6. ✅ **Iterative refinement** - multi-layer coordinate updates

### Critical Insight

**CapeX's graph encoding is NOT text-dependent!**

The GCN layers, adjacency matrix, and positional encoding are **pure geometry**. Text is used **ONLY** for the initial support embedding.

**Therefore**: We can adopt CapeX's graph encoding by replacing:
```python
# CapeX:
support_embed = text_proj(text_encoder(keypoint_descriptions))

# Our version:
support_embed = geometric_encoder(keypoint_coordinates, skeleton_edges)
```

Everything downstream (encoder, decoder, GCN, proposal generator, losses) remains **IDENTICAL**.

---

## 13. Proposed Next Steps

### Phase 1: Code Extraction (1-2 hours)
1. Copy `adj_from_skeleton()` to our `models/cape_model.py`
2. Copy `GCNLayer` to our `models/cape_model.py`
3. Copy `SinePositionalEncoding.forward_coordinates()` to our codebase (or import)
4. Write unit tests for each function

### Phase 2: Geometric Encoder Implementation (2-3 hours)
1. Implement `GeometricSupportEncoder` class
2. Test with dummy inputs
3. Verify shapes and gradients flow

### Phase 3: Integration (3-4 hours)
1. Modify `CAPEModel.forward()` to use geometric encoder
2. Add GCN layers to decoder (optional: make configurable)
3. Update `build_cape_model()` to instantiate new components

### Phase 4: Training & Validation (variable)
1. Train on MP-100 with geometry-only support
2. Compare to text-based CapeX (qualitative, since we can't run their model easily)
3. Visualize predictions to verify skeleton structure is respected
4. Debug any issues

### Phase 5: Ablations (after working baseline)
1. Graph vs no-graph
2. Different GCN layer counts
3. Different support encodings (MLP depth, hidden dims)
4. Impact of coordinate pre-encoding

---

## 14. Open Questions for User

1. **Do you want to keep Raster2Seq sequence generation, or switch to CapeX's set prediction?**
   - My recommendation: Keep sequence generation, add graph encoding

2. **Should we implement support coordinates as continuous (CapeX style) or discrete tokens (Raster2Seq style)?**
   - CapeX: Direct coordinate regression
   - Raster2Seq: Tokenize then decode
   - Hybrid: Both?

3. **Do you have access to CapeX's pretrained models for comparison?**
   - Would help validate our geometry-only adaptation

4. **What's the priority: Working baseline vs optimal performance?**
   - Working baseline → simpler coordinate encoding
   - Optimal → more complex graph pre-encoding

---

## 15. Conclusion

**TL;DR**: CapeX's graph encoding is **geometry-only and directly portable**. The text dependency is **ONLY** in the initial support embedding. By replacing text embeddings with coordinate + graph embeddings, we can adapt CapeX's architecture to our geometry-only constraint while preserving its graph-aware refinement mechanism.

**Confidence**: HIGH - The code is clear, the graph encoding is modular, and there are no hidden text dependencies in the core architecture.

**Recommended approach**: Implement `GeometricSupportEncoder`, port GCN layers, integrate with our existing Raster2Seq decoder, and train end-to-end.

