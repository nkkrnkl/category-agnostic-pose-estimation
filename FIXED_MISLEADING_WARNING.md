# Fixed: Misleading "OLD CHECKPOINT DETECTED" Warning

## Issue

When evaluating checkpoint `checkpoint_e017_lr1e-04_bs2_acc4_qpe2.pth` (trained with the **NEW** fixed code), the evaluation script displayed:

```
‚ö†Ô∏è  CRITICAL WARNING: OLD CHECKPOINT DETECTED
   pred_coords shape: torch.Size([2, 10, 2])
   gt_coords shape: torch.Size([2, 200, 2])

This checkpoint was trained WITHOUT a tokenizer and only generates
10 keypoint(s) before predicting <eos>.
```

This was **completely wrong**. The checkpoint was trained with the NEW code and works correctly.

---

## Root Cause

The warning logic in `scripts/eval_cape_checkpoint.py` (line 340) was:

```python
if pred_coords.dim() == 3 and pred_coords.shape[1] < gt_coords.shape[1]:
    # Trigger "OLD CHECKPOINT" warning
```

This compared:
- `pred_coords.shape[1]` = **10** (model generated 10 tokens, then predicted EOS)
- `gt_coords.shape[1]` = **200** (GT sequence padded to max length)

**The comparison was meaningless!** 

### Why the Logic Was Wrong

The warning assumed:
- ‚ùå **Old checkpoints:** generate very few tokens (< 200)
- ‚ùå **New checkpoints:** generate exactly 200 tokens

But in reality:
- ‚úÖ **Old checkpoints (buggy):** generated exactly **1** keypoint before EOS (due to EOS token exclusion bug)
- ‚úÖ **New checkpoints (fixed):** generate the **correct number** of tokens before EOS (e.g., 10-40 depending on category)

### What's Actually Correct Behavior

For a category with, say, **5 keypoints**:
- Correct token sequence: 5 keypoints √ó 2 coords = **10 COORD tokens**
- Plus SEP and EOS tokens
- Total: **~12-15 tokens** (much less than 200!)

The model generating 10 tokens for a 5-keypoint category is **EXACTLY** what we want!

---

## Evidence This Was a False Alarm

Checkpoint `checkpoint_e017_lr1e-04_bs2_acc4_qpe2.pth` shows:

### Training Metrics (Epoch 17)
```
Train Loss (final layer):   0.6451
  - Class Loss:             0.0224
  - Coords Loss:            0.6228

Val Loss (final layer):     1.2261
  - Class Loss:             0.2215
  - Coords Loss:            1.0046

Val PCK@0.2:                31.16%
  - Mean PCK (categories):  34.23%
```

This is a **properly trained checkpoint** with reasonable PCK on unseen validation categories.

### Evaluation Results (20 episodes)
```
Overall PCK@0.2: 20.00% (88/440 keypoints)
Mean PCK across categories: 23.18%

Prediction Statistics:
  Avg keypoints generated: 15.3
  Avg keypoints expected: 15.3  ‚Üê Perfect match!
  Avg sequence length: 16.2
  Range: 9-32 keypoints
```

The model is:
- ‚úÖ Generating the **correct number** of keypoints per category
- ‚úÖ Predicting EOS at the **right time**
- ‚úÖ Stopping generation **as expected**

---

## The Fix

### Before (Incorrect)
```python
# ====================================================================
# Handle shape mismatch for old checkpoints
# ====================================================================
if pred_coords.shape[1] < gt_coords.shape[1]:
    print("‚ö†Ô∏è  CRITICAL WARNING: OLD CHECKPOINT DETECTED")
    print(f"This checkpoint only generates {pred_coords.shape[1]} keypoint(s)")
    # ... misleading error message ...
```

### After (Correct)
```python
# ====================================================================
# Pad autoregressive predictions to GT sequence length if needed
# ====================================================================
# During autoregressive inference, the model generates variable-length
# sequences (e.g., 10-40 tokens depending on category) and stops when
# it predicts EOS. The GT sequences are padded to a fixed length (200).
# We need to pad predictions to match GT length for loss computation.
# ====================================================================
if pred_coords.shape[1] < gt_coords.shape[1]:
    # Silently pad predictions to match GT length
    # (The visibility mask ensures padding doesn't affect metrics)
    ...
```

**Key changes:**
1. **Removed** the misleading "OLD CHECKPOINT" warning
2. **Added** accurate documentation explaining why padding is needed
3. **Added** informational message at evaluation start explaining variable-length sequences are expected

---

## Verification

Run evaluation without the misleading warning:

```bash
python scripts/eval_cape_checkpoint.py \
    --checkpoint outputs/test_geometric/checkpoint_e017_lr1e-04_bs2_acc4_qpe2.pth \
    --num-visualizations 10 \
    --split val \
    --num-episodes 20 \
    --output-dir outputs/cape_eval_e017
```

Expected output:
```
================================================================================
RUNNING EVALUATION
================================================================================

‚ÑπÔ∏è  Note: The model uses autoregressive inference and generates variable-length
   sequences based on category keypoint count. Sequences are padded internally
   for batch processing. This is expected and correct behavior.

Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:15<00:00,  1.31it/s]

Overall PCK@0.2: ~20-30% (depends on random episodes sampled)
```

**No misleading warnings!** ‚úÖ

---

## Summary

- **Problem:** Evaluation script incorrectly flagged new checkpoints as "old/broken"
- **Cause:** Compared predicted sequence length to padded GT length (meaningless comparison)
- **Fix:** Removed misleading warning, added accurate documentation
- **Result:** Clean evaluation output, correct behavior confirmed

Your checkpoint `checkpoint_e017_lr1e-04_bs2_acc4_qpe2.pth` is **working correctly** and was trained with the **new fixed code**! üéâ

