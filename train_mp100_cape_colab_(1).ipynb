{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yr9fJlVbsd3"
      },
      "source": [
        "# MP-100 CAPE Training on Google Colab\n",
        "\n",
        "This notebook trains Category-Agnostic Pose Estimation (CAPE) on the MP-100 dataset using Google Colab's GPU.\n",
        "\n",
        "## Setup Instructions\n",
        "1. Enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU (T4 or better)\n",
        "2. Run all cells in order\n",
        "3. The notebook will:\n",
        "   - Clone code from GitHub\n",
        "   - Install dependencies\n",
        "   - Authenticate to GCP\n",
        "   - Mount GCS bucket with data\n",
        "   - Run training with \"tiny\" mode\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOiUB-lwbsd4"
      },
      "source": [
        "## 1. Check GPU Availability\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWDvxzLWbsd5",
        "outputId": "88aa1681-d771-41bd-bb0f-0fd056ef8d38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n",
            "GPU: NVIDIA A100-SXM4-40GB\n",
            "CUDA version: 12.6\n",
            "GPU Memory: 42.47 GB\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No GPU detected! Please enable GPU in Runtime > Change runtime type > GPU\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUmgObskbsd5",
        "outputId": "62e2e4a0-d7c8-40ff-d7dd-e4beda899d80"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For private repositories, you need to authenticate.\n",
            "Option 1: Enter your GitHub Personal Access Token\n",
            "  (Get one from: https://github.com/settings/tokens)\n",
            "Option 2: Press Enter to try without token (will fail if repo is private)\n",
            "\n",
            "Enter GitHub Personal Access Token (or press Enter to skip): ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "Cloning repository from https://github.com/nkkrnkl/category-agnostic-pose-estimation.git (branch: pavlos-topic-copy)...\n",
            "Cloning into '/content/category-agnostic-pose-estimation'...\n",
            "remote: Enumerating objects: 1015, done.\u001b[K\n",
            "remote: Counting objects: 100% (241/241), done.\u001b[K\n",
            "remote: Compressing objects: 100% (187/187), done.\u001b[K\n",
            "remote: Total 1015 (delta 75), reused 208 (delta 51), pack-reused 774 (from 2)\u001b[K\n",
            "Receiving objects: 100% (1015/1015), 73.26 MiB | 15.66 MiB/s, done.\n",
            "Resolving deltas: 100% (325/325), done.\n",
            "Updating files: 100% (215/215), done.\n",
            "remote: Enumerating objects: 13, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 7 (delta 6), reused 7 (delta 6), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects: 100% (7/7), 2.65 KiB | 135.00 KiB/s, done.\n",
            "From https://github.com/nkkrnkl/category-agnostic-pose-estimation\n",
            " * branch            pavlos-topic-copy -> FETCH_HEAD\n",
            "   f42f1f2..fe4dd58  pavlos-topic-copy -> origin/pavlos-topic-copy\n",
            "Updating f42f1f2..fe4dd58\n",
            "Fast-forward\n",
            " datasets/episodic_sampler.py     |  12 \u001b[32m++++\u001b[m\u001b[31m-\u001b[m\n",
            " models/train_cape_episodic.py    | 114 \u001b[32m++++++++++++++++++++++++++++++++++++\u001b[m\u001b[31m---\u001b[m\n",
            " train_mp100_cape_colab (1).ipynb |  16 \u001b[32m+++++\u001b[m\u001b[31m-\u001b[m\n",
            " 3 files changed, 132 insertions(+), 10 deletions(-)\n",
            "‚úÖ Repository cloned successfully to /content/category-agnostic-pose-estimation\n",
            "* \u001b[32mpavlos-topic-copy\u001b[m\n"
          ]
        }
      ],
      "source": [
        "# Clone repository from GitHub\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "REPO_URL = \"https://github.com/nkkrnkl/category-agnostic-pose-estimation.git\"\n",
        "BRANCH = \"pavlos-topic-copy\"\n",
        "PROJECT_ROOT = \"/content/category-agnostic-pose-estimation\"\n",
        "\n",
        "# Remove existing directory if it exists\n",
        "if os.path.exists(PROJECT_ROOT):\n",
        "    print(f\"Removing existing directory: {PROJECT_ROOT}\")\n",
        "    !rm -rf {PROJECT_ROOT}\n",
        "\n",
        "# For private repositories, you need to authenticate\n",
        "# Option 1: Use Personal Access Token (recommended)\n",
        "# Get token from: https://github.com/settings/tokens\n",
        "# Create a token with 'repo' scope\n",
        "print(\"For private repositories, you need to authenticate.\")\n",
        "print(\"Option 1: Enter your GitHub Personal Access Token\")\n",
        "print(\"  (Get one from: https://github.com/settings/tokens)\")\n",
        "print(\"Option 2: Press Enter to try without token (will fail if repo is private)\")\n",
        "print()\n",
        "\n",
        "GITHUB_TOKEN = getpass(\"Enter GitHub Personal Access Token (or press Enter to skip): \")\n",
        "\n",
        "if GITHUB_TOKEN.strip():\n",
        "    # Use token in URL\n",
        "    # Format: https://TOKEN@github.com/username/repo.git\n",
        "    AUTH_REPO_URL = REPO_URL.replace(\"https://github.com/\", f\"https://{GITHUB_TOKEN}@github.com/\")\n",
        "    print(f\"Cloning repository from {REPO_URL} (branch: {BRANCH})...\")\n",
        "    !git clone -b {BRANCH} {AUTH_REPO_URL} {PROJECT_ROOT}\n",
        "    !git pull origin {BRANCH}\n",
        "else:\n",
        "    # Try without token (will work if repo is public)\n",
        "    print(f\"Cloning repository from {REPO_URL} (branch: {BRANCH})...\")\n",
        "    !git clone -b {BRANCH} {REPO_URL} {PROJECT_ROOT}\n",
        "\n",
        "# Verify clone\n",
        "if os.path.exists(PROJECT_ROOT) and os.path.exists(os.path.join(PROJECT_ROOT, \".git\")):\n",
        "    print(f\"‚úÖ Repository cloned successfully to {PROJECT_ROOT}\")\n",
        "    !cd {PROJECT_ROOT} && git branch\n",
        "else:\n",
        "    print(\"‚ùå Failed to clone repository\")\n",
        "    print(\"\\nIf the repository is private, you need to:\")\n",
        "    print(\"1. Create a Personal Access Token at: https://github.com/settings/tokens\")\n",
        "    print(\"2. Select 'repo' scope\")\n",
        "    print(\"3. Run this cell again and paste the token when prompted\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWNIieJCgvy5",
        "outputId": "5a1ff4c4-5432-45af-a11f-a81b6ebec86e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pulling latest changes from branch pavlos-topic-copy...\n",
            "From https://github.com/nkkrnkl/category-agnostic-pose-estimation\n",
            " * branch            pavlos-topic-copy -> FETCH_HEAD\n",
            "Already up to date.\n",
            "‚úÖ Git pull complete!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ROOT = \"/content/category-agnostic-pose-estimation\"\n",
        "BRANCH = \"pavlos-topic-copy\"\n",
        "\n",
        "print(f\"Pulling latest changes from branch {BRANCH}...\")\n",
        "!cd {PROJECT_ROOT} && git pull origin {BRANCH}\n",
        "\n",
        "print(\"‚úÖ Git pull complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ULReV9hbsd5"
      },
      "source": [
        "## 3. Install Requirements\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9OT7CLHbsd6",
        "outputId": "33224ac5-93dd-4fe3-8691-e867d25fa3e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing additional dependencies (descartes, shapely, etc.)...\n",
            "‚úÖ Additional dependencies installed!\n"
          ]
        }
      ],
      "source": [
        "# Install additional dependencies needed for plot_utils and other utilities\n",
        "# (descartes, shapely, etc. - these are in requirements.txt but not requirements_cape.txt)\n",
        "print(\"Installing additional dependencies (descartes, shapely, etc.)...\")\n",
        "!pip install -q descartes shapely>=1.8.0\n",
        "print(\"‚úÖ Additional dependencies installed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eglnc1Xabsd6",
        "outputId": "756ca044-5503-4531-ecee-e40a11ed1a2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing requirements from requirements_cape.txt...\n",
            "\n",
            "Installing detectron2...\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "‚úÖ All dependencies installed!\n"
          ]
        }
      ],
      "source": [
        "# Install requirements\n",
        "import os\n",
        "\n",
        "PROJECT_ROOT = \"/content/category-agnostic-pose-estimation\"\n",
        "REQUIREMENTS_FILE = os.path.join(PROJECT_ROOT, \"requirements_cape.txt\")\n",
        "\n",
        "print(\"Installing requirements from requirements_cape.txt...\")\n",
        "!cd {PROJECT_ROOT} && pip install -q -r {REQUIREMENTS_FILE}\n",
        "\n",
        "# Install detectron2 for CUDA 11.8 (Colab typically has CUDA 11.8)\n",
        "print(\"\\nInstalling detectron2...\")\n",
        "!pip install -q 'git+https://github.com/facebookresearch/detectron2.git'\n",
        "\n",
        "print(\"‚úÖ All dependencies installed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtoTtjcibsd6"
      },
      "source": [
        "## 4. Authenticate to GCP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7l9TBNNbsd6",
        "outputId": "bc0a5d55-cc18-4738-8ada-b5d53b9c8a2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authenticating to GCP...\n",
            "\u001b[1;33mWARNING:\u001b[0m [nk699@cornell.edu] does not have permission to access projects instance [dl-category-agnostic-pose-est] (or it may not exist): The caller does not have permission. This command is authenticated as nk699@cornell.edu which is the active account specified by the [core/account] property\n",
            "Are you sure you wish to set property [core/project] to \n",
            "dl-category-agnostic-pose-est?\n",
            "\n",
            "Do you want to continue (Y/n)?  Y\n",
            "\n",
            "Updated property [core/project].\n",
            "‚úÖ Authenticated to GCP project: dl-category-agnostic-pose-est\n"
          ]
        }
      ],
      "source": [
        "# Authenticate to GCP\n",
        "from google.colab import auth\n",
        "\n",
        "print(\"Authenticating to GCP...\")\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Set GCP project\n",
        "GCP_PROJECT = \"dl-category-agnostic-pose-est\"\n",
        "!gcloud config set project {GCP_PROJECT}\n",
        "\n",
        "print(f\"‚úÖ Authenticated to GCP project: {GCP_PROJECT}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIaksWWwbsd6"
      },
      "source": [
        "## 5. Mount GCS Bucket\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jocm_WW5bsd6",
        "outputId": "7030af78-ce26-46eb-d6ce-841396a7a5cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing gcsfuse...\n",
            "deb http://packages.cloud.google.com/apt gcsfuse-jammy main\n",
            "gpg: cannot open '/dev/tty': No such device or address\n",
            "curl: (23) Failed writing body\n",
            "/usr/bin/gcsfuse\n",
            "‚úÖ gcsfuse installed\n",
            "Creating mount point: /content/category-agnostic-pose-estimation/Raster2Seq_internal-main/data\n",
            "Mounting gs://dl-category-agnostic-pose-mp100-data to /content/category-agnostic-pose-estimation/Raster2Seq_internal-main/data...\n",
            "This may take a moment...\n",
            "Running: gcsfuse --implicit-dirs dl-category-agnostic-pose-mp100-data /content/category-agnostic-pose-estimation/Raster2Seq_internal-main/data\n",
            "Waiting for mount to initialize...\n",
            "\n",
            "Checking mount status...\n",
            "Mount log:\n",
            ":4,\"RandomSeekThreshold\":3,\"StartBlocksPerHandle\":1},\"WorkloadInsight\":{\"ForwardMergeThresholdMb\":0,\"OutputFile\":\"\",\"Visualize\":false},\"Write\":{\"BlockSizeMb\":32,\"CreateEmptyFile\":false,\"EnableRapidAppends\":true,\"EnableStreamingWrites\":true,\"FinalizeFileForRapid\":false,\"GlobalMaxBlocks\":4,\"MaxBlocksPerFile\":1}}}\n",
            "{\"timestamp\":{\"seconds\":1764111100,\"nanos\":541151770},\"severity\":\"INFO\",\"message\":\"File system has been successfully mounted.\",\"mount-id\":\"dl-category-agnostic-pose-mp100-data-81d85c56\"}\n",
            "\n",
            "\n",
            "Verifying bucket access with gsutil...\n",
            "gs://dl-category-agnostic-pose-mp100-data/amur_tiger_body/\n",
            "gs://dl-category-agnostic-pose-mp100-data/annotations/\n",
            "gs://dl-category-agnostic-pose-mp100-data/antelope_body/\n",
            "gs://dl-category-agnostic-pose-mp100-data/arcticwolf_face/\n",
            "gs://dl-category-agnostic-pose-mp100-data/beaver_body/\n",
            "gs://dl-category-agnostic-pose-mp100-data/bed/\n",
            "gs://dl-category-agnostic-pose-mp100-data/bighornsheep_face/\n",
            "gs://dl-category-agnostic-pose-mp100-data/bison_body/\n",
            "gs://dl-category-agnostic-pose-mp100-data/blackbuck_face/\n",
            "gs://dl-category-agnostic-pose-mp100-data/bobcat_body/\n",
            "Exception ignored in: <_io.TextIOWrapper name='<stdout>' mode='w' encoding='utf-8'>\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\n",
            "Verifying mount at: /content/category-agnostic-pose-estimation/Raster2Seq_internal-main/data\n",
            "Path exists: True\n",
            "Is mounted: True\n",
            "‚úÖ GCS bucket mounted successfully!\n",
            "Mount point: /content/category-agnostic-pose-estimation/Raster2Seq_internal-main/data\n",
            "Found 99 items in bucket\n",
            "   - amur_tiger_body (directory)\n",
            "   - annotations (directory)\n",
            "   - antelope_body (directory)\n",
            "   - arcticwolf_face (directory)\n",
            "   - beaver_body (directory)\n",
            "   - bed (directory)\n",
            "   - bighornsheep_face (directory)\n",
            "   - bison_body (directory)\n",
            "   - blackbuck_face (directory)\n",
            "   - bobcat_body (directory)\n"
          ]
        }
      ],
      "source": [
        "# Mount GCS bucket using gcsfuse\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "PROJECT_ROOT = \"/content/category-agnostic-pose-estimation\"\n",
        "BUCKET_NAME = \"dl-category-agnostic-pose-mp100-data\"\n",
        "MOUNT_POINT = os.path.join(PROJECT_ROOT, \"Raster2Seq_internal-main\", \"data\")\n",
        "\n",
        "# Install gcsfuse from Google's official repository\n",
        "print(\"Installing gcsfuse...\")\n",
        "# Add Google's gcsfuse repository (updated method for newer Ubuntu versions)\n",
        "!export GCSFUSE_REPO=gcsfuse-`lsb_release -c -s` && \\\n",
        "echo \"deb http://packages.cloud.google.com/apt $GCSFUSE_REPO main\" | sudo tee /etc/apt/sources.list.d/gcsfuse.list && \\\n",
        "curl -fsSL https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo gpg --dearmor -o /usr/share/keyrings/cloud.google.gpg && \\\n",
        "echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt $GCSFUSE_REPO main\" | sudo tee /etc/apt/sources.list.d/gcsfuse.list && \\\n",
        "sudo apt-get update && \\\n",
        "sudo apt-get install -y gcsfuse\n",
        "\n",
        "# Verify installation\n",
        "!which gcsfuse\n",
        "print(\"‚úÖ gcsfuse installed\")\n",
        "\n",
        "# Create mount point directory and parent directories\n",
        "print(f\"Creating mount point: {MOUNT_POINT}\")\n",
        "os.makedirs(os.path.dirname(MOUNT_POINT), exist_ok=True)\n",
        "os.makedirs(MOUNT_POINT, exist_ok=True)\n",
        "\n",
        "# Check if already mounted\n",
        "try:\n",
        "    result = subprocess.run(['mountpoint', '-q', MOUNT_POINT], capture_output=True)\n",
        "    if result.returncode == 0:\n",
        "        print(f\"‚úÖ Already mounted at {MOUNT_POINT}\")\n",
        "    else:\n",
        "        # Try to unmount if exists but not properly mounted\n",
        "        try:\n",
        "            subprocess.run(['fusermount', '-u', MOUNT_POINT], capture_output=True, timeout=5)\n",
        "        except:\n",
        "            try:\n",
        "                subprocess.run(['umount', MOUNT_POINT], capture_output=True, timeout=5)\n",
        "            except:\n",
        "                pass\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Mount the bucket\n",
        "print(f\"Mounting gs://{BUCKET_NAME} to {MOUNT_POINT}...\")\n",
        "print(\"This may take a moment...\")\n",
        "\n",
        "# Run gcsfuse in background\n",
        "# Note: In Colab, we need to run gcsfuse in background using shell &\n",
        "print(f\"Running: gcsfuse --implicit-dirs {BUCKET_NAME} {MOUNT_POINT}\")\n",
        "!nohup gcsfuse --implicit-dirs {BUCKET_NAME} {MOUNT_POINT} > /tmp/gcsfuse.log 2>&1 &\n",
        "\n",
        "# Wait a moment for mount to initialize\n",
        "print(\"Waiting for mount to initialize...\")\n",
        "time.sleep(8)  # Give it more time to mount\n",
        "\n",
        "# Check mount status\n",
        "print(\"\\nChecking mount status...\")\n",
        "# Check mount log for errors\n",
        "if os.path.exists(\"/tmp/gcsfuse.log\"):\n",
        "    with open(\"/tmp/gcsfuse.log\", \"r\") as f:\n",
        "        log_content = f.read()\n",
        "        if log_content:\n",
        "            print(\"Mount log:\")\n",
        "            print(log_content[-500:])  # Last 500 chars\n",
        "        else:\n",
        "            print(\"Mount log is empty (mount might still be initializing)\")\n",
        "\n",
        "# Also verify we can access the bucket directly with gsutil\n",
        "print(\"\\nVerifying bucket access with gsutil...\")\n",
        "!gsutil ls gs://{BUCKET_NAME}/ | head -10\n",
        "\n",
        "# Verify mount\n",
        "print(f\"\\nVerifying mount at: {MOUNT_POINT}\")\n",
        "print(f\"Path exists: {os.path.exists(MOUNT_POINT)}\")\n",
        "\n",
        "# Check if actually mounted using mountpoint command\n",
        "try:\n",
        "    result = subprocess.run(['mountpoint', '-q', MOUNT_POINT], capture_output=True)\n",
        "    is_mounted = (result.returncode == 0)\n",
        "    print(f\"Is mounted: {is_mounted}\")\n",
        "except:\n",
        "    # Fallback: check mount table\n",
        "    result = subprocess.run(['mount'], capture_output=True, text=True)\n",
        "    is_mounted = MOUNT_POINT in result.stdout\n",
        "    print(f\"Is mounted (from mount table): {is_mounted}\")\n",
        "\n",
        "if os.path.exists(MOUNT_POINT) and is_mounted:\n",
        "    try:\n",
        "        # Try to list contents\n",
        "        items = os.listdir(MOUNT_POINT)\n",
        "        if len(items) > 0:\n",
        "            print(f\"‚úÖ GCS bucket mounted successfully!\")\n",
        "            print(f\"Mount point: {MOUNT_POINT}\")\n",
        "            print(f\"Found {len(items)} items in bucket\")\n",
        "            # List a few items to verify\n",
        "            for item in items[:10]:\n",
        "                item_path = os.path.join(MOUNT_POINT, item)\n",
        "                item_type = \"directory\" if os.path.isdir(item_path) else \"file\"\n",
        "                print(f\"   - {item} ({item_type})\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è  Mount point exists but is empty (0 items)\")\n",
        "            print(f\"   This might indicate:\")\n",
        "            print(f\"   1. Bucket is empty\")\n",
        "            print(f\"   2. Mount didn't work correctly\")\n",
        "            print(f\"   3. Permission issues\")\n",
        "    except PermissionError as e:\n",
        "        print(f\"‚ö†Ô∏è  Permission error accessing mount: {e}\")\n",
        "        print(\"   Mount might still be initializing, wait a moment and try again\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Mount point exists but cannot list contents: {e}\")\n",
        "        print(\"   This might indicate a mount issue\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "elif os.path.exists(MOUNT_POINT) and not is_mounted:\n",
        "    print(f\"‚ö†Ô∏è  Directory exists but is not mounted\")\n",
        "    print(f\"   The directory exists but gcsfuse mount is not active\")\n",
        "    print(f\"   Trying to mount again...\")\n",
        "    # Try mounting again\n",
        "    !nohup gcsfuse --implicit-dirs {BUCKET_NAME} {MOUNT_POINT} > /tmp/gcsfuse.log 2>&1 &\n",
        "    time.sleep(5)\n",
        "    # Re-check\n",
        "    items = os.listdir(MOUNT_POINT) if os.path.exists(MOUNT_POINT) else []\n",
        "    if len(items) > 0:\n",
        "        print(f\"‚úÖ Mount successful after retry! Found {len(items)} items\")\n",
        "    else:\n",
        "        print(f\"‚ùå Mount still not working\")\n",
        "else:\n",
        "    print(\"‚ùå Failed to mount GCS bucket\")\n",
        "    print(f\"   Mount point: {MOUNT_POINT}\")\n",
        "    print(f\"   Check:\")\n",
        "    print(f\"   1. GCP authentication (run the GCP auth cell)\")\n",
        "    print(f\"   2. Bucket name is correct: {BUCKET_NAME}\")\n",
        "    print(f\"   3. You have read access to the bucket\")\n",
        "    print(f\"   4. Check mount log: /tmp/gcsfuse.log\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-7Hx-_cbsd7"
      },
      "source": [
        "## 6. Create Data Symlink\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNKCPxb7bsd7",
        "outputId": "7a7899e6-b8eb-49cc-b1c6-691b4151219a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking mount point: /content/category-agnostic-pose-estimation/Raster2Seq_internal-main/data\n",
            "  Exists: True\n",
            "  Is directory: True\n",
            "  Can list contents: Yes (99 items)\n",
            "\n",
            "Creating symlink:\n",
            "  From: /content/category-agnostic-pose-estimation/data\n",
            "  To: /content/category-agnostic-pose-estimation/Raster2Seq_internal-main/data\n",
            "‚úÖ Created symlink: /content/category-agnostic-pose-estimation/data -> /content/category-agnostic-pose-estimation/Raster2Seq_internal-main/data\n",
            "‚úÖ Symlink verified: /content/category-agnostic-pose-estimation/data\n",
            "  Is symlink: True\n",
            "‚úÖ Can access 99 items through symlink\n",
            "   First 5 items: ['amur_tiger_body', 'annotations', 'antelope_body', 'arcticwolf_face', 'beaver_body']\n"
          ]
        }
      ],
      "source": [
        "# Create symlink from data to mounted GCS bucket (as expected by START_TRAINING.sh)\n",
        "import os\n",
        "\n",
        "PROJECT_ROOT = \"/content/category-agnostic-pose-estimation\"\n",
        "MOUNTED_DATA = os.path.join(PROJECT_ROOT, \"Raster2Seq_internal-main\", \"data\")\n",
        "DATA_SYMLINK = os.path.join(PROJECT_ROOT, \"data\")\n",
        "\n",
        "print(f\"Checking mount point: {MOUNTED_DATA}\")\n",
        "print(f\"  Exists: {os.path.exists(MOUNTED_DATA)}\")\n",
        "if os.path.exists(MOUNTED_DATA):\n",
        "    print(f\"  Is directory: {os.path.isdir(MOUNTED_DATA)}\")\n",
        "    try:\n",
        "        items = os.listdir(MOUNTED_DATA)\n",
        "        print(f\"  Can list contents: Yes ({len(items)} items)\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Can list contents: No ({e})\")\n",
        "\n",
        "# Remove existing symlink or directory if it exists\n",
        "if os.path.exists(DATA_SYMLINK):\n",
        "    if os.path.islink(DATA_SYMLINK):\n",
        "        print(f\"Removing existing symlink: {DATA_SYMLINK}\")\n",
        "        os.unlink(DATA_SYMLINK)\n",
        "    elif os.path.isdir(DATA_SYMLINK):\n",
        "        print(f\"Warning: {DATA_SYMLINK} exists as a directory (not a symlink)\")\n",
        "        print(\"   Removing it to create symlink...\")\n",
        "        import shutil\n",
        "        shutil.rmtree(DATA_SYMLINK)\n",
        "    else:\n",
        "        print(f\"Warning: {DATA_SYMLINK} exists and is not a symlink or directory\")\n",
        "        os.remove(DATA_SYMLINK)\n",
        "\n",
        "# Create symlink\n",
        "if os.path.exists(MOUNTED_DATA) and os.path.isdir(MOUNTED_DATA):\n",
        "    try:\n",
        "        # Use absolute path for symlink target\n",
        "        MOUNTED_DATA_ABS = os.path.abspath(MOUNTED_DATA)\n",
        "        print(f\"\\nCreating symlink:\")\n",
        "        print(f\"  From: {DATA_SYMLINK}\")\n",
        "        print(f\"  To: {MOUNTED_DATA_ABS}\")\n",
        "        os.symlink(MOUNTED_DATA_ABS, DATA_SYMLINK)\n",
        "        print(f\"‚úÖ Created symlink: {DATA_SYMLINK} -> {MOUNTED_DATA_ABS}\")\n",
        "\n",
        "        # Verify symlink\n",
        "        if os.path.exists(DATA_SYMLINK):\n",
        "            print(f\"‚úÖ Symlink verified: {DATA_SYMLINK}\")\n",
        "            print(f\"  Is symlink: {os.path.islink(DATA_SYMLINK)}\")\n",
        "            # Try to list contents through symlink\n",
        "            try:\n",
        "                items = os.listdir(DATA_SYMLINK)\n",
        "                print(f\"‚úÖ Can access {len(items)} items through symlink\")\n",
        "                print(f\"   First 5 items: {items[:5]}\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è  Symlink exists but cannot access contents: {e}\")\n",
        "        else:\n",
        "            print(f\"‚ùå Symlink creation failed - path does not exist after creation\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error creating symlink: {e}\")\n",
        "        print(f\"   Source: {MOUNTED_DATA}\")\n",
        "        print(f\"   Target: {DATA_SYMLINK}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    print(f\"‚ùå Mounted data not found at {MOUNTED_DATA}\")\n",
        "    print(f\"   Please check that GCS bucket is mounted correctly\")\n",
        "    print(f\"   Run the mount cell above and check for errors\")\n",
        "    print(f\"   Mount point should exist and be accessible\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "77sWyC00bsd8"
      },
      "outputs": [],
      "source": [
        "## 7. Run Single Image Training\n",
        "\n",
        "# This section trains the model on a **single image** from a specific category for 20 epochs.\n",
        "# This is useful for:\n",
        "# - Quick overfitting test to verify the model can learn\n",
        "# - Debugging the training pipeline\n",
        "# - Testing on Colab GPU\n",
        "\n",
        "# **Training Configuration:**\n",
        "# - Single image mode: Enabled\n",
        "# - Category: 40 (zebra) - you can change this\n",
        "# - Epochs: 20\n",
        "# - All logs will be saved to `training_logs.txt`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8187U9Mbsd8"
      },
      "source": [
        "# Configure single image training parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtldOllGbsd8",
        "outputId": "e230f63e-81c2-4115-aae9-1ddaeef4cab3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Single Image Training Configuration\n",
            "================================================================================\n",
            "Category ID:        40\n",
            "Epochs:            20\n",
            "Batch size:        1\n",
            "Queries/episode:   1\n",
            "Episodes/epoch:    20\n",
            "Output directory:  /content/category-agnostic-pose-estimation/output/single_image_colab\n",
            "Log file:          /content/category-agnostic-pose-estimation/output/single_image_colab/training_logs.txt\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Configure single image training\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "PROJECT_ROOT = \"/content/category-agnostic-pose-estimation\"\n",
        "\n",
        "# Training configuration\n",
        "SINGLE_IMAGE_CATEGORY = 40  # Category ID (40 = zebra, change if needed)\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 1\n",
        "NUM_QUERIES_PER_EPISODE = 1\n",
        "EPISODES_PER_EPOCH = 20\n",
        "\n",
        "# Output directories\n",
        "OUTPUT_DIR = os.path.join(PROJECT_ROOT, \"output\", \"single_image_colab\")\n",
        "LOG_FILE = os.path.join(OUTPUT_DIR, \"training_logs.txt\")\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"Single Image Training Configuration\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Category ID:        {SINGLE_IMAGE_CATEGORY}\")\n",
        "print(f\"Epochs:            {EPOCHS}\")\n",
        "print(f\"Batch size:        {BATCH_SIZE}\")\n",
        "print(f\"Queries/episode:   {NUM_QUERIES_PER_EPISODE}\")\n",
        "print(f\"Episodes/epoch:    {EPISODES_PER_EPOCH}\")\n",
        "print(f\"Output directory:  {OUTPUT_DIR}\")\n",
        "print(f\"Log file:          {LOG_FILE}\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1HI8BiQFDqA"
      },
      "source": [
        "# Run Training with Logging\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find first image from cleaned train annotations that exists in mounted data\n",
        "import os\n",
        "import json\n",
        "\n",
        "PROJECT_ROOT = \"/content/category-agnostic-pose-estimation\"\n",
        "DATA_DIR = os.path.join(PROJECT_ROOT, \"data\")\n",
        "ANNOTATION_FILE = os.path.join(PROJECT_ROOT, \"annotations\", \"mp100_split1_train.json\")\n",
        "\n",
        "print(\"Finding first valid image from cleaned train annotations...\")\n",
        "print(f\"Data directory: {DATA_DIR}\")\n",
        "print(f\"Annotation file: {ANNOTATION_FILE}\")\n",
        "print()\n",
        "\n",
        "# Load train annotations\n",
        "if os.path.exists(ANNOTATION_FILE):\n",
        "    with open(ANNOTATION_FILE, 'r') as f:\n",
        "        coco_data = json.load(f)\n",
        "    \n",
        "    images = coco_data.get('images', [])\n",
        "    print(f\"Found {len(images)} images in train annotations\")\n",
        "    \n",
        "    # Find first image that exists\n",
        "    found_image = None\n",
        "    for img_info in images:\n",
        "        file_name = img_info['file_name']  # e.g., \"camel_face/camel_133.jpg\"\n",
        "        full_path = os.path.join(DATA_DIR, file_name)\n",
        "        \n",
        "        if os.path.exists(full_path):\n",
        "            found_image = full_path\n",
        "            print(f\"‚úÖ Found first valid image:\")\n",
        "            print(f\"   File name: {file_name}\")\n",
        "            print(f\"   Full path: {full_path}\")\n",
        "            print(f\"   Image ID: {img_info.get('id', 'N/A')}\")\n",
        "            break\n",
        "    \n",
        "    if found_image:\n",
        "        SINGLE_IMAGE_PATH = found_image\n",
        "        print(f\"\\n‚úÖ Set SINGLE_IMAGE_PATH = {SINGLE_IMAGE_PATH}\")\n",
        "    else:\n",
        "        print(\"‚ùå No images found in mounted data!\")\n",
        "        print(\"   Check that:\")\n",
        "        print(\"   1. GCS bucket is mounted correctly\")\n",
        "        print(\"   2. Data symlink is created\")\n",
        "        print(\"   3. Images exist in the data directory\")\n",
        "        SINGLE_IMAGE_PATH = None\n",
        "else:\n",
        "    print(f\"‚ùå Annotation file not found: {ANNOTATION_FILE}\")\n",
        "    SINGLE_IMAGE_PATH = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wr94Z3s7OuDp"
      },
      "outputs": [],
      "source": [
        "# SINGLE_IMAGE_PATH is now set automatically by the previous cell\n",
        "# It finds the first image from cleaned train annotations that exists in mounted data\n",
        "# If you want to use a specific image instead, uncomment and set it here:\n",
        "# SINGLE_IMAGE_PATH = '/content/category-agnostic-pose-estimation/data/bison_body/000000001113.jpg'\n",
        "\n",
        "# Verify SINGLE_IMAGE_PATH is set\n",
        "if 'SINGLE_IMAGE_PATH' not in globals() or SINGLE_IMAGE_PATH is None:\n",
        "    print(\"‚ö†Ô∏è  SINGLE_IMAGE_PATH not set! Please run the previous cell to find a valid image.\")\n",
        "else:\n",
        "    print(f\"‚úÖ Using image: {SINGLE_IMAGE_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vj5IXtlqNvUl",
        "outputId": "f2078af0-cb61-4bca-ed6e-43a471da8671"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n",
            "Command: /usr/bin/python3 -m models.train_cape_episodic --dataset_root /content/category-agnostic-pose-estimation --category_split_file /content/category-agnostic-pose-estimation/category_splits.json --output_dir /content/category-agnostic-pose-estimation/output/single_image_colab --device cuda:0 --debug_single_image_path /content/category-agnostic-pose-estimation/data/bison_body/000000001113.jpg --epochs 20 --batch_size 1 --num_queries_per_episode 1 --episodes_per_epoch 20 --lr 1e-4 --lr_backbone 1e-5 --weight_decay 1e-4 --clip_max_norm 0.1 --support_encoder_layers 3 --support_fusion_method cross_attention --backbone resnet50 --hidden_dim 256 --nheads 8 --enc_layers 6 --dec_layers 6 --dim_feedforward 1024 --dropout 0.1 --image_size 256 --vocab_size 2000 --seq_len 200 --num_queries 200 --num_polys 1 --cls_loss_coef 2.0 --coords_loss_coef 5.0 --room_cls_loss_coef 0.0 --semantic_classes 70 --num_feature_levels 4 --dec_n_points 4 --enc_n_points 4 --aux_loss --with_poly_refine --num_workers 2 --seed 42 --print_freq 5 --use_amp --cudnn_benchmark --job_name single_image_colab_20251125_225720\n",
            "Logging to: /content/category-agnostic-pose-estimation/output/single_image_colab/training_logs.txt\n",
            "================================================================================\n",
            "\n",
            "/content/category-agnostic-pose-estimation/models/deformable_transformer.py:87: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  :param input_padding_mask          (N, \\sum_{l=0}^{L-1} H_l \\cdot W_l), True for padding elements, False for non-padding elements\n",
            "/content/category-agnostic-pose-estimation/datasets/mp100_cape.py:870: UserWarning: Argument(s) 'var_limit, mean' are not valid for transform GaussNoise\n",
            "  A.GaussNoise(\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/content/category-agnostic-pose-estimation/models/train_cape_episodic.py\", line 915, in <module>\n",
            "    main(args)\n",
            "  File \"/content/category-agnostic-pose-estimation/models/train_cape_episodic.py\", line 489, in main\n",
            "    raise ValueError(\n",
            "ValueError: Image not found: /content/category-agnostic-pose-estimation/data/bison_body/000000001113.jpg\n",
            "  Searched for: content/category-agnostic-pose-estimation/data/bison_body/000000001113.jpg\n",
            "  Full path checked: /content/category-agnostic-pose-estimation/data/content/category-agnostic-pose-estimation/data/bison_body/000000001113.jpg\n",
            "  Please verify:\n",
            "    1. The image path is correct\n",
            "    2. The image exists in the data directory\n",
            "    3. The image is in the train annotations\n",
            "================================================================================\n",
            "Category-Agnostic Pose Estimation (CAPE) - Episodic Training\n",
            "================================================================================\n",
            "\n",
            "Mode: Episodic meta-learning with support pose graphs\n",
            "Support encoder layers: 3\n",
            "Fusion method: cross_attention\n",
            "Queries per episode: 1\n",
            "Episodes per epoch: 20\n",
            "\n",
            "Using device: cuda:0\n",
            "  GPU: NVIDIA A100-SXM4-40GB\n",
            "  CUDA Version: 12.6\n",
            "  GPU Memory: 39.56 GB\n",
            "\n",
            "‚úì cuDNN benchmark mode enabled (faster convolutions)\n",
            "‚úì Mixed precision training (AMP) enabled\n",
            "  ‚Üí Expected speedup: ~2x on modern GPUs\n",
            "‚úì CUDA Memory: 39.56 GB\n",
            "‚úì CUDA Version: 12.6\n",
            "‚úì cuDNN Version: 91002\n",
            "\n",
            "loading annotations into memory...\n",
            "Done (t=0.27s)\n",
            "creating index...\n",
            "index created!\n",
            "üìä Multi-instance statistics:\n",
            "   - Images with multiple instances: 615/12816 (4.8%)\n",
            "   - Total instances available: 13712\n",
            "   - Instances actually used: 12816 (93.5%)\n",
            "   - Instances skipped: 896 (6.5%)\n",
            "   - Max instances in single image: 9\n",
            "   ‚ö†Ô∏è  Note: Currently using only first instance per image\n",
            "Loaded MP-100 train dataset: 12816 images\n",
            "loading annotations into memory...\n",
            "Done (t=0.03s)\n",
            "creating index...\n",
            "index created!\n",
            "üìä Multi-instance statistics:\n",
            "   - Images with multiple instances: 62/1703 (3.6%)\n",
            "   - Total instances available: 1795\n",
            "   - Instances actually used: 1703 (94.9%)\n",
            "   - Instances skipped: 92 (5.1%)\n",
            "   - Max instances in single image: 8\n",
            "   ‚ö†Ô∏è  Note: Currently using only first instance per image\n",
            "Loaded MP-100 val dataset: 1703 images\n",
            "Tokenizer: <datasets.discrete_tokenizer.DiscreteTokenizerV2 object at 0x7a2ce85639b0>\n",
            "  vocab_size: 1940\n",
            "  num_bins: 44\n",
            "\n",
            "Building base Raster2Seq model...\n",
            "Initializing CAPE Support Pose Encoder...\n",
            "Building CAPE-specific loss criterion...\n",
            "‚úì CAPE criterion created with visibility masking support\n",
            "Wrapping with CAPE support conditioning...\n",
            "‚úì Model moved to device: cuda:0\n",
            "Total trainable parameters: 50,370,293\n",
            "Creating episodic dataloaders...\n",
            "\n",
            "================================================================================\n",
            "‚ö†Ô∏è  DEBUG SINGLE IMAGE MODE ENABLED (by file path)\n",
            "================================================================================\n",
            "Training on SINGLE IMAGE with path: /content/category-agnostic-pose-estimation/data/bison_body/000000001113.jpg\n",
            "loading annotations into memory...\n",
            "Done (t=0.27s)\n",
            "creating index...\n",
            "index created!\n",
            "üìä Multi-instance statistics:\n",
            "   - Images with multiple instances: 615/12816 (4.8%)\n",
            "   - Total instances available: 13712\n",
            "   - Instances actually used: 12816 (93.5%)\n",
            "   - Instances skipped: 896 (6.5%)\n",
            "   - Max instances in single image: 9\n",
            "   ‚ö†Ô∏è  Note: Currently using only first instance per image\n",
            "Loaded MP-100 train dataset: 12816 images\n",
            "\n",
            "================================================================================\n",
            "‚ùå Training failed with return code: 1\n",
            "Check logs at: /content/category-agnostic-pose-estimation/output/single_image_colab/training_logs.txt\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Run training on single image with full logging\n",
        "import subprocess\n",
        "import sys\n",
        "from datetime import datetime\n",
        "\n",
        "PROJECT_ROOT = \"/content/category-agnostic-pose-estimation\"\n",
        "OUTPUT_DIR = os.path.join(PROJECT_ROOT, \"output\", \"single_image_colab\")\n",
        "LOG_FILE = os.path.join(OUTPUT_DIR, \"training_logs.txt\")\n",
        "\n",
        "# Build training command\n",
        "cmd = [\n",
        "    sys.executable, \"-m\", \"models.train_cape_episodic\",\n",
        "    \"--dataset_root\", PROJECT_ROOT,\n",
        "    \"--category_split_file\", os.path.join(PROJECT_ROOT, \"category_splits.json\"),\n",
        "    \"--output_dir\", OUTPUT_DIR,\n",
        "    \"--device\", \"cuda:0\",\n",
        "]\n",
        "\n",
        "# Add single image mode argument (path takes precedence over category)\n",
        "if SINGLE_IMAGE_PATH:\n",
        "    cmd.extend([\"--debug_single_image_path\", SINGLE_IMAGE_PATH])\n",
        "elif 'SINGLE_IMAGE_CATEGORY' in globals() and SINGLE_IMAGE_CATEGORY is not None:\n",
        "    cmd.extend([\"--debug_single_image\", str(SINGLE_IMAGE_CATEGORY)])\n",
        "\n",
        "# Add remaining training arguments\n",
        "cmd.extend([\n",
        "    \"--epochs\", str(EPOCHS),\n",
        "    \"--batch_size\", str(BATCH_SIZE),\n",
        "    \"--num_queries_per_episode\", str(NUM_QUERIES_PER_EPISODE),\n",
        "    \"--episodes_per_epoch\", str(EPISODES_PER_EPOCH),\n",
        "    \"--lr\", \"1e-4\",\n",
        "    \"--lr_backbone\", \"1e-5\",\n",
        "    \"--weight_decay\", \"1e-4\",\n",
        "    \"--clip_max_norm\", \"0.1\",\n",
        "    \"--support_encoder_layers\", \"3\",\n",
        "    \"--support_fusion_method\", \"cross_attention\",\n",
        "    \"--backbone\", \"resnet50\",\n",
        "    \"--hidden_dim\", \"256\",\n",
        "    \"--nheads\", \"8\",\n",
        "    \"--enc_layers\", \"6\",\n",
        "    \"--dec_layers\", \"6\",\n",
        "    \"--dim_feedforward\", \"1024\",\n",
        "    \"--dropout\", \"0.1\",\n",
        "    \"--image_size\", \"256\",\n",
        "    \"--vocab_size\", \"2000\",\n",
        "    \"--seq_len\", \"200\",\n",
        "    \"--num_queries\", \"200\",\n",
        "    \"--num_polys\", \"1\",\n",
        "    \"--cls_loss_coef\", \"2.0\",\n",
        "    \"--coords_loss_coef\", \"5.0\",\n",
        "    \"--room_cls_loss_coef\", \"0.0\",\n",
        "    \"--semantic_classes\", \"70\",\n",
        "    \"--num_feature_levels\", \"4\",\n",
        "    \"--dec_n_points\", \"4\",\n",
        "    \"--enc_n_points\", \"4\",\n",
        "    \"--aux_loss\",\n",
        "    \"--with_poly_refine\",\n",
        "    \"--num_workers\", \"2\",\n",
        "    \"--seed\", \"42\",\n",
        "    \"--print_freq\", \"5\",\n",
        "    \"--use_amp\",  # Enable mixed precision for faster training\n",
        "    \"--cudnn_benchmark\",  # Enable cuDNN benchmark\n",
        "    \"--job_name\", f\"single_image_colab_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "]\n",
        ")\n",
        "\n",
        "print(\"Starting training...\")\n",
        "print(f\"Command: {' '.join(cmd)}\")\n",
        "print(f\"Logging to: {LOG_FILE}\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "# Change to project directory\n",
        "os.chdir(PROJECT_ROOT)\n",
        "\n",
        "# Run training with logging to both stdout and file\n",
        "with open(LOG_FILE, 'w') as log_file:\n",
        "    # Write header to log file\n",
        "    log_file.write(\"=\" * 80 + \"\\n\")\n",
        "    log_file.write(f\"Single Image Training Log\\n\")\n",
        "    log_file.write(f\"Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "    if 'SINGLE_IMAGE_PATH' in globals() and SINGLE_IMAGE_PATH:\n",
        "        log_file.write(f\"Image path: {SINGLE_IMAGE_PATH}\\n\")\n",
        "    elif 'SINGLE_IMAGE_CATEGORY' in globals() and SINGLE_IMAGE_CATEGORY is not None:\n",
        "        log_file.write(f\"Category: {SINGLE_IMAGE_CATEGORY}\\n\")\n",
        "    log_file.write(f\"Epochs: {EPOCHS}\\n\")\n",
        "    log_file.write(\"=\" * 80 + \"\\n\\n\")\n",
        "    log_file.flush()\n",
        "\n",
        "    # Run process and stream output to both stdout and file\n",
        "    process = subprocess.Popen(\n",
        "        cmd,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        universal_newlines=True,\n",
        "        bufsize=1\n",
        "    )\n",
        "\n",
        "    # Stream output in real-time\n",
        "    for line in process.stdout:\n",
        "        print(line, end='')  # Print to notebook\n",
        "        log_file.write(line)  # Write to log file\n",
        "        log_file.flush()  # Ensure immediate write\n",
        "\n",
        "    # Wait for process to complete\n",
        "    return_code = process.wait()\n",
        "\n",
        "    # Write footer to log file\n",
        "    log_file.write(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
        "    log_file.write(f\"Training completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "    log_file.write(f\"Return code: {return_code}\\n\")\n",
        "    log_file.write(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "if return_code == 0:\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"‚úÖ Training completed successfully!\")\n",
        "    print(f\"Checkpoints saved to: {OUTPUT_DIR}\")\n",
        "    print(f\"Full logs saved to: {LOG_FILE}\")\n",
        "    print(\"=\" * 80)\n",
        "else:\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"‚ùå Training failed with return code: {return_code}\")\n",
        "    print(f\"Check logs at: {LOG_FILE}\")\n",
        "    print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zem_FhAMbsd8"
      },
      "outputs": [],
      "source": [
        "## 8. Check Training Results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_AS2oM9PY5O"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMEKlLFvqa4q"
      },
      "outputs": [],
      "source": [
        "# Check training results and find best checkpoint\n",
        "import os\n",
        "import glob\n",
        "\n",
        "PROJECT_ROOT = \"/content/category-agnostic-pose-estimation\"\n",
        "OUTPUT_DIR = os.path.join(PROJECT_ROOT, \"output\", \"single_image_colab\")\n",
        "\n",
        "print(\"Checking training results...\")\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")\n",
        "print()\n",
        "\n",
        "# List all checkpoints\n",
        "checkpoints = glob.glob(os.path.join(OUTPUT_DIR, \"*.pth\"))\n",
        "if checkpoints:\n",
        "    print(f\"Found {len(checkpoints)} checkpoint(s):\")\n",
        "    for ckpt in sorted(checkpoints):\n",
        "        size_mb = os.path.getsize(ckpt) / (1024 * 1024)\n",
        "        print(f\"  - {os.path.basename(ckpt)} ({size_mb:.1f} MB)\")\n",
        "\n",
        "    # Find best checkpoint\n",
        "    best_ckpts = glob.glob(os.path.join(OUTPUT_DIR, \"checkpoint_best_pck*.pth\"))\n",
        "    if best_ckpts:\n",
        "        best_ckpt = sorted(best_ckpts)[-1]  # Get most recent\n",
        "        print(f\"\\n‚úÖ Best checkpoint: {os.path.basename(best_ckpt)}\")\n",
        "        BEST_CHECKPOINT = best_ckpt\n",
        "    else:\n",
        "        # Use most recent checkpoint\n",
        "        best_ckpt = sorted(checkpoints, key=os.path.getmtime)[-1]\n",
        "        print(f\"\\n‚ö†Ô∏è  No 'best' checkpoint found, using most recent: {os.path.basename(best_ckpt)}\")\n",
        "        BEST_CHECKPOINT = best_ckpt\n",
        "else:\n",
        "    print(\"‚ùå No checkpoints found!\")\n",
        "    BEST_CHECKPOINT = None\n",
        "\n",
        "# Show log file info\n",
        "LOG_FILE = os.path.join(OUTPUT_DIR, \"training_logs.txt\")\n",
        "if os.path.exists(LOG_FILE):\n",
        "    size_mb = os.path.getsize(LOG_FILE) / (1024 * 1024)\n",
        "    print(f\"\\nüìÑ Training log: {os.path.basename(LOG_FILE)} ({size_mb:.2f} MB)\")\n",
        "\n",
        "    # Show last few lines of log\n",
        "    print(\"\\nLast 20 lines of training log:\")\n",
        "    print(\"-\" * 80)\n",
        "    with open(LOG_FILE, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        for line in lines[-20:]:\n",
        "            print(line.rstrip())\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  Training log not found\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b8ca77f"
      },
      "source": [
        "## 9. Visualize Predictions\n",
        "\n",
        "This section visualizes the model's predictions on the single training image.\n",
        "We'll use the trained checkpoint to generate predictions and visualize them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnrdf9cSFDqB"
      },
      "outputs": [],
      "source": [
        "# Visualize predictions using the trained model\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "from datetime import datetime\n",
        "\n",
        "PROJECT_ROOT = \"/content/category-agnostic-pose-estimation\"\n",
        "OUTPUT_DIR = os.path.join(PROJECT_ROOT, \"output\", \"single_image_colab\")\n",
        "VISUALIZATION_DIR = os.path.join(OUTPUT_DIR, \"visualizations\")\n",
        "\n",
        "# Find best checkpoint\n",
        "import glob\n",
        "best_ckpts = glob.glob(os.path.join(OUTPUT_DIR, \"checkpoint_best_pck*.pth\"))\n",
        "if not best_ckpts:\n",
        "    # Try any checkpoint\n",
        "    all_ckpts = glob.glob(os.path.join(OUTPUT_DIR, \"*.pth\"))\n",
        "    if all_ckpts:\n",
        "        best_ckpts = [sorted(all_ckpts, key=os.path.getmtime)[-1]]\n",
        "\n",
        "if not best_ckpts:\n",
        "    print(\"‚ùå No checkpoint found! Please run training first.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "CHECKPOINT = sorted(best_ckpts)[-1]\n",
        "\n",
        "# Single image to visualize (the overfitted training image)\n",
        "SINGLE_IMAGE_PATH = \"/content/category-agnostic-pose-estimation/data/camel_face/camel_133.jpg\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"CAPE Prediction Visualization\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Checkpoint: {os.path.basename(CHECKPOINT)}\")\n",
        "print(f\"Image:      {SINGLE_IMAGE_PATH}\")\n",
        "print(f\"Output:     {VISUALIZATION_DIR}\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "# Create visualization directory\n",
        "os.makedirs(VISUALIZATION_DIR, exist_ok=True)\n",
        "\n",
        "# Build visualization command\n",
        "cmd = [\n",
        "    sys.executable, \"-m\", \"models.visualize_cape_predictions\",\n",
        "    \"--checkpoint\", CHECKPOINT,\n",
        "    \"--dataset_root\", PROJECT_ROOT,\n",
        "    \"--device\", \"cuda\",  # Use \"cuda\" not \"cuda:0\" - script only accepts cpu, cuda, or mps\n",
        "    \"--single_image_path\", SINGLE_IMAGE_PATH,  # Visualize specific overfitted image\n",
        "    \"--output_dir\", VISUALIZATION_DIR\n",
        "]\n",
        "\n",
        "print(\"Running visualization...\")\n",
        "print(f\"Command: {' '.join(cmd)}\")\n",
        "print()\n",
        "\n",
        "# Change to project directory\n",
        "os.chdir(PROJECT_ROOT)\n",
        "\n",
        "# Run visualization\n",
        "try:\n",
        "    result = subprocess.run(cmd, check=True, capture_output=True, text=True)\n",
        "    print(result.stdout)\n",
        "    if result.stderr:\n",
        "        print(\"Warnings/Errors:\")\n",
        "        print(result.stderr)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"‚úÖ Visualization complete!\")\n",
        "    print(f\"Results saved to: {VISUALIZATION_DIR}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # List generated visualizations\n",
        "    vis_files = glob.glob(os.path.join(VISUALIZATION_DIR, \"*.png\"))\n",
        "    if vis_files:\n",
        "        print(f\"\\nGenerated {len(vis_files)} visualization(s):\")\n",
        "        for vis_file in sorted(vis_files):\n",
        "            print(f\"  - {os.path.basename(vis_file)}\")\n",
        "\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print(f\"\\n‚ùå Visualization failed with return code: {e.returncode}\")\n",
        "    print(\"STDOUT:\")\n",
        "    print(e.stdout)\n",
        "    print(\"\\nSTDERR:\")\n",
        "    print(e.stderr)\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Error during visualization: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IACB-c6UFDqB"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cM2N0AhVFDqB"
      },
      "outputs": [],
      "source": [
        "# Download results from Colab\n",
        "from google.colab import files\n",
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "import glob\n",
        "\n",
        "PROJECT_ROOT = \"/content/category-agnostic-pose-estimation\"\n",
        "OUTPUT_DIR = os.path.join(PROJECT_ROOT, \"output\", \"single_image_colab\")\n",
        "\n",
        "print(\"Preparing results for download...\")\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")\n",
        "print()\n",
        "\n",
        "# Create a zip file with all results\n",
        "ZIP_FILE = \"/content/single_image_training_results.zip\"\n",
        "\n",
        "with zipfile.ZipFile(ZIP_FILE, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "    # Add training logs\n",
        "    log_file = os.path.join(OUTPUT_DIR, \"training_logs.txt\")\n",
        "    if os.path.exists(log_file):\n",
        "        zipf.write(log_file, \"training_logs.txt\")\n",
        "        print(f\"‚úì Added training logs\")\n",
        "\n",
        "    # Add checkpoints (only best one to save space)\n",
        "    best_ckpts = glob.glob(os.path.join(OUTPUT_DIR, \"checkpoint_best_pck*.pth\"))\n",
        "    if best_ckpts:\n",
        "        best_ckpt = sorted(best_ckpts)[-1]\n",
        "        zipf.write(best_ckpt, f\"checkpoints/{os.path.basename(best_ckpt)}\")\n",
        "        print(f\"‚úì Added best checkpoint: {os.path.basename(best_ckpt)}\")\n",
        "\n",
        "    # Add visualizations\n",
        "    vis_dir = os.path.join(OUTPUT_DIR, \"visualizations\")\n",
        "    if os.path.exists(vis_dir):\n",
        "        for root, dirs, files in os.walk(vis_dir):\n",
        "            for file in files:\n",
        "                file_path = os.path.join(root, file)\n",
        "                arcname = os.path.relpath(file_path, OUTPUT_DIR)\n",
        "                zipf.write(file_path, arcname)\n",
        "        print(f\"‚úì Added visualizations\")\n",
        "\n",
        "print(f\"\\n‚úÖ Created zip file: {ZIP_FILE}\")\n",
        "print(f\"Size: {os.path.getsize(ZIP_FILE) / (1024*1024):.2f} MB\")\n",
        "print(\"\\nDownloading...\")\n",
        "files.download(ZIP_FILE)\n",
        "\n",
        "print(\"\\n‚úÖ Download complete!\")\n",
        "print(\"\\nThe zip file contains:\")\n",
        "print(\"  - training_logs.txt (full training output)\")\n",
        "print(\"  - checkpoints/ (best model checkpoint)\")\n",
        "print(\"  - visualizations/ (prediction visualizations)\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
