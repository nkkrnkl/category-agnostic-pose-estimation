#!/usr/bin/env python3
"""
Regression Test: Verify forward_inference returns FULL sequence, not just last token.

This test ensures the critical bug in models/roomformer_v2.py is fixed:
- BUG: forward_inference only returned last token (shape B, 1, 2)
- FIX: Accumulate cls_output and reg_output across all iterations

If this test FAILS, the bug has regressed!
"""

import os
os.environ.setdefault('PYTORCH_ENABLE_MPS_FALLBACK', '1')

import torch
import argparse
from pathlib import Path

# Load real checkpoint and test with real data
def test_with_real_checkpoint():
    """Test using a real checkpoint to verify full sequence generation."""
    
    print("=" * 80)
    print("REGRESSION TEST: Forward Inference Full Sequence Output")
    print("=" * 80)
    print()
    
    checkpoint_path = Path('outputs/cape_run/checkpoint_e010_lr1e-04_bs2_acc4_qpe2.pth')
    
    if not checkpoint_path.exists():
        print(f"‚ö†Ô∏è  Checkpoint not found: {checkpoint_path}")
        print("   Skipping test (requires trained checkpoint)")
        return True
    
    print(f"Loading checkpoint: {checkpoint_path.name}")
    checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)
    args = checkpoint['args']
    print(f"  Epoch: {checkpoint.get('epoch', '?')}")
    print()
    
    # Build dataset + model (same as evaluation script)
    import sys
    sys.path.insert(0, str(Path.cwd()))
    
    from datasets.mp100_cape import build_mp100_cape
    from models import build_model
    from models.cape_model import build_cape_model
    
    print("Building dataset...")
    dataset = build_mp100_cape('train', args)
    tokenizer = dataset.get_tokenizer()
    print(f"‚úì Tokenizer: vocab_size={len(tokenizer)}")
    print()
    
    print("Building model...")
    build_result = build_model(args, train=False, tokenizer=tokenizer)
    if isinstance(build_result, tuple):
        base_model = build_result[0]
    else:
        base_model = build_result
    model = build_cape_model(args, base_model)
    model.load_state_dict(checkpoint['model'], strict=False)
    model.eval()
    print(f"‚úì Model loaded")
    print()
    
    # Create dummy input
    B = 2
    C, H, W = 3, 512, 512
    N = 17
    
    query_images = torch.randn(B, C, H, W)
    support_coords = torch.rand(B, N, 2)
    support_mask = torch.ones(B, N).bool()
    
    print("Running forward_inference...")
    with torch.no_grad():
        try:
            outputs = model.forward_inference(
                samples=query_images,
                support_coords=support_coords,
                support_mask=support_mask,
                skeleton_edges=None
            )
        except Exception as e:
            print(f"‚ùå FAIL: forward_inference raised exception: {e}")
            return False
    
    print("‚úì forward_inference completed")
    print()
    
    # ============================================================================
    # CRITICAL CHECKS
    # ============================================================================
    
    print("=" * 80)
    print("CRITICAL CHECKS")
    print("=" * 80)
    print()
    
    pred_coords = outputs.get('coordinates')
    gen_out = outputs.get('gen_out')
    
    if pred_coords is None:
        print("‚ùå FAIL: pred_coords is None!")
        return False
    
    print(f"‚úì pred_coords exists: {pred_coords.shape}")
    print()
    
    # Check 1: Sequence length > 1
    seq_len = pred_coords.shape[1]
    print(f"Check 1: Sequence length > 1")
    print(f"  Actual: {seq_len}")
    if seq_len == 1:
        print(f"  ‚ùå FAIL: Only 1 token returned (BUG EXISTS!)")
        return False
    else:
        print(f"  ‚úÖ PASS: Multiple tokens returned")
    print()
    
    # Check 2: gen_out matches pred_coords
    if isinstance(gen_out, list) and len(gen_out) > 0:
        gen_len = len(gen_out[0])
        print(f"Check 2: gen_out length matches pred_coords")
        print(f"  gen_out[0] length: {gen_len}")
        print(f"  pred_coords seq length: {seq_len}")
        
        if gen_len != seq_len:
            print(f"  ‚ùå FAIL: Mismatch between gen_out and pred_coords")
            return False
        else:
            print(f"  ‚úÖ PASS: Lengths match")
    print()
    
    # Check 3: pred_logits also has full sequence
    pred_logits = outputs.get('pred_logits')
    if pred_logits is not None:
        logits_seq_len = pred_logits.shape[1]
        print(f"Check 3: pred_logits has full sequence")
        print(f"  pred_logits shape: {pred_logits.shape}")
        
        if logits_seq_len == 1:
            print(f"  ‚ùå FAIL: pred_logits only has 1 position")
            return False
        elif logits_seq_len != seq_len:
            print(f"  ‚ö†Ô∏è  Warning: pred_logits ({logits_seq_len}) != pred_coords ({seq_len})")
        else:
            print(f"  ‚úÖ PASS: pred_logits has full sequence")
    else:
        print(f"Check 3: pred_logits is None (may be OK depending on model)")
    print()
    
    print("=" * 80)
    print("‚úÖ ALL CRITICAL CHECKS PASSED")
    print("=" * 80)
    print()
    print("The forward_inference output bug is FIXED:")
    print(f"  - Returns {seq_len} tokens (not 1)")
    print(f"  - gen_out and pred_coords match")
    print(f"  - No shape mismatch errors")
    print()
    
    return True


if __name__ == '__main__':
    success = test_with_real_checkpoint()
    
    if success:
        print("üéâ REGRESSION TEST PASSED - Bug is fixed!")
        exit(0)
    else:
        print("‚ùå REGRESSION TEST FAILED - Bug exists or regressed!")
        exit(1)

