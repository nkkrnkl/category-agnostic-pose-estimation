{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yr9fJlVbsd3"
      },
      "source": [
        "# ğŸš€ MP-100 CAPE Training on Google Colab\n",
        "\n",
        "This notebook trains Category-Agnostic Pose Estimation (CAPE) on the full MP-100 dataset using Google Colab's GPU.\n",
        "\n",
        "## ğŸ“‹ Setup Instructions\n",
        "1. **Enable GPU**: Runtime â†’ Change runtime type â†’ GPU (A100 recommended, T4/V100 works)\n",
        "2. **Run all cells in order** (takes ~8-12 hours)\n",
        "3. The notebook will:\n",
        "   - Clone code from GitHub (`pavlos-topic` branch with CUDA fix)\n",
        "   - Install dependencies\n",
        "   - Authenticate to GCP\n",
        "   - Mount GCS bucket with MP-100 dataset (read-only)\n",
        "   - Mount Google Drive for checkpoint storage (persistent)\n",
        "   - Run full CAPE training (300 epochs with early stopping)\n",
        "\n",
        "## ğŸ’¾ Data Storage Strategy\n",
        "- **Input Data**: GCS Bucket `dl-category-agnostic-pose-mp100-data` (read-only, ~100 categories)\n",
        "- **Output Checkpoints**: Google Drive `MyDrive/cape_training_output/` (persistent, survives Colab session)\n",
        "- **Why?**: GCS for large shared dataset, Google Drive for your personal model files\n",
        "\n",
        "## ğŸ“¦ What Gets Saved\n",
        "All files are saved to **Google Drive** (`MyDrive/cape_training_output/`):\n",
        "- âœ… `checkpoint_e###_*.pth` - Every epoch (for resume/analysis)\n",
        "- âœ… `checkpoint_best_pck_*.pth` - Best validation PCK model (for evaluation)\n",
        "- âœ… `training_logs.txt` - Complete training output\n",
        "- âœ… `TRAINING_SUMMARY.txt` - Quick summary with best model info\n",
        "\n",
        "**â†’ You can download these from Google Drive after training completes!**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOiUB-lwbsd4"
      },
      "source": [
        "## 1. Check GPU Availability\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWDvxzLWbsd5",
        "outputId": "0e83bc17-754e-44a0-9caa-22b67d4d5b3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA available: True\n",
            "GPU: NVIDIA A100-SXM4-80GB\n",
            "CUDA version: 12.6\n",
            "GPU Memory: 85.17 GB\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"âš ï¸  No GPU detected! Please enable GPU in Runtime > Change runtime type > GPU\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUmgObskbsd5",
        "outputId": "e047cf01-823b-4f5c-9ecf-b5177bc4e2f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "For private repositories, you need to authenticate.\n",
            "Option 1: Enter your GitHub Personal Access Token\n",
            "  (Get one from: https://github.com/settings/tokens)\n",
            "Option 2: Press Enter to try without token (will fail if repo is private)\n",
            "\n",
            "Enter GitHub Personal Access Token (or press Enter to skip): Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Cloning repository from https://github.com/nkkrnkl/category-agnostic-pose-estimation.git (branch: main)...\n",
            "Cloning into '/content/category-agnostic-pose-estimation'...\n",
            "remote: Enumerating objects: 1428, done.\u001b[K\n",
            "remote: Counting objects: 100% (137/137), done.\u001b[K\n",
            "remote: Compressing objects: 100% (99/99), done.\u001b[K\n",
            "remote: Total 1428 (delta 66), reused 79 (delta 38), pack-reused 1291 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1428/1428), 73.80 MiB | 17.17 MiB/s, done.\n",
            "Resolving deltas: 100% (610/610), done.\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "âœ… Repository cloned successfully to /content/category-agnostic-pose-estimation\n",
            "* \u001b[32mmain\u001b[m\n"
          ]
        }
      ],
      "source": [
        "# Clone repository from GitHub\n",
        "import os\n",
        "import subprocess\n",
        "from getpass import getpass\n",
        "\n",
        "REPO_URL = \"https://github.com/nkkrnkl/category-agnostic-pose-estimation.git\"\n",
        "BRANCH = \"main\"\n",
        "PROJECT_ROOT = \"/content/category-agnostic-pose-estimation\"\n",
        "\n",
        "# Remove existing directory if it exists (SAFE method - won't hang)\n",
        "if os.path.exists(PROJECT_ROOT):\n",
        "    print(f\"Directory exists: {PROJECT_ROOT}\")\n",
        "    print(\"Cleaning up safely...\")\n",
        "\n",
        "    # Step 1: Remove symlinks first (prevent rm -rf from following them into GCS mounts)\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            ['find', PROJECT_ROOT, '-maxdepth', '2', '-type', 'l', '-delete'],\n",
        "            timeout=10,\n",
        "            capture_output=True,\n",
        "            text=True\n",
        "        )\n",
        "        print(\"  âœ“ Removed symlinks\")\n",
        "    except subprocess.TimeoutExpired:\n",
        "        print(\"  âš ï¸  Symlink removal timed out (continuing anyway)\")\n",
        "    except Exception as e:\n",
        "        print(f\"  âš ï¸  Symlink removal warning: {e}\")\n",
        "\n",
        "    # Step 2: Now safely remove directory with timeout\n",
        "    try:\n",
        "        result = subprocess.run(\n",
        "            ['rm', '-rf', PROJECT_ROOT],\n",
        "            timeout=30,  # 30 second timeout prevents hanging\n",
        "            capture_output=True,\n",
        "            text=True,\n",
        "            check=True\n",
        "        )\n",
        "        print(f\"  âœ“ Removed {PROJECT_ROOT}\")\n",
        "    except subprocess.TimeoutExpired:\n",
        "        print(\"  âš ï¸  rm -rf timed out, using Python fallback...\")\n",
        "        import shutil\n",
        "        shutil.rmtree(PROJECT_ROOT, ignore_errors=True)\n",
        "        print(f\"  âœ“ Removed with shutil\")\n",
        "    except Exception as e:\n",
        "        print(f\"  âš ï¸  Removal error: {e}\")\n",
        "        print(\"  â†’ Attempting to continue anyway\")\n",
        "\n",
        "# For private repositories, you need to authenticate\n",
        "# Option 1: Use Personal Access Token (recommended)\n",
        "# Get token from: https://github.com/settings/tokens\n",
        "# Create a token with 'repo' scope\n",
        "print(\"For private repositories, you need to authenticate.\")\n",
        "print(\"Option 1: Enter your GitHub Personal Access Token\")\n",
        "print(\"  (Get one from: https://github.com/settings/tokens)\")\n",
        "print(\"Option 2: Press Enter to try without token (will fail if repo is private)\")\n",
        "print()\n",
        "\n",
        "GITHUB_TOKEN = getpass(\"Enter GitHub Personal Access Token (or press Enter to skip): \")\n",
        "\n",
        "if GITHUB_TOKEN.strip():\n",
        "    # Use token in URL\n",
        "    # Format: https://TOKEN@github.com/username/repo.git\n",
        "    AUTH_REPO_URL = REPO_URL.replace(\"https://github.com/\", f\"https://{GITHUB_TOKEN}@github.com/\")\n",
        "    print(f\"Cloning repository from {REPO_URL} (branch: {BRANCH})...\")\n",
        "    !git clone -b {BRANCH} {AUTH_REPO_URL} {PROJECT_ROOT}\n",
        "    !git pull origin {BRANCH}\n",
        "else:\n",
        "    # Try without token (will work if repo is public)\n",
        "    print(f\"Cloning repository from {REPO_URL} (branch: {BRANCH})...\")\n",
        "    !git clone -b {BRANCH} {REPO_URL} {PROJECT_ROOT}\n",
        "\n",
        "# Verify clone\n",
        "if os.path.exists(PROJECT_ROOT) and os.path.exists(os.path.join(PROJECT_ROOT, \".git\")):\n",
        "    print(f\"âœ… Repository cloned successfully to {PROJECT_ROOT}\")\n",
        "    !cd {PROJECT_ROOT} && git branch\n",
        "else:\n",
        "    print(\"âŒ Failed to clone repository\")\n",
        "    print(\"\\nIf the repository is private, you need to:\")\n",
        "    print(\"1. Create a Personal Access Token at: https://github.com/settings/tokens\")\n",
        "    print(\"2. Select 'repo' scope\")\n",
        "    print(\"3. Run this cell again and paste the token when prompted\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWNIieJCgvy5",
        "outputId": "dfe3df0f-7bb7-4ca2-ee06-e0fb4424ac2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pulling latest changes from branch main...\n",
            "remote: Enumerating objects: 5, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 3 (delta 2), reused 3 (delta 2), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects: 100% (3/3), 1.26 KiB | 117.00 KiB/s, done.\n",
            "From https://github.com/nkkrnkl/category-agnostic-pose-estimation\n",
            " * branch            main       -> FETCH_HEAD\n",
            "   7272279..a0a9d49  main       -> origin/main\n",
            "Updating d33076e..a0a9d49\n",
            "error: Your local changes to the following files would be overwritten by merge:\n",
            "\tdatasets/episodic_sampler.py\n",
            "Please commit your changes or stash them before you merge.\n",
            "Aborting\n",
            "âœ… Git pull complete!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ROOT = \"/content/category-agnostic-pose-estimation\"\n",
        "BRANCH = \"main\"\n",
        "\n",
        "print(f\"Pulling latest changes from branch {BRANCH}...\")\n",
        "!cd {PROJECT_ROOT} && git pull origin {BRANCH}\n",
        "\n",
        "print(\"âœ… Git pull complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "urQW-HS4xgIV"
      },
      "source": [
        "## 2.5 Apply CUDA Fixes (Important!)\n",
        "\n",
        "The GitHub repo may not have the latest CUDA-specific fixes. This cell patches two critical files:\n",
        "\n",
        "1. **`geometric_support_encoder.py`**: Adds safety check for all-masked batches (prevents `to_padded_tensor` crash)\n",
        "2. **`episodic_sampler.py`**: Fixes mask convention (True=ignore, False=use)\n",
        "\n",
        "**Why is this needed?**\n",
        "- CUDA uses nested tensor optimization that crashes when all keypoints are masked\n",
        "- MPS (Mac) doesn't use this optimization, so it works locally but crashes on Colab\n",
        "- Early EOS prediction can cause temporary all-masked batches in epoch 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0zas-kBxgIV",
        "outputId": "7095de7b-5cf4-44a7-aa4a-ffb34ea6a973"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fix 1: Safety check already present in geometric_support_encoder.py âœ…\n",
            "Applying Fix 2: Correcting mask convention in episodic_sampler.py...\n",
            "  âœ… Fix 2 applied successfully!\n",
            "\n",
            "============================================================\n",
            "VERIFICATION:\n",
            "============================================================\n",
            "  Fix 1 (Safety check in geometric_support_encoder.py): âœ… OK\n",
            "  Fix 2 (Mask convention in episodic_sampler.py):       âœ… OK\n",
            "\n",
            "âœ… All CUDA fixes applied successfully!\n",
            "   Training should now work on Colab with A100/V100/T4 GPUs.\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# ğŸ”§ CRITICAL: Apply CUDA Nested Tensor Fixes\n",
        "# ============================================================================\n",
        "# These patches fix PyTorch nested tensor issues that crash on CUDA when\n",
        "# all keypoints in a batch are masked (happens with early EOS prediction).\n",
        "#\n",
        "# Fixes applied:\n",
        "# 1. geometric_support_encoder.py: Add safety check for all-masked batches\n",
        "# 2. episodic_sampler.py: Correct mask convention (True=ignore, False=use)\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "\n",
        "PROJECT_ROOT = \"/content/category-agnostic-pose-estimation\"\n",
        "\n",
        "# ============================================================================\n",
        "# Fix 1: Add safety check to geometric_support_encoder.py\n",
        "# ============================================================================\n",
        "GEOMETRIC_ENCODER_FILE = os.path.join(PROJECT_ROOT, \"models/geometric_support_encoder.py\")\n",
        "\n",
        "# Read the file\n",
        "with open(GEOMETRIC_ENCODER_FILE, 'r') as f:\n",
        "    content = f.read()\n",
        "\n",
        "# Check if the fix is already applied\n",
        "if \"CRITICAL SAFETY CHECK\" not in content:\n",
        "    print(\"Applying Fix 1: Adding safety check to geometric_support_encoder.py...\")\n",
        "\n",
        "    # Find the location to insert the safety check\n",
        "    # We need to modify the forward method where self.transformer_encoder is called\n",
        "    old_code = '''        # 6. Transformer self-attention\n",
        "        support_features = self.transformer_encoder(\n",
        "            embeddings,\n",
        "            src_key_padding_mask=support_mask\n",
        "        )'''\n",
        "\n",
        "    new_code = '''        # 6. Transformer self-attention\n",
        "        # support_mask: True = positions to ignore (mask out)\n",
        "        # PyTorch convention: True = ignore\n",
        "\n",
        "        # CRITICAL SAFETY CHECK: Handle edge case where ALL keypoints are masked\n",
        "        # This can happen with invalid data where all keypoints have visibility==0\n",
        "        # PyTorch's nested tensor conversion fails when all elements are masked\n",
        "        # Check if any batch element has all keypoints masked (all True)\n",
        "        all_masked_per_batch = support_mask.all(dim=1)  # [bs]\n",
        "\n",
        "        if all_masked_per_batch.any():\n",
        "            # At least one batch element has all keypoints masked (invalid data)\n",
        "            # This causes nested tensor conversion to fail\n",
        "            # Workaround: temporarily unmask the first keypoint for those batches\n",
        "            temp_mask = support_mask.clone()\n",
        "            for b in range(support_mask.shape[0]):\n",
        "                if all_masked_per_batch[b]:\n",
        "                    # Unmask the first keypoint to avoid nested tensor error\n",
        "                    # (This shouldn't happen with proper data validation, but we handle it gracefully)\n",
        "                    temp_mask[b, 0] = False\n",
        "\n",
        "            support_features = self.transformer_encoder(\n",
        "                embeddings,\n",
        "                src_key_padding_mask=temp_mask\n",
        "            )\n",
        "\n",
        "            # Zero out features for fully-masked batches (they contain invalid data)\n",
        "            support_features[all_masked_per_batch] = 0.0\n",
        "        else:\n",
        "            # Normal case - process as usual\n",
        "            support_features = self.transformer_encoder(\n",
        "                embeddings,\n",
        "                src_key_padding_mask=support_mask\n",
        "            )'''\n",
        "\n",
        "    if old_code in content:\n",
        "        content = content.replace(old_code, new_code)\n",
        "        with open(GEOMETRIC_ENCODER_FILE, 'w') as f:\n",
        "            f.write(content)\n",
        "        print(\"  âœ… Fix 1 applied successfully!\")\n",
        "    else:\n",
        "        # Try alternative pattern (might have slight differences)\n",
        "        print(\"  âš ï¸  Could not find exact pattern. Checking if fix is already present...\")\n",
        "        if \"all_masked_per_batch\" in content:\n",
        "            print(\"  âœ… Fix appears to be already applied!\")\n",
        "        else:\n",
        "            print(\"  âŒ Could not apply fix - pattern not found. Manual intervention needed.\")\n",
        "else:\n",
        "    print(\"Fix 1: Safety check already present in geometric_support_encoder.py âœ…\")\n",
        "\n",
        "# ============================================================================\n",
        "# Fix 2: Correct mask convention in episodic_sampler.py\n",
        "# ============================================================================\n",
        "EPISODIC_SAMPLER_FILE = os.path.join(PROJECT_ROOT, \"datasets/episodic_sampler.py\")\n",
        "\n",
        "with open(EPISODIC_SAMPLER_FILE, 'r') as f:\n",
        "    sampler_content = f.read()\n",
        "\n",
        "# Check if the WRONG mask convention is present\n",
        "if \"[v > 0 for v in support_visibility]\" in sampler_content:\n",
        "    print(\"Applying Fix 2: Correcting mask convention in episodic_sampler.py...\")\n",
        "\n",
        "    # Fix the mask convention: True should mean \"ignore\", not \"visible\"\n",
        "    old_mask_code = \"[v > 0 for v in support_visibility]\"\n",
        "    new_mask_code = \"[v == 0 for v in support_visibility]\"\n",
        "\n",
        "    sampler_content = sampler_content.replace(old_mask_code, new_mask_code)\n",
        "\n",
        "    # Also add a comment explaining the convention\n",
        "    if \"# CRITICAL: Mask should be True=ignore\" not in sampler_content:\n",
        "        sampler_content = sampler_content.replace(\n",
        "            \"support_mask = torch.tensor(\",\n",
        "            \"# CRITICAL: Mask should be True=ignore, False=use\\n        # So we want True when visibility == 0 (not labeled)\\n        support_mask = torch.tensor(\"\n",
        "        )\n",
        "\n",
        "    with open(EPISODIC_SAMPLER_FILE, 'w') as f:\n",
        "        f.write(sampler_content)\n",
        "    print(\"  âœ… Fix 2 applied successfully!\")\n",
        "elif \"[v == 0 for v in support_visibility]\" in sampler_content:\n",
        "    print(\"Fix 2: Correct mask convention already in episodic_sampler.py âœ…\")\n",
        "else:\n",
        "    print(\"  âš ï¸  Could not find mask pattern in episodic_sampler.py - manual check needed\")\n",
        "\n",
        "# ============================================================================\n",
        "# Verification: Check both fixes are present\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"VERIFICATION:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Re-read files and verify\n",
        "with open(GEOMETRIC_ENCODER_FILE, 'r') as f:\n",
        "    ge_content = f.read()\n",
        "\n",
        "with open(EPISODIC_SAMPLER_FILE, 'r') as f:\n",
        "    es_content = f.read()\n",
        "\n",
        "fix1_ok = \"CRITICAL SAFETY CHECK\" in ge_content or \"all_masked_per_batch\" in ge_content\n",
        "fix2_ok = \"[v == 0 for v in support_visibility]\" in es_content\n",
        "\n",
        "print(f\"  Fix 1 (Safety check in geometric_support_encoder.py): {'âœ… OK' if fix1_ok else 'âŒ MISSING'}\")\n",
        "print(f\"  Fix 2 (Mask convention in episodic_sampler.py):       {'âœ… OK' if fix2_ok else 'âŒ MISSING'}\")\n",
        "\n",
        "if fix1_ok and fix2_ok:\n",
        "    print(\"\\nâœ… All CUDA fixes applied successfully!\")\n",
        "    print(\"   Training should now work on Colab with A100/V100/T4 GPUs.\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸  Some fixes may be missing. Check the output above.\")\n",
        "    print(\"   If training fails with 'to_padded_tensor' error, manually apply the fixes.\")\n",
        "\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ULReV9hbsd5"
      },
      "source": [
        "## 3. Install Requirements\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9OT7CLHbsd6",
        "outputId": "764cf03d-06d3-4aa9-a4ed-b05f6d60a549"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing additional dependencies (descartes, shapely, etc.)...\n",
            "âœ… Additional dependencies installed!\n"
          ]
        }
      ],
      "source": [
        "# Install additional dependencies needed for plot_utils and other utilities\n",
        "# (descartes, shapely, etc. - these are in requirements.txt but not requirements_cape.txt)\n",
        "print(\"Installing additional dependencies (descartes, shapely, etc.)...\")\n",
        "!pip install -q descartes shapely>=1.8.0\n",
        "print(\"âœ… Additional dependencies installed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eglnc1Xabsd6",
        "outputId": "104bcae2-7304-4aa2-e601-f2563e640c09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing requirements from requirements_cape.txt...\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\n",
            "Installing detectron2...\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for detectron2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "âœ… All dependencies installed!\n"
          ]
        }
      ],
      "source": [
        "# Install requirements\n",
        "import os\n",
        "\n",
        "PROJECT_ROOT = \"/content/category-agnostic-pose-estimation\"\n",
        "REQUIREMENTS_FILE = os.path.join(PROJECT_ROOT, \"requirements_cape.txt\")\n",
        "\n",
        "print(\"Installing requirements from requirements_cape.txt...\")\n",
        "!cd {PROJECT_ROOT} && pip install -q -r {REQUIREMENTS_FILE}\n",
        "\n",
        "# Install detectron2 for CUDA 11.8 (Colab typically has CUDA 11.8)\n",
        "print(\"\\nInstalling detectron2...\")\n",
        "!pip install -q 'git+https://github.com/facebookresearch/detectron2.git'\n",
        "\n",
        "print(\"âœ… All dependencies installed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtoTtjcibsd6"
      },
      "source": [
        "## 4. Authenticate to GCP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7l9TBNNbsd6",
        "outputId": "38c0929e-3a9e-4a72-b682-36c1c363295d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authenticating to GCP...\n",
            "\u001b[1;33mWARNING:\u001b[0m [nk699@cornell.edu] does not have permission to access projects instance [dl-category-agnostic-pose-est] (or it may not exist): The caller does not have permission. This command is authenticated as nk699@cornell.edu which is the active account specified by the [core/account] property\n",
            "Are you sure you wish to set property [core/project] to \n",
            "dl-category-agnostic-pose-est?\n",
            "\n",
            "Do you want to continue (Y/n)?  Y\n",
            "\n",
            "Updated property [core/project].\n",
            "âœ… Authenticated to GCP project: dl-category-agnostic-pose-est\n"
          ]
        }
      ],
      "source": [
        "# Authenticate to GCP\n",
        "from google.colab import auth\n",
        "\n",
        "print(\"Authenticating to GCP...\")\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Set GCP project\n",
        "GCP_PROJECT = \"dl-category-agnostic-pose-est\"\n",
        "!gcloud config set project {GCP_PROJECT}\n",
        "\n",
        "print(f\"âœ… Authenticated to GCP project: {GCP_PROJECT}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIaksWWwbsd6"
      },
      "source": [
        "## 5. Mount GCS Bucket\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jocm_WW5bsd6",
        "outputId": "a082ea87-9df9-4695-9ba1-89e6ffc8f0cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing gcsfuse...\n",
            "deb http://packages.cloud.google.com/apt gcsfuse-jammy main\n",
            "gpg: cannot open '/dev/tty': No such device or address\n",
            "curl: (23) Failed writing body\n",
            "/usr/bin/gcsfuse\n",
            "âœ… gcsfuse installed\n",
            "Creating mount point: /content/category-agnostic-pose-estimation/Raster2Seq_internal-main/data\n",
            "âœ… Already mounted at /content/category-agnostic-pose-estimation/Raster2Seq_internal-main/data\n",
            "Mounting gs://dl-category-agnostic-pose-mp100-data to /content/category-agnostic-pose-estimation/Raster2Seq_internal-main/data...\n",
            "This may take a moment...\n",
            "Running: gcsfuse --implicit-dirs dl-category-agnostic-pose-mp100-data /content/category-agnostic-pose-estimation/Raster2Seq_internal-main/data\n",
            "Waiting for mount to initialize...\n",
            "\n",
            "Checking mount status...\n",
            "Mount log:\n",
            "\":4,\"RandomSeekThreshold\":3,\"StartBlocksPerHandle\":1},\"WorkloadInsight\":{\"ForwardMergeThresholdMb\":0,\"OutputFile\":\"\",\"Visualize\":false},\"Write\":{\"BlockSizeMb\":32,\"CreateEmptyFile\":false,\"EnableRapidAppends\":true,\"EnableStreamingWrites\":true,\"FinalizeFileForRapid\":false,\"GlobalMaxBlocks\":4,\"MaxBlocksPerFile\":1}}}\n",
            "{\"timestamp\":{\"seconds\":1764541095,\"nanos\":89610683},\"severity\":\"INFO\",\"message\":\"File system has been successfully mounted.\",\"mount-id\":\"dl-category-agnostic-pose-mp100-data-9fe4055c\"}\n",
            "\n",
            "\n",
            "Verifying bucket access with gsutil...\n",
            "gs://dl-category-agnostic-pose-mp100-data/beaver_body/\n",
            "gs://dl-category-agnostic-pose-mp100-data/bed/\n",
            "gs://dl-category-agnostic-pose-mp100-data/bighornsheep_face/\n",
            "gs://dl-category-agnostic-pose-mp100-data/bison_body/\n",
            "gs://dl-category-agnostic-pose-mp100-data/blackbuck_face/\n",
            "gs://dl-category-agnostic-pose-mp100-data/bobcat_body/\n",
            "gs://dl-category-agnostic-pose-mp100-data/bonobo_face/\n",
            "gs://dl-category-agnostic-pose-mp100-data/bus/\n",
            "gs://dl-category-agnostic-pose-mp100-data/californiansealion_face/\n",
            "gs://dl-category-agnostic-pose-mp100-data/camel_face/\n",
            "Exception ignored in: <_io.TextIOWrapper name='<stdout>' mode='w' encoding='utf-8'>\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "\n",
            "Verifying mount at: /content/category-agnostic-pose-estimation/Raster2Seq_internal-main/data\n",
            "Path exists: True\n",
            "Is mounted: True\n",
            "âœ… GCS bucket mounted successfully!\n",
            "Mount point: /content/category-agnostic-pose-estimation/Raster2Seq_internal-main/data\n",
            "Found 93 items in bucket\n",
            "   - beaver_body (directory)\n",
            "   - bed (directory)\n",
            "   - bighornsheep_face (directory)\n",
            "   - bison_body (directory)\n",
            "   - blackbuck_face (directory)\n",
            "   - bobcat_body (directory)\n",
            "   - bonobo_face (directory)\n",
            "   - bus (directory)\n",
            "   - californiansealion_face (directory)\n",
            "   - camel_face (directory)\n"
          ]
        }
      ],
      "source": [
        "# Mount GCS bucket using gcsfuse\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "PROJECT_ROOT = \"/content/category-agnostic-pose-estimation\"\n",
        "BUCKET_NAME = \"dl-category-agnostic-pose-mp100-data\"\n",
        "MOUNT_POINT = os.path.join(PROJECT_ROOT, \"Raster2Seq_internal-main\", \"data\")\n",
        "\n",
        "# Install gcsfuse from Google's official repository\n",
        "print(\"Installing gcsfuse...\")\n",
        "# Add Google's gcsfuse repository (updated method for newer Ubuntu versions)\n",
        "!export GCSFUSE_REPO=gcsfuse-`lsb_release -c -s` && \\\n",
        "echo \"deb http://packages.cloud.google.com/apt $GCSFUSE_REPO main\" | sudo tee /etc/apt/sources.list.d/gcsfuse.list && \\\n",
        "curl -fsSL https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo gpg --dearmor -o /usr/share/keyrings/cloud.google.gpg && \\\n",
        "echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt $GCSFUSE_REPO main\" | sudo tee /etc/apt/sources.list.d/gcsfuse.list && \\\n",
        "sudo apt-get update && \\\n",
        "sudo apt-get install -y gcsfuse\n",
        "\n",
        "# Verify installation\n",
        "!which gcsfuse\n",
        "print(\"âœ… gcsfuse installed\")\n",
        "\n",
        "# Create mount point directory and parent directories\n",
        "print(f\"Creating mount point: {MOUNT_POINT}\")\n",
        "os.makedirs(os.path.dirname(MOUNT_POINT), exist_ok=True)\n",
        "os.makedirs(MOUNT_POINT, exist_ok=True)\n",
        "\n",
        "# Check if already mounted\n",
        "try:\n",
        "    result = subprocess.run(['mountpoint', '-q', MOUNT_POINT], capture_output=True)\n",
        "    if result.returncode == 0:\n",
        "        print(f\"âœ… Already mounted at {MOUNT_POINT}\")\n",
        "    else:\n",
        "        # Try to unmount if exists but not properly mounted\n",
        "        try:\n",
        "            subprocess.run(['fusermount', '-u', MOUNT_POINT], capture_output=True, timeout=5)\n",
        "        except:\n",
        "            try:\n",
        "                subprocess.run(['umount', MOUNT_POINT], capture_output=True, timeout=5)\n",
        "            except:\n",
        "                pass\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Mount the bucket\n",
        "print(f\"Mounting gs://{BUCKET_NAME} to {MOUNT_POINT}...\")\n",
        "print(\"This may take a moment...\")\n",
        "\n",
        "# Run gcsfuse in background\n",
        "# Note: In Colab, we need to run gcsfuse in background using shell &\n",
        "print(f\"Running: gcsfuse --implicit-dirs {BUCKET_NAME} {MOUNT_POINT}\")\n",
        "!nohup gcsfuse --implicit-dirs {BUCKET_NAME} {MOUNT_POINT} > /tmp/gcsfuse.log 2>&1 &\n",
        "\n",
        "# Wait a moment for mount to initialize\n",
        "print(\"Waiting for mount to initialize...\")\n",
        "time.sleep(8)  # Give it more time to mount\n",
        "\n",
        "# Check mount status\n",
        "print(\"\\nChecking mount status...\")\n",
        "# Check mount log for errors\n",
        "if os.path.exists(\"/tmp/gcsfuse.log\"):\n",
        "    with open(\"/tmp/gcsfuse.log\", \"r\") as f:\n",
        "        log_content = f.read()\n",
        "        if log_content:\n",
        "            print(\"Mount log:\")\n",
        "            print(log_content[-500:])  # Last 500 chars\n",
        "        else:\n",
        "            print(\"Mount log is empty (mount might still be initializing)\")\n",
        "\n",
        "# Also verify we can access the bucket directly with gsutil\n",
        "print(\"\\nVerifying bucket access with gsutil...\")\n",
        "!gsutil ls gs://{BUCKET_NAME}/ | head -10\n",
        "\n",
        "# Verify mount\n",
        "print(f\"\\nVerifying mount at: {MOUNT_POINT}\")\n",
        "print(f\"Path exists: {os.path.exists(MOUNT_POINT)}\")\n",
        "\n",
        "# Check if actually mounted using mountpoint command\n",
        "try:\n",
        "    result = subprocess.run(['mountpoint', '-q', MOUNT_POINT], capture_output=True)\n",
        "    is_mounted = (result.returncode == 0)\n",
        "    print(f\"Is mounted: {is_mounted}\")\n",
        "except:\n",
        "    # Fallback: check mount table\n",
        "    result = subprocess.run(['mount'], capture_output=True, text=True)\n",
        "    is_mounted = MOUNT_POINT in result.stdout\n",
        "    print(f\"Is mounted (from mount table): {is_mounted}\")\n",
        "\n",
        "if os.path.exists(MOUNT_POINT) and is_mounted:\n",
        "    try:\n",
        "        # Try to list contents\n",
        "        items = os.listdir(MOUNT_POINT)\n",
        "        if len(items) > 0:\n",
        "            print(f\"âœ… GCS bucket mounted successfully!\")\n",
        "            print(f\"Mount point: {MOUNT_POINT}\")\n",
        "            print(f\"Found {len(items)} items in bucket\")\n",
        "            # List a few items to verify\n",
        "            for item in items[:10]:\n",
        "                item_path = os.path.join(MOUNT_POINT, item)\n",
        "                item_type = \"directory\" if os.path.isdir(item_path) else \"file\"\n",
        "                print(f\"   - {item} ({item_type})\")\n",
        "        else:\n",
        "            print(f\"âš ï¸  Mount point exists but is empty (0 items)\")\n",
        "            print(f\"   This might indicate:\")\n",
        "            print(f\"   1. Bucket is empty\")\n",
        "            print(f\"   2. Mount didn't work correctly\")\n",
        "            print(f\"   3. Permission issues\")\n",
        "    except PermissionError as e:\n",
        "        print(f\"âš ï¸  Permission error accessing mount: {e}\")\n",
        "        print(\"   Mount might still be initializing, wait a moment and try again\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸  Mount point exists but cannot list contents: {e}\")\n",
        "        print(\"   This might indicate a mount issue\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "elif os.path.exists(MOUNT_POINT) and not is_mounted:\n",
        "    print(f\"âš ï¸  Directory exists but is not mounted\")\n",
        "    print(f\"   The directory exists but gcsfuse mount is not active\")\n",
        "    print(f\"   Trying to mount again...\")\n",
        "    # Try mounting again\n",
        "    !nohup gcsfuse --implicit-dirs {BUCKET_NAME} {MOUNT_POINT} > /tmp/gcsfuse.log 2>&1 &\n",
        "    time.sleep(5)\n",
        "    # Re-check\n",
        "    items = os.listdir(MOUNT_POINT) if os.path.exists(MOUNT_POINT) else []\n",
        "    if len(items) > 0:\n",
        "        print(f\"âœ… Mount successful after retry! Found {len(items)} items\")\n",
        "    else:\n",
        "        print(f\"âŒ Mount still not working\")\n",
        "else:\n",
        "    print(\"âŒ Failed to mount GCS bucket\")\n",
        "    print(f\"   Mount point: {MOUNT_POINT}\")\n",
        "    print(f\"   Check:\")\n",
        "    print(f\"   1. GCP authentication (run the GCP auth cell)\")\n",
        "    print(f\"   2. Bucket name is correct: {BUCKET_NAME}\")\n",
        "    print(f\"   3. You have read access to the bucket\")\n",
        "    print(f\"   4. Check mount log: /tmp/gcsfuse.log\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-7Hx-_cbsd7"
      },
      "source": [
        "## 6. Create Data Symlink\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNKCPxb7bsd7",
        "outputId": "c049fe2e-4abd-48b6-8309-17e3332119c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking mount point: /content/category-agnostic-pose-estimation/Raster2Seq_internal-main/data\n",
            "  Exists: True\n",
            "  Is directory: True\n",
            "  Can list contents: Yes (93 items)\n",
            "Removing existing symlink: /content/category-agnostic-pose-estimation/data\n",
            "\n",
            "Creating symlink:\n",
            "  From: /content/category-agnostic-pose-estimation/data\n",
            "  To: /content/category-agnostic-pose-estimation/Raster2Seq_internal-main/data\n",
            "âœ… Created symlink: /content/category-agnostic-pose-estimation/data -> /content/category-agnostic-pose-estimation/Raster2Seq_internal-main/data\n",
            "âœ… Symlink verified: /content/category-agnostic-pose-estimation/data\n",
            "  Is symlink: True\n",
            "âœ… Can access 93 items through symlink\n",
            "   First 5 items: ['beaver_body', 'bed', 'bighornsheep_face', 'bison_body', 'blackbuck_face']\n",
            "\n",
            "================================================================================\n",
            "Creating Annotations Symlink\n",
            "================================================================================\n",
            "Checking mounted annotations: /content/category-agnostic-pose-estimation/Raster2Seq_internal-main/annotations\n",
            "  Exists: False\n",
            "âš ï¸  Mounted annotations not found at /content/category-agnostic-pose-estimation/Raster2Seq_internal-main/annotations\n",
            "   This might be OK if annotations are in a different location\n",
            "   Training will look for annotations in: /content/category-agnostic-pose-estimation/annotations\n"
          ]
        }
      ],
      "source": [
        "# Create symlinks from data and annotations to mounted GCS bucket (as expected by START_TRAINING.sh)\n",
        "import os\n",
        "\n",
        "PROJECT_ROOT = \"/content/category-agnostic-pose-estimation\"\n",
        "MOUNTED_DATA = os.path.join(PROJECT_ROOT, \"Raster2Seq_internal-main\", \"data\")\n",
        "MOUNTED_ANNOTATIONS = os.path.join(PROJECT_ROOT, \"Raster2Seq_internal-main\", \"annotations\")\n",
        "DATA_SYMLINK = os.path.join(PROJECT_ROOT, \"data\")\n",
        "ANNOTATIONS_SYMLINK = os.path.join(PROJECT_ROOT, \"annotations\")\n",
        "\n",
        "print(f\"Checking mount point: {MOUNTED_DATA}\")\n",
        "print(f\"  Exists: {os.path.exists(MOUNTED_DATA)}\")\n",
        "if os.path.exists(MOUNTED_DATA):\n",
        "    print(f\"  Is directory: {os.path.isdir(MOUNTED_DATA)}\")\n",
        "    try:\n",
        "        items = os.listdir(MOUNTED_DATA)\n",
        "        print(f\"  Can list contents: Yes ({len(items)} items)\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Can list contents: No ({e})\")\n",
        "\n",
        "# Remove existing symlink or directory if it exists\n",
        "if os.path.exists(DATA_SYMLINK):\n",
        "    if os.path.islink(DATA_SYMLINK):\n",
        "        print(f\"Removing existing symlink: {DATA_SYMLINK}\")\n",
        "        os.unlink(DATA_SYMLINK)\n",
        "    elif os.path.isdir(DATA_SYMLINK):\n",
        "        print(f\"Warning: {DATA_SYMLINK} exists as a directory (not a symlink)\")\n",
        "        print(\"   Removing it to create symlink...\")\n",
        "        import shutil\n",
        "        shutil.rmtree(DATA_SYMLINK)\n",
        "    else:\n",
        "        print(f\"Warning: {DATA_SYMLINK} exists and is not a symlink or directory\")\n",
        "        os.remove(DATA_SYMLINK)\n",
        "\n",
        "# Create symlink\n",
        "if os.path.exists(MOUNTED_DATA) and os.path.isdir(MOUNTED_DATA):\n",
        "    try:\n",
        "        # Use absolute path for symlink target\n",
        "        MOUNTED_DATA_ABS = os.path.abspath(MOUNTED_DATA)\n",
        "        print(f\"\\nCreating symlink:\")\n",
        "        print(f\"  From: {DATA_SYMLINK}\")\n",
        "        print(f\"  To: {MOUNTED_DATA_ABS}\")\n",
        "        os.symlink(MOUNTED_DATA_ABS, DATA_SYMLINK)\n",
        "        print(f\"âœ… Created symlink: {DATA_SYMLINK} -> {MOUNTED_DATA_ABS}\")\n",
        "\n",
        "        # Verify symlink\n",
        "        if os.path.exists(DATA_SYMLINK):\n",
        "            print(f\"âœ… Symlink verified: {DATA_SYMLINK}\")\n",
        "            print(f\"  Is symlink: {os.path.islink(DATA_SYMLINK)}\")\n",
        "            # Try to list contents through symlink\n",
        "            try:\n",
        "                items = os.listdir(DATA_SYMLINK)\n",
        "                print(f\"âœ… Can access {len(items)} items through symlink\")\n",
        "                print(f\"   First 5 items: {items[:5]}\")\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸  Symlink exists but cannot access contents: {e}\")\n",
        "        else:\n",
        "            print(f\"âŒ Symlink creation failed - path does not exist after creation\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error creating symlink: {e}\")\n",
        "        print(f\"   Source: {MOUNTED_DATA}\")\n",
        "        print(f\"   Target: {DATA_SYMLINK}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    print(f\"âŒ Mounted data not found at {MOUNTED_DATA}\")\n",
        "    print(f\"   Please check that GCS bucket is mounted correctly\")\n",
        "    print(f\"   Run the mount cell above and check for errors\")\n",
        "    print(f\"   Mount point should exist and be accessible\")\n",
        "\n",
        "# ============================================================================\n",
        "# Create annotations symlink\n",
        "# ============================================================================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Creating Annotations Symlink\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"Checking mounted annotations: {MOUNTED_ANNOTATIONS}\")\n",
        "print(f\"  Exists: {os.path.exists(MOUNTED_ANNOTATIONS)}\")\n",
        "if os.path.exists(MOUNTED_ANNOTATIONS):\n",
        "    print(f\"  Is directory: {os.path.isdir(MOUNTED_ANNOTATIONS)}\")\n",
        "    try:\n",
        "        items = os.listdir(MOUNTED_ANNOTATIONS)\n",
        "        print(f\"  Can list contents: Yes ({len(items)} items)\")\n",
        "        if items:\n",
        "            print(f\"   First 5 items: {items[:5]}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Can list contents: No ({e})\")\n",
        "\n",
        "# Remove existing symlink or directory if it exists\n",
        "if os.path.exists(ANNOTATIONS_SYMLINK):\n",
        "    if os.path.islink(ANNOTATIONS_SYMLINK):\n",
        "        print(f\"Removing existing symlink: {ANNOTATIONS_SYMLINK}\")\n",
        "        os.unlink(ANNOTATIONS_SYMLINK)\n",
        "    elif os.path.isdir(ANNOTATIONS_SYMLINK):\n",
        "        print(f\"Warning: {ANNOTATIONS_SYMLINK} exists as a directory (not a symlink)\")\n",
        "        print(\"   Removing it to create symlink...\")\n",
        "        import shutil\n",
        "        shutil.rmtree(ANNOTATIONS_SYMLINK)\n",
        "    else:\n",
        "        print(f\"Warning: {ANNOTATIONS_SYMLINK} exists and is not a symlink or directory\")\n",
        "        os.remove(ANNOTATIONS_SYMLINK)\n",
        "\n",
        "# Create symlink\n",
        "if os.path.exists(MOUNTED_ANNOTATIONS) and os.path.isdir(MOUNTED_ANNOTATIONS):\n",
        "    try:\n",
        "        # Use absolute path for symlink target\n",
        "        MOUNTED_ANNOTATIONS_ABS = os.path.abspath(MOUNTED_ANNOTATIONS)\n",
        "        print(f\"\\nCreating annotations symlink:\")\n",
        "        print(f\"  From: {ANNOTATIONS_SYMLINK}\")\n",
        "        print(f\"  To: {MOUNTED_ANNOTATIONS_ABS}\")\n",
        "        os.symlink(MOUNTED_ANNOTATIONS_ABS, ANNOTATIONS_SYMLINK)\n",
        "        print(f\"âœ… Created symlink: {ANNOTATIONS_SYMLINK} -> {MOUNTED_ANNOTATIONS_ABS}\")\n",
        "\n",
        "        # Verify symlink\n",
        "        if os.path.exists(ANNOTATIONS_SYMLINK):\n",
        "            print(f\"âœ… Annotations symlink verified: {ANNOTATIONS_SYMLINK}\")\n",
        "            print(f\"  Is symlink: {os.path.islink(ANNOTATIONS_SYMLINK)}\")\n",
        "            # Try to list contents through symlink\n",
        "            try:\n",
        "                items = os.listdir(ANNOTATIONS_SYMLINK)\n",
        "                print(f\"âœ… Can access {len(items)} annotation files through symlink\")\n",
        "                # Check for expected files\n",
        "                expected_files = [\"mp100_split1_train.json\", \"mp100_split1_test.json\"]\n",
        "                for exp_file in expected_files:\n",
        "                    if exp_file in items:\n",
        "                        print(f\"   âœ“ Found: {exp_file}\")\n",
        "                    else:\n",
        "                        print(f\"   âš ï¸  Missing: {exp_file}\")\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸  Symlink exists but cannot access contents: {e}\")\n",
        "        else:\n",
        "            print(f\"âŒ Annotations symlink creation failed - path does not exist after creation\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error creating annotations symlink: {e}\")\n",
        "        print(f\"   Source: {MOUNTED_ANNOTATIONS}\")\n",
        "        print(f\"   Target: {ANNOTATIONS_SYMLINK}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    print(f\"âš ï¸  Mounted annotations not found at {MOUNTED_ANNOTATIONS}\")\n",
        "    print(f\"   This might be OK if annotations are in a different location\")\n",
        "    print(f\"   Training will look for annotations in: {ANNOTATIONS_SYMLINK}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "77sWyC00bsd8"
      },
      "outputs": [],
      "source": [
        "## 7. Run Fast Training (1 Epoch, Split 1)\n",
        "\n",
        "# This section trains the CAPE model for a quick test run.\n",
        "\n",
        "# **Training Configuration:**\n",
        "# - Full episodic training mode\n",
        "# - Epochs: 1 (fast test run)\n",
        "# - Split: 1 (default)\n",
        "# - Episodes per epoch: 500 (reduced for speed)\n",
        "# - Batch size: 10\n",
        "# - Queries per episode: 2\n",
        "# - All logs will be saved to `training_logs.txt`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8187U9Mbsd8"
      },
      "source": [
        "# Configure full training parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtldOllGbsd8",
        "outputId": "34d61188-b650-4737-b3f8-2180d85e3553"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounting Google Drive for checkpoint persistence...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "\n",
            "================================================================================\n",
            "Fast CAPE Training Configuration (1 Epoch, Split 1)\n",
            "================================================================================\n",
            "Mode:              Fast test run (1 epoch)\n",
            "\n",
            "ğŸ“ STORAGE (Google Drive - Persistent):\n",
            "  Output directory:  /content/drive/MyDrive/cape_training_output\n",
            "  Log file:          /content/drive/MyDrive/cape_training_output/training_logs.txt\n",
            "\n",
            "ğŸ’¡ All files will be saved to Google Drive\n",
            "   You can access them after the Colab session ends\n",
            "\n",
            "Training configuration:\n",
            "  - Epochs: 1 (fast test run)\n",
            "  - Split: 1 (default)\n",
            "  - Batch size: 10\n",
            "  - Episodes per epoch: 500 (reduced for speed)\n",
            "  - Learning rate: 1e-4 (backbone: 1e-5)\n",
            "\n",
            "Non-default settings:\n",
            "  - Geometric support encoder: ENABLED\n",
            "  - GCN pre-encoding: ENABLED\n",
            "  - Classification loss weight: 2.0 (default: 1.0)\n",
            "  - Fixed validation episodes: ENABLED (stable curves)\n",
            "  - GPU optimizations: Mixed precision + cuDNN tuning\n",
            "================================================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Configure output directories\n",
        "import os\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "\n",
        "PROJECT_ROOT = \"/content/category-agnostic-pose-estimation\"\n",
        "\n",
        "# Mount Google Drive for persistent checkpoint storage\n",
        "print(\"Mounting Google Drive for checkpoint persistence...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Output directory - save to Google Drive\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/cape_training_output\"\n",
        "LOG_FILE = os.path.join(OUTPUT_DIR, \"training_logs.txt\")\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Fast CAPE Training Configuration (1 Epoch, Split 1)\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Mode:              Fast test run (1 epoch)\")\n",
        "print(f\"\\nğŸ“ STORAGE (Google Drive - Persistent):\")\n",
        "print(f\"  Output directory:  {OUTPUT_DIR}\")\n",
        "print(f\"  Log file:          {LOG_FILE}\")\n",
        "print(f\"\\nğŸ’¡ All files will be saved to Google Drive\")\n",
        "print(f\"   You can access them after the Colab session ends\")\n",
        "print(f\"\\nTraining configuration:\")\n",
        "print(f\"  - Epochs: 1 (fast test run)\")\n",
        "print(f\"  - Split: 1 (default)\")\n",
        "print(f\"  - Batch size: 10\")\n",
        "print(f\"  - Episodes per epoch: 500 (reduced for speed)\")\n",
        "print(f\"  - Learning rate: 1e-4 (backbone: 1e-5)\")\n",
        "print(f\"\\nNon-default settings:\")\n",
        "print(f\"  - Geometric support encoder: ENABLED\")\n",
        "print(f\"  - GCN pre-encoding: ENABLED\")\n",
        "print(f\"  - Classification loss weight: 2.0 (default: 1.0)\")\n",
        "print(f\"  - Fixed validation episodes: ENABLED (stable curves)\")\n",
        "print(f\"  - GPU optimizations: Mixed precision + cuDNN tuning\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nr7UuYDQ6J9_",
        "outputId": "1311ef3b-d9ed-48fa-c8a9-5777aedf193b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Pulling Latest Code from GitHub\n",
            "================================================================================\n",
            "Repository: /content/category-agnostic-pose-estimation\n",
            "Branch: main\n",
            "\n",
            "âš ï¸  Git pull had issues (return code: 1)\n",
            "Error: From https://github.com/nkkrnkl/category-agnostic-pose-estimation\n",
            " * branch            main       -> FETCH_HEAD\n",
            "error: Your local changes to the following files would be overwritten by merge:\n",
            "\tdatasets/episodic_sampler.py\n",
            "Please commit your changes or stash them before you merge.\n",
            "Aborting\n",
            "\n",
            "\n",
            "Continuing anyway - using existing code...\n",
            "\n",
            "================================================================================\n",
            "Ready to proceed with training\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Ensure we have the latest code before training\n",
        "# This cell stashes local changes, pulls latest code, then reapplies CUDA fixes\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "PROJECT_ROOT = \"/content/category-agnostic-pose-estimation\"\n",
        "BRANCH = \"main\"  # Use main branch\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"Pulling Latest Code from GitHub\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Repository: {PROJECT_ROOT}\")\n",
        "print(f\"Branch: {BRANCH}\")\n",
        "print()\n",
        "\n",
        "# Change to project directory\n",
        "os.chdir(PROJECT_ROOT)\n",
        "\n",
        "# Check for local changes\n",
        "result = subprocess.run(\n",
        "    ['git', 'status', '--porcelain'],\n",
        "    capture_output=True,\n",
        "    text=True,\n",
        "    timeout=10\n",
        ")\n",
        "\n",
        "if result.stdout.strip():\n",
        "    print(\"âš ï¸  Local changes detected. Stashing them before pull...\")\n",
        "    print(\"   (These will be reapplied after pull if needed)\")\n",
        "    stash_result = subprocess.run(\n",
        "        ['git', 'stash'],\n",
        "        capture_output=True,\n",
        "        text=True,\n",
        "        timeout=10\n",
        "    )\n",
        "    if stash_result.returncode == 0:\n",
        "        print(\"âœ… Local changes stashed\")\n",
        "    else:\n",
        "        print(f\"âš ï¸  Stash had issues: {stash_result.stderr}\")\n",
        "\n",
        "# Pull latest changes\n",
        "try:\n",
        "    result = subprocess.run(\n",
        "        ['git', 'pull', 'origin', BRANCH],\n",
        "        capture_output=True,\n",
        "        text=True,\n",
        "        timeout=30\n",
        "    )\n",
        "    \n",
        "    if result.returncode == 0:\n",
        "        print(\"âœ… Successfully pulled latest code!\")\n",
        "        if result.stdout.strip():\n",
        "            print(\"\\nChanges:\")\n",
        "            print(result.stdout)\n",
        "    else:\n",
        "        print(f\"âš ï¸  Git pull had issues (return code: {result.returncode})\")\n",
        "        if result.stderr:\n",
        "            print(f\"Error: {result.stderr}\")\n",
        "        print(\"\\nContinuing anyway - using existing code...\")\n",
        "except subprocess.TimeoutExpired:\n",
        "    print(\"âš ï¸  Git pull timed out - continuing with existing code...\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸  Error during git pull: {e}\")\n",
        "    print(\"Continuing with existing code...\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Ready to proceed with training\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nğŸ’¡ Note: CUDA fixes will be reapplied in the next cell if needed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g1HI8BiQFDqA"
      },
      "source": [
        "# Run Training with Logging\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSTx6StbXe2t",
        "outputId": "5f72d369-1d12-49af-fac6-abb336ef0d9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Verifying dataset for full training...\n",
            "Data directory: /content/category-agnostic-pose-estimation/data\n",
            "Annotation file: /content/category-agnostic-pose-estimation/data/cleaned_annotations/mp100_split1_train.json\n",
            "\n",
            "âœ… Training annotations loaded:\n",
            "   Images: 12816\n",
            "   Annotations: 13712\n",
            "   Categories: 70\n",
            "âœ… Data directory accessible: 93 items found\n",
            "\n",
            "âœ… Dataset verification complete. Ready to train on full MP-100 dataset.\n"
          ]
        }
      ],
      "source": [
        "# Verify data is accessible before training\n",
        "import os\n",
        "import json\n",
        "\n",
        "PROJECT_ROOT = \"/content/category-agnostic-pose-estimation\"\n",
        "DATA_DIR = os.path.join(PROJECT_ROOT, \"data\")\n",
        "ANNOTATION_FILE = '/content/category-agnostic-pose-estimation/data/cleaned_annotations/mp100_split1_train.json'\n",
        "\n",
        "print(\"Verifying dataset for full training...\")\n",
        "print(f\"Data directory: {DATA_DIR}\")\n",
        "print(f\"Annotation file: {ANNOTATION_FILE}\")\n",
        "print()\n",
        "\n",
        "# Verify annotations exist\n",
        "if os.path.exists(ANNOTATION_FILE):\n",
        "    with open(ANNOTATION_FILE, 'r') as f:\n",
        "        coco_data = json.load(f)\n",
        "\n",
        "    num_images = len(coco_data.get('images', []))\n",
        "    num_annotations = len(coco_data.get('annotations', []))\n",
        "    num_categories = len(coco_data.get('categories', []))\n",
        "\n",
        "    print(f\"âœ… Training annotations loaded:\")\n",
        "    print(f\"   Images: {num_images}\")\n",
        "    print(f\"   Annotations: {num_annotations}\")\n",
        "    print(f\"   Categories: {num_categories}\")\n",
        "else:\n",
        "    print(f\"âŒ Annotation file not found: {ANNOTATION_FILE}\")\n",
        "    print(\"Training will fail!\")\n",
        "\n",
        "# Verify data directory is accessible\n",
        "if os.path.exists(DATA_DIR) and os.path.islink(DATA_DIR):\n",
        "    items = os.listdir(DATA_DIR)\n",
        "    print(f\"âœ… Data directory accessible: {len(items)} items found\")\n",
        "else:\n",
        "    print(f\"âŒ Data directory not accessible: {DATA_DIR}\")\n",
        "    print(\"Training will fail!\")\n",
        "\n",
        "print(\"\\nâœ… Dataset verification complete. Ready to train on full MP-100 dataset.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zem_FhAMbsd8"
      },
      "outputs": [],
      "source": [
        "## 8. Check Training Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtC2wBB04ouj",
        "outputId": "318b8360-4cfb-483c-8d0a-c89ea631e16f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Starting CAPE Full Training\n",
            "================================================================================\n",
            "Output directory: /content/drive/MyDrive/cape_training_output\n",
            "Logging to: /content/drive/MyDrive/cape_training_output/training_logs.txt\n",
            "\n",
            "Training parameters:\n",
            "  - Epochs: 1 (fast test run)\n",
            "  - Split: 1\n",
            "  - Batch size: 10 (with accumulation_steps=4 â†’ effective=40)\n",
            "  - Episodes per epoch: 500 (train), 200 (val)\n",
            "  - Learning rate: 1e-4 (backbone: 1e-5)\n",
            "\n",
            "Validation stability:\n",
            "  - Fixed validation episodes: YES\n",
            "  - Same 200 episodes reused each epoch for reproducible curves\n",
            "\n",
            "GPU optimizations enabled:\n",
            "  - Mixed precision (AMP)\n",
            "  - cuDNN auto-tuning\n",
            "================================================================================\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/content/category-agnostic-pose-estimation/models/train_cape_episodic.py\", line 1002, in <module>\n",
            "    main(args)\n",
            "  File \"/content/category-agnostic-pose-estimation/models/train_cape_episodic.py\", line 397, in main\n",
            "    from datasets.episodic_sampler import build_episodic_dataloader\n",
            "  File \"/content/category-agnostic-pose-estimation/datasets/episodic_sampler.py\", line 205\n",
            "    support_skeleton = support_data.get('skeleton', [])\n",
            "IndentationError: unexpected indent\n",
            "================================================================================\n",
            "Category-Agnostic Pose Estimation (CAPE) - Episodic Training\n",
            "================================================================================\n",
            "\n",
            "Mode: Episodic meta-learning with support pose graphs\n",
            "Support encoder: Geometric (CapeX-inspired)\n",
            "  - GCN pre-encoding: Enabled\n",
            "  - GCN layers: 2\n",
            "Support encoder layers: 3\n",
            "Fusion method: cross_attention\n",
            "Queries per episode: 2\n",
            "Train episodes per epoch: 500\n",
            "Val episodes per epoch: 200\n",
            "Fixed validation episodes: YES (seed=42) - stable curves\n",
            "\n",
            "Using device: cuda:0\n",
            "  GPU: NVIDIA A100-SXM4-80GB\n",
            "  CUDA Version: 12.6\n",
            "  GPU Memory: 79.32 GB\n",
            "  cuDNN benchmark: Enabled (auto-tuning convolution algorithms)\n",
            "  Mixed Precision (AMP): Enabled (FP16/FP32 training)\n",
            "\n",
            "Using annotations: /content/category-agnostic-pose-estimation/data/cleaned_annotations/mp100_split1_train.json\n",
            "loading annotations into memory...\n",
            "Done (t=0.27s)\n",
            "creating index...\n",
            "index created!\n",
            "ğŸ“Š Multi-instance statistics:\n",
            "   - Images with multiple instances: 615/12816 (4.8%)\n",
            "   - Total instances available: 13712\n",
            "   - Instances actually used: 12816 (93.5%)\n",
            "   - Instances skipped: 896 (6.5%)\n",
            "   - Max instances in single image: 9\n",
            "   âš ï¸  Note: Currently using only first instance per image\n",
            "Loaded MP-100 train dataset: 12816 images\n",
            "Using annotations: /content/category-agnostic-pose-estimation/data/cleaned_annotations/mp100_split1_val.json\n",
            "loading annotations into memory...\n",
            "Done (t=0.03s)\n",
            "creating index...\n",
            "index created!\n",
            "ğŸ“Š Multi-instance statistics:\n",
            "   - Images with multiple instances: 62/1703 (3.6%)\n",
            "   - Total instances available: 1795\n",
            "   - Instances actually used: 1703 (94.9%)\n",
            "   - Instances skipped: 92 (5.1%)\n",
            "   - Max instances in single image: 8\n",
            "   âš ï¸  Note: Currently using only first instance per image\n",
            "Loaded MP-100 val dataset: 1703 images\n",
            "Tokenizer: <datasets.discrete_tokenizer.DiscreteTokenizerV2 object at 0x7ddba03dc0b0>\n",
            "  vocab_size: 1940\n",
            "  num_bins: 44\n",
            "\n",
            "Building base Raster2Seq model...\n",
            "Building CAPE-specific loss criterion...\n",
            "âœ“ CAPE criterion: EOS token weight = 20.0Ã— (to combat class imbalance)\n",
            "âœ“ CAPE criterion created with visibility masking support\n",
            "Wrapping with CAPE support conditioning...\n",
            "âœ“ Model moved to device: cuda:0\n",
            "Total trainable parameters: 47,973,876\n",
            "Creating episodic dataloaders...\n",
            "\n",
            "================================================================================\n",
            "âŒ Training failed with return code: 1\n",
            "Check logs at: /content/drive/MyDrive/cape_training_output/training_logs.txt\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Run full CAPE training with logging\n",
        "import subprocess\n",
        "import sys\n",
        "from datetime import datetime\n",
        "\n",
        "PROJECT_ROOT = \"/content/category-agnostic-pose-estimation\"\n",
        "\n",
        "# Build training command - only non-default flags\n",
        "cmd = [\n",
        "    sys.executable, \"-m\", \"models.train_cape_episodic\",\n",
        "\n",
        "    # Required paths (Colab-specific)\n",
        "    \"--dataset_root\", PROJECT_ROOT,\n",
        "    \"--output_dir\", OUTPUT_DIR,\n",
        "\n",
        "    # Architecture improvements (better performance)\n",
        "    \"--use_geometric_encoder\",  # Use CapeX-inspired geometric encoder\n",
        "    \"--use_gcn_preenc\",          # Add GCN pre-processing for better keypoint relationships\n",
        "\n",
        "    # Loss tuning (faster convergence)\n",
        "    \"--cls_loss_coef\", \"2.0\",    # Weight token classification loss 2x (helps sequence learning)\n",
        "\n",
        "    # Validation stability (reproducible metrics across epochs)\n",
        "    \"--fixed_val_episodes\",      # Cache validation episodes for stable curves\n",
        "\n",
        "    # GPU optimizations (2x faster on Colab)\n",
        "    \"--use_amp\",                 # Mixed precision training (FP16/FP32)\n",
        "    \"--cudnn_benchmark\",         # Auto-tune cuDNN convolutions\n",
        "\n",
        "    # Logging\n",
        "    \"--print_freq\", \"10\",        # Print stats every 10 batches\n",
        "    \"--early_stopping_patience\", \"300\",\n",
        "    \"--accumulation_steps\", \"4\",\n",
        "    \"--episodes_per_epoch\", \"500\",\n",
        "    \"--batch_size\", \"10\",\n",
        "    \"--epochs\", \"1\",             # Fast test: 1 epoch\n",
        "    \"--mp100_split\", \"1\",        # Use split 1\n",
        "    \"--val_seed\", \"42\"\n",
        "]\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"Starting CAPE Full Training\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")\n",
        "print(f\"Logging to: {LOG_FILE}\")\n",
        "print(f\"\\nTraining parameters:\")\n",
        "print(f\"  - Epochs: 1 (fast test run)\")\n",
        "print(f\"  - Split: 1\")\n",
        "print(f\"  - Batch size: 10 (with accumulation_steps=4 â†’ effective=40)\")\n",
        "print(f\"  - Episodes per epoch: 500 (train), 200 (val)\")\n",
        "print(f\"  - Learning rate: 1e-4 (backbone: 1e-5)\")\n",
        "print(f\"\\nValidation stability:\")\n",
        "print(f\"  - Fixed validation episodes: YES\")\n",
        "print(f\"  - Same 200 episodes reused each epoch for reproducible curves\")\n",
        "print(f\"\\nGPU optimizations enabled:\")\n",
        "print(f\"  - Mixed precision (AMP)\")\n",
        "print(f\"  - cuDNN auto-tuning\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "# Change to project directory\n",
        "os.chdir(PROJECT_ROOT)\n",
        "\n",
        "# Run training with logging to both stdout and file\n",
        "with open(LOG_FILE, 'w') as log_file:\n",
        "    # Write header to log file\n",
        "    log_file.write(\"=\" * 80 + \"\\n\")\n",
        "    log_file.write(f\"CAPE Full Training Log\\n\")\n",
        "    log_file.write(f\"Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "    log_file.write(f\"Using default training parameters (see train_cape_episodic.py)\\n\")\n",
        "    log_file.write(f\"Non-default flags: geometric_encoder, gcn_preenc, cls_loss_coef=2.0, fixed_val_episodes\\n\")\n",
        "    log_file.write(f\"Validation: 200 fixed episodes per epoch (stable curves)\\n\")\n",
        "    log_file.write(f\"GPU optimizations: AMP, cuDNN benchmark\\n\")\n",
        "    log_file.write(\"=\" * 80 + \"\\n\\n\")\n",
        "    log_file.flush()\n",
        "\n",
        "    # Run process and stream output to both stdout and file\n",
        "    process = subprocess.Popen(\n",
        "        cmd,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        universal_newlines=True,\n",
        "        bufsize=1\n",
        "    )\n",
        "\n",
        "    # Stream output in real-time\n",
        "    for line in process.stdout:\n",
        "        print(line, end='')  # Print to notebook\n",
        "        log_file.write(line)  # Write to log file\n",
        "        log_file.flush()  # Ensure immediate write\n",
        "\n",
        "    # Wait for process to complete\n",
        "    return_code = process.wait()\n",
        "\n",
        "    # Write footer to log file\n",
        "    log_file.write(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
        "    log_file.write(f\"Training completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "    log_file.write(f\"Return code: {return_code}\\n\")\n",
        "    log_file.write(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "if return_code == 0:\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"âœ… Training completed successfully!\")\n",
        "    print(f\"Checkpoints saved to: {OUTPUT_DIR}\")\n",
        "    print(f\"Full logs saved to: {LOG_FILE}\")\n",
        "\n",
        "    # Create training summary file for easy recovery\n",
        "    import glob\n",
        "    summary_file = os.path.join(OUTPUT_DIR, \"TRAINING_SUMMARY.txt\")\n",
        "    with open(summary_file, 'w') as f:\n",
        "        f.write(\"=\" * 80 + \"\\n\")\n",
        "        f.write(\"CAPE FULL TRAINING - RESULTS SUMMARY\\n\")\n",
        "        f.write(\"=\" * 80 + \"\\n\\n\")\n",
        "        f.write(f\"Training completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "        f.write(f\"Location: Google Drive/cape_training_output/\\n\\n\")\n",
        "\n",
        "        # Count checkpoints\n",
        "        all_ckpts = glob.glob(os.path.join(OUTPUT_DIR, \"checkpoint_e*.pth\"))\n",
        "        best_ckpts = glob.glob(os.path.join(OUTPUT_DIR, \"checkpoint_best_pck*.pth\"))\n",
        "\n",
        "        f.write(f\"CHECKPOINTS:\\n\")\n",
        "        f.write(f\"  Total epoch checkpoints: {len(all_ckpts)}\\n\")\n",
        "        f.write(f\"  Best PCK checkpoints: {len(best_ckpts)}\\n\\n\")\n",
        "\n",
        "        if best_ckpts:\n",
        "            # Find the actual best checkpoint (highest PCK)\n",
        "            best_ckpt = sorted(best_ckpts)[-1]  # Last one alphabetically has highest PCK\n",
        "            f.write(f\"BEST MODEL:\\n\")\n",
        "            f.write(f\"  {os.path.basename(best_ckpt)}\\n\\n\")\n",
        "\n",
        "        f.write(f\"FILES IN THIS DIRECTORY:\\n\")\n",
        "        f.write(f\"  - checkpoint_e***.pth         : Per-epoch checkpoints\\n\")\n",
        "        f.write(f\"  - checkpoint_best_pck_*.pth   : Best validation PCK checkpoints\\n\")\n",
        "        f.write(f\"  - training_logs.txt           : Complete training logs\\n\")\n",
        "        f.write(f\"  - TRAINING_SUMMARY.txt        : This file\\n\\n\")\n",
        "\n",
        "        f.write(f\"TO EVALUATE A CHECKPOINT:\\n\")\n",
        "        f.write(f\"  1. Download checkpoint from Google Drive\\n\")\n",
        "        f.write(f\"  2. Run: python scripts/eval_cape_checkpoint.py \\\\\\n\")\n",
        "        f.write(f\"             --checkpoint <path_to_checkpoint.pth> \\\\\\n\")\n",
        "        f.write(f\"             --dataset_root . \\\\\\n\")\n",
        "        f.write(f\"             --split test\\n\\n\")\n",
        "\n",
        "        f.write(f\"GOOGLE DRIVE PATH:\\n\")\n",
        "        f.write(f\"  {OUTPUT_DIR}\\n\\n\")\n",
        "        f.write(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "    print(f\"Training summary saved to: {summary_file}\")\n",
        "    print(\"=\" * 80)\n",
        "else:\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"âŒ Training failed with return code: {return_code}\")\n",
        "    print(f\"Check logs at: {LOG_FILE}\")\n",
        "    print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mB-d0zAnxgIY"
      },
      "source": [
        "## ğŸ“ Accessing Your Trained Models\n",
        "\n",
        "All training results are **automatically saved to Google Drive** and will persist after the Colab session ends.\n",
        "\n",
        "### ğŸ” What's Saved:\n",
        "\n",
        "1. **Checkpoints** (`*.pth` files):\n",
        "   - `checkpoint_e###_lr*_bs*_acc*_qpe*.pth` - Every epoch\n",
        "   - `checkpoint_best_pck_*.pth` - Best validation PCK models\n",
        "   \n",
        "2. **Logs**:\n",
        "   - `training_logs.txt` - Complete training output\n",
        "   - `TRAINING_SUMMARY.txt` - Quick summary with best model info\n",
        "\n",
        "### ğŸ“‚ Location:\n",
        "**Google Drive â†’ My Drive â†’ cape_training_output/**\n",
        "\n",
        "### ğŸ’¾ To Download for Local Evaluation:\n",
        "\n",
        "1. **On Google Drive Web:**\n",
        "   - Go to: https://drive.google.com/drive/my-drive\n",
        "   - Navigate to `cape_training_output/`\n",
        "   - Download the `checkpoint_best_pck_*.pth` file\n",
        "\n",
        "2. **On Your Computer:**\n",
        "   ```bash\n",
        "   # Evaluate on test set (unseen categories)\n",
        "   python scripts/eval_cape_checkpoint.py \\\n",
        "       --checkpoint path/to/checkpoint_best_pck_*.pth \\\n",
        "       --dataset_root . \\\n",
        "       --split test \\\n",
        "       --device cuda\n",
        "   ```\n",
        "\n",
        "### âš ï¸ Important Notes:\n",
        "- Checkpoints are **~580MB each** - only download what you need\n",
        "- The **best PCK checkpoint** is usually what you want for evaluation\n",
        "- All results persist in Google Drive even after Colab session ends\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMEKlLFvqa4q",
        "outputId": "717c4e00-56b5-4eec-8131-ad19bd0976ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "TRAINING RESULTS - Saved to Google Drive\n",
            "================================================================================\n",
            "ğŸ“ Location: /content/drive/MyDrive/cape_training_output\n",
            "\n",
            "âš ï¸  No TRAINING_SUMMARY.txt found - training may not be complete yet\n",
            "\n",
            "âŒ No checkpoints found!\n",
            "\n",
            "ğŸ“„ Training log: training_logs.txt (0.00 MB)\n",
            "\n",
            "Last 20 lines of training log:\n",
            "--------------------------------------------------------------------------------\n",
            "  - GCN layers: 2\n",
            "Support encoder layers: 3\n",
            "Fusion method: cross_attention\n",
            "Queries per episode: 2\n",
            "Train episodes per epoch: 500\n",
            "Val episodes per epoch: 200\n",
            "Fixed validation episodes: YES (seed=42) - stable curves\n",
            "\n",
            "Using device: cuda:0\n",
            "  GPU: NVIDIA A100-SXM4-80GB\n",
            "  CUDA Version: 12.6\n",
            "  GPU Memory: 79.32 GB\n",
            "  cuDNN benchmark: Enabled (auto-tuning convolution algorithms)\n",
            "  Mixed Precision (AMP): Enabled (FP16/FP32 training)\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Training completed: 2025-11-30 22:21:50\n",
            "Return code: 1\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Check training results and find best checkpoint\n",
        "import os\n",
        "import glob\n",
        "\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/cape_training_output\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"TRAINING RESULTS - Saved to Google Drive\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"ğŸ“ Location: {OUTPUT_DIR}\")\n",
        "print()\n",
        "\n",
        "# Check if training summary exists\n",
        "summary_file = os.path.join(OUTPUT_DIR, \"TRAINING_SUMMARY.txt\")\n",
        "if os.path.exists(summary_file):\n",
        "    print(\"âœ… Training Summary:\")\n",
        "    print(\"â”€\" * 80)\n",
        "    with open(summary_file, 'r') as f:\n",
        "        print(f.read())\n",
        "    print(\"â”€\" * 80)\n",
        "    print()\n",
        "else:\n",
        "    print(\"âš ï¸  No TRAINING_SUMMARY.txt found - training may not be complete yet\")\n",
        "    print()\n",
        "\n",
        "# List all checkpoints\n",
        "checkpoints = glob.glob(os.path.join(OUTPUT_DIR, \"*.pth\"))\n",
        "if checkpoints:\n",
        "    print(f\"ğŸ“¦ Found {len(checkpoints)} checkpoint file(s):\")\n",
        "\n",
        "    # Separate into epoch and best checkpoints\n",
        "    epoch_ckpts = [c for c in checkpoints if \"checkpoint_e\" in os.path.basename(c) and \"best\" not in os.path.basename(c)]\n",
        "    best_ckpts = [c for c in checkpoints if \"best_pck\" in os.path.basename(c)]\n",
        "\n",
        "    if best_ckpts:\n",
        "        print(f\"\\n  ğŸ† Best PCK Checkpoints ({len(best_ckpts)}):\")\n",
        "        for ckpt in sorted(best_ckpts):\n",
        "            size_mb = os.path.getsize(ckpt) / (1024 * 1024)\n",
        "            print(f\"    - {os.path.basename(ckpt)} ({size_mb:.1f} MB)\")\n",
        "\n",
        "    if epoch_ckpts:\n",
        "        print(f\"\\n  ğŸ“Š Epoch Checkpoints ({len(epoch_ckpts)}):\")\n",
        "        # Show first 3 and last 3\n",
        "        for ckpt in sorted(epoch_ckpts)[:3]:\n",
        "            size_mb = os.path.getsize(ckpt) / (1024 * 1024)\n",
        "            print(f\"    - {os.path.basename(ckpt)} ({size_mb:.1f} MB)\")\n",
        "        if len(epoch_ckpts) > 6:\n",
        "            print(f\"    ... ({len(epoch_ckpts) - 6} more) ...\")\n",
        "        for ckpt in sorted(epoch_ckpts)[-3:]:\n",
        "            size_mb = os.path.getsize(ckpt) / (1024 * 1024)\n",
        "            print(f\"    - {os.path.basename(ckpt)} ({size_mb:.1f} MB)\")\n",
        "\n",
        "    # Calculate total size\n",
        "    total_size = sum(os.path.getsize(c) for c in checkpoints) / (1024 * 1024 * 1024)\n",
        "    print(f\"\\n  ğŸ’¾ Total checkpoint size: {total_size:.2f} GB\")\n",
        "    print()\n",
        "\n",
        "    # Find best checkpoint\n",
        "    best_ckpts = glob.glob(os.path.join(OUTPUT_DIR, \"checkpoint_best_pck*.pth\"))\n",
        "    if best_ckpts:\n",
        "        best_ckpt = sorted(best_ckpts)[-1]  # Get most recent\n",
        "        print(f\"\\nâœ… Best checkpoint: {os.path.basename(best_ckpt)}\")\n",
        "        BEST_CHECKPOINT = best_ckpt\n",
        "    else:\n",
        "        # Use most recent checkpoint\n",
        "        best_ckpt = sorted(checkpoints, key=os.path.getmtime)[-1]\n",
        "        print(f\"\\nâš ï¸  No 'best' checkpoint found, using most recent: {os.path.basename(best_ckpt)}\")\n",
        "        BEST_CHECKPOINT = best_ckpt\n",
        "else:\n",
        "    print(\"âŒ No checkpoints found!\")\n",
        "    BEST_CHECKPOINT = None\n",
        "\n",
        "# Show log file info\n",
        "LOG_FILE = os.path.join(OUTPUT_DIR, \"training_logs.txt\")\n",
        "if os.path.exists(LOG_FILE):\n",
        "    size_mb = os.path.getsize(LOG_FILE) / (1024 * 1024)\n",
        "    print(f\"\\nğŸ“„ Training log: {os.path.basename(LOG_FILE)} ({size_mb:.2f} MB)\")\n",
        "\n",
        "    # Show last few lines of log\n",
        "    print(\"\\nLast 20 lines of training log:\")\n",
        "    print(\"-\" * 80)\n",
        "    with open(LOG_FILE, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        for line in lines[-20:]:\n",
        "            print(line.rstrip())\n",
        "else:\n",
        "    print(\"\\nâš ï¸  Training log not found\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b8ca77f"
      },
      "source": [
        "## 9. Evaluate Best Checkpoint on Test Set\n",
        "\n",
        "This section evaluates the best trained checkpoint on the **test set** (unseen categories) using the curated `eval_cape_checkpoint.py` script.\n",
        "\n",
        "The script will:\n",
        "- Run autoregressive inference on test episodes\n",
        "- Compute PCK@0.2 metrics (overall + per-category)\n",
        "- Generate visualizations (support + GT + predicted keypoints)\n",
        "- Save all results to Google Drive for later recovery\n",
        "\n",
        "Results are saved to: `Google Drive/cape_training_output/evaluate_checkpoint_<epoch>_<date>/`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnrdf9cSFDqB",
        "outputId": "ff6bae4e-c51e-4047-be75-57b70ee79da5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Using last checkpoint: checkpoint_best_pck_e010_pck0.8889_meanpck0.8889.pth\n",
            "\n",
            "Available checkpoints (56 total):\n",
            "  - checkpoint_e045_lr1e-04_bs1_acc4_qpe1.pth (epoch 45)\n",
            "  - checkpoint_e046_lr1e-04_bs1_acc4_qpe1.pth (epoch 46)\n",
            "  - checkpoint_e047_lr1e-04_bs1_acc4_qpe1.pth (epoch 47)\n",
            "  - checkpoint_e048_lr1e-04_bs1_acc4_qpe1.pth (epoch 48)\n",
            "  - checkpoint_e049_lr1e-04_bs1_acc4_qpe1.pth (epoch 49)\n",
            "  ... and 51 more\n",
            "\n",
            "================================================================================\n",
            "CAPE Prediction Visualization\n",
            "================================================================================\n",
            "Checkpoint: checkpoint_best_pck_e010_pck0.8889_meanpck0.8889.pth\n",
            "Image:      /content/category-agnostic-pose-estimation/data/camel_face/camel_16.jpg\n",
            "Output:     /content/category-agnostic-pose-estimation/output/single_image_colab/visualizations\n",
            "================================================================================\n",
            "\n",
            "Running visualization...\n",
            "Command: /usr/bin/python3 -m models.visualize_cape_predictions --checkpoint /content/category-agnostic-pose-estimation/output/single_image_colab/checkpoint_best_pck_e010_pck0.8889_meanpck0.8889.pth --dataset_root /content/category-agnostic-pose-estimation --device cuda --single_image_path /content/category-agnostic-pose-estimation/data/camel_face/camel_16.jpg --output_dir /content/category-agnostic-pose-estimation/output/single_image_colab/visualizations\n",
            "\n",
            "================================================================================\n",
            "CAPE Pose Estimation Visualization\n",
            "================================================================================\n",
            "Checkpoint: /content/category-agnostic-pose-estimation/output/single_image_colab/checkpoint_best_pck_e010_pck0.8889_meanpck0.8889.pth\n",
            "Device: cuda\n",
            "Output: /content/category-agnostic-pose-estimation/output/single_image_colab/visualizations\n",
            "================================================================================\n",
            "\n",
            "Loading model from /content/category-agnostic-pose-estimation/output/single_image_colab/checkpoint_best_pck_e010_pck0.8889_meanpck0.8889.pth...\n",
            "  Using training config (epoch 10)\n",
            "  Warning: Unexpected keys in checkpoint: 59 keys\n",
            "âœ“ Model loaded successfully\n",
            "âœ“ Model loaded and moved to cuda\n",
            "  Checkpoint epoch: 10\n",
            "  Checkpoint validation PCK: 88.89%\n",
            "\n",
            "Loading MP-100 dataset...\n",
            "\n",
            "âš ï¸  Single image mode: /content/category-agnostic-pose-estimation/data/camel_face/camel_16.jpg\n",
            "   Using this SAME image as both support and query (for overfitting visualization)\n",
            "loading annotations into memory...\n",
            "Done (t=0.27s)\n",
            "creating index...\n",
            "index created!\n",
            "ğŸ“Š Multi-instance statistics:\n",
            "   - Images with multiple instances: 615/12816 (4.8%)\n",
            "   - Total instances available: 13712\n",
            "   - Instances actually used: 12816 (93.5%)\n",
            "   - Instances skipped: 896 (6.5%)\n",
            "   - Max instances in single image: 9\n",
            "   âš ï¸  Note: Currently using only first instance per image\n",
            "Loaded MP-100 train dataset: 12816 images\n",
            "âœ“ Loaded train dataset: 12816 images\n",
            "  âš ï¸  Single-image mode: Using SAME image as both support and query (overfitting visualization)\n",
            "  â†’ Saved: /content/category-agnostic-pose-estimation/output/single_image_colab/visualizations/single_image_camel_16_epoch10.png\n",
            "\n",
            "âœ“ Visualization complete!\n",
            "  Saved to: /content/category-agnostic-pose-estimation/output/single_image_colab/visualizations/single_image_camel_16_epoch10.png\n",
            "  PCK@0.2: 11.11%\n",
            "\n",
            "  âš ï¸  WARNING: Low PCK (11.11%) detected!\n",
            "     This may indicate:\n",
            "     1. The checkpoint is from an early epoch (before overfitting)\n",
            "     2. The checkpoint was trained on a different image\n",
            "     3. Training failed and you're using an old checkpoint\n",
            "     4. Coordinate normalization mismatch\n",
            "\n",
            "     Expected PCK for overfitted model: ~100%\n",
            "     Current checkpoint epoch: 10\n",
            "     Check training logs to verify which checkpoint should be used.\n",
            "\n",
            "Warnings/Errors:\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:515: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
            "  output = torch._nested_tensor_from_mask(\n",
            "\n",
            "\n",
            "================================================================================\n",
            "âœ… Visualization complete!\n",
            "Results saved to: /content/category-agnostic-pose-estimation/output/single_image_colab/visualizations\n",
            "================================================================================\n",
            "\n",
            "Generated 1 visualization(s):\n",
            "  - single_image_camel_16_epoch10.png\n",
            "\n",
            "ğŸ’¡ Visualizations show:\n",
            "   - Query image (the single training image)\n",
            "   - Ground truth keypoints (what the model should predict)\n",
            "   - Predicted keypoints (what the model actually predicted)\n",
            "   - PCK score (percentage of correctly predicted keypoints)\n"
          ]
        }
      ],
      "source": [
        "# Evaluate best checkpoint using curated eval_cape_checkpoint.py script\n",
        "# Run both 1-shot and 5-shot evaluations\n",
        "import os\n",
        "import sys\n",
        "import subprocess\n",
        "from datetime import datetime\n",
        "import glob\n",
        "import re\n",
        "\n",
        "PROJECT_ROOT = \"/content/category-agnostic-pose-estimation\"\n",
        "TRAINING_OUTPUT_DIR = \"/content/drive/MyDrive/cape_training_output\"\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"CHECKPOINT EVALUATION ON TEST SET (1-Shot and 5-Shot)\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "\n",
        "# Find best PCK checkpoint\n",
        "print(\"Finding best checkpoint...\")\n",
        "best_ckpts = glob.glob(os.path.join(TRAINING_OUTPUT_DIR, \"checkpoint_best_pck*.pth\"))\n",
        "\n",
        "if not best_ckpts:\n",
        "    # Fallback: use most recent epoch checkpoint\n",
        "    epoch_ckpts = glob.glob(os.path.join(TRAINING_OUTPUT_DIR, \"checkpoint_e*.pth\"))\n",
        "    if epoch_ckpts:\n",
        "        best_checkpoint = sorted(epoch_ckpts, key=os.path.getmtime)[-1]\n",
        "        print(f\"âš ï¸  No best checkpoint found, using most recent: {os.path.basename(best_checkpoint)}\")\n",
        "    else:\n",
        "        print(\"âŒ No checkpoint found!\")\n",
        "        print(f\"   Looking in: {TRAINING_OUTPUT_DIR}\")\n",
        "        print(\"   Please run training first (Cell 21)\")\n",
        "        sys.exit(1)\n",
        "else:\n",
        "    # Use the most recent best checkpoint (highest PCK if multiple)\n",
        "    best_checkpoint = sorted(best_ckpts)[-1]\n",
        "    print(f\"âœ… Found best checkpoint: {os.path.basename(best_checkpoint)}\")\n",
        "\n",
        "# Extract epoch number from checkpoint filename\n",
        "# Format: checkpoint_best_pck_e###_pck0.####_meanpck0.####.pth\n",
        "match = re.search(r'_e(\\d+)_', os.path.basename(best_checkpoint))\n",
        "if match:\n",
        "    epoch_num = int(match.group(1))\n",
        "else:\n",
        "    # Fallback: use 'unknown' if can't parse\n",
        "    epoch_num = 'unknown'\n",
        "\n",
        "print(f\"Epoch number: {epoch_num}\")\n",
        "print()\n",
        "\n",
        "# Change to project root\n",
        "os.chdir(PROJECT_ROOT)\n",
        "\n",
        "# Run evaluations for both 1-shot and 5-shot\n",
        "for num_support in [1, 5]:\n",
        "    shot_name = f\"{num_support}-shot\"\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"EVALUATION: {shot_name.upper()}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Create evaluation output directory with shot info\n",
        "    date_str = datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
        "    eval_folder_name = f\"evaluate_checkpoint_e{epoch_num}_{shot_name}_{date_str}\"\n",
        "    EVAL_OUTPUT_DIR = os.path.join(TRAINING_OUTPUT_DIR, eval_folder_name)\n",
        "    os.makedirs(EVAL_OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "    # Build evaluation command\n",
        "    cmd = [\n",
        "        sys.executable,\n",
        "        \"scripts/eval_cape_checkpoint.py\",\n",
        "\n",
        "        # Required: checkpoint to evaluate\n",
        "        \"--checkpoint\", best_checkpoint,\n",
        "\n",
        "        # Evaluate on test set (unseen categories)\n",
        "        \"--split\", \"test\",\n",
        "\n",
        "        # Dataset location\n",
        "        \"--dataset-root\", PROJECT_ROOT,\n",
        "\n",
        "        # Output directory (in Google Drive)\n",
        "        \"--output-dir\", EVAL_OUTPUT_DIR,\n",
        "\n",
        "        # Evaluation parameters\n",
        "        \"--num-episodes\", \"200\",         # 200 test episodes\n",
        "        \"--num-support-per-episode\", str(num_support),  # 1-shot or 5-shot\n",
        "        \"--num-visualizations\", \"50\",   # Save 50 example visualizations\n",
        "        \"--pck-threshold\", \"0.2\",       # PCK@0.2 (standard metric)\n",
        "        \"--draw-skeleton\",               # Draw skeleton edges\n",
        "        \"--eval_seed\", \"42\",            # Reproducible evaluation\n",
        "\n",
        "        # System\n",
        "        \"--device\", \"cuda\",\n",
        "        \"--num-workers\", \"2\",\n",
        "    ]\n",
        "\n",
        "    print(f\"Checkpoint:  {os.path.basename(best_checkpoint)}\")\n",
        "    print(f\"Split:       test (unseen categories)\")\n",
        "    print(f\"Shot:        {shot_name} ({num_support} support image(s) per episode)\")\n",
        "    print(f\"Episodes:    200\")\n",
        "    print(f\"Output:      {eval_folder_name}\")\n",
        "    print()\n",
        "\n",
        "    # Run evaluation\n",
        "    try:\n",
        "        result = subprocess.run(cmd, check=True, capture_output=False, text=True)\n",
        "\n",
        "        print(f\"\\nâœ… {shot_name.upper()} EVALUATION COMPLETE\")\n",
        "        print(f\"Results saved to: {EVAL_OUTPUT_DIR}\")\n",
        "\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"\\nâŒ {shot_name} evaluation failed with return code: {e.returncode}\")\n",
        "        print(f\"Check output above for error details\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ Error during {shot_name} evaluation: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"âœ… ALL EVALUATIONS COMPLETE\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Results saved to Google Drive:\")\n",
        "print(f\"  MyDrive/cape_training_output/\")\n",
        "print()\n",
        "print(\"Look for folders:\")\n",
        "print(f\"  - evaluate_checkpoint_e{epoch_num}_1-shot_*\")\n",
        "print(f\"  - evaluate_checkpoint_e{epoch_num}_5-shot_*\")\n",
        "print()\n",
        "print(\"Each folder contains:\")\n",
        "print(f\"  - metrics_test.json       : PCK metrics (overall + per-category)\")\n",
        "print(f\"  - visualizations/*.png    : 50 example visualizations\")\n",
        "print(\"=\" * 80)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cM2N0AhVFDqB"
      },
      "outputs": [],
      "source": [
        "# View evaluation results summary\n",
        "import os\n",
        "import json\n",
        "import glob\n",
        "\n",
        "# Evaluation output directory (from previous cell)\n",
        "if 'EVAL_OUTPUT_DIR' not in globals():\n",
        "    # Fallback: find most recent evaluation folder\n",
        "    TRAINING_OUTPUT_DIR = \"/content/drive/MyDrive/cape_training_output\"\n",
        "    eval_folders = glob.glob(os.path.join(TRAINING_OUTPUT_DIR, \"evaluate_checkpoint_*\"))\n",
        "    if eval_folders:\n",
        "        EVAL_OUTPUT_DIR = sorted(eval_folders)[-1]  # Most recent\n",
        "        print(f\"Using most recent evaluation: {os.path.basename(EVAL_OUTPUT_DIR)}\")\n",
        "    else:\n",
        "        print(\"âŒ No evaluation results found. Run Cell 24 first.\")\n",
        "        import sys\n",
        "        sys.exit(1)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"EVALUATION RESULTS SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"ğŸ“ Location: {EVAL_OUTPUT_DIR}\")\n",
        "print()\n",
        "\n",
        "# Load and display evaluation metrics\n",
        "metrics_file = os.path.join(EVAL_OUTPUT_DIR, \"metrics_test.json\")\n",
        "\n",
        "if os.path.exists(metrics_file):\n",
        "    print(\"ğŸ“Š Test Set Evaluation Metrics:\")\n",
        "    print(\"â”€\" * 80)\n",
        "\n",
        "    with open(metrics_file, 'r') as f:\n",
        "        metrics = json.load(f)\n",
        "\n",
        "    # Display overall metrics\n",
        "    pck_overall = metrics.get('pck_overall', 0.0)\n",
        "    mean_pck = metrics.get('mean_pck_categories', 0.0)\n",
        "    total_correct = metrics.get('total_correct', 0)\n",
        "    total_visible = metrics.get('total_visible', 0)\n",
        "\n",
        "    print(f\"  Overall PCK@0.2:          {pck_overall:.4f} ({pck_overall*100:.2f}%)\")\n",
        "    print(f\"  Mean PCK (categories):    {mean_pck:.4f} ({mean_pck*100:.2f}%)\")\n",
        "    print(f\"  Correct keypoints:        {total_correct} / {total_visible}\")\n",
        "    print()\n",
        "\n",
        "    # Display per-category results (top 5 and bottom 5)\n",
        "    if 'per_category' in metrics and len(metrics['per_category']) > 0:\n",
        "        cat_pcks = [(cat_id, cat_data['pck']) for cat_id, cat_data in metrics['per_category'].items()]\n",
        "        cat_pcks_sorted = sorted(cat_pcks, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        print(f\"  Top 5 Categories:\")\n",
        "        for cat_id, pck in cat_pcks_sorted[:5]:\n",
        "            print(f\"    Category {cat_id}: {pck:.2%}\")\n",
        "\n",
        "        print()\n",
        "        print(f\"  Bottom 5 Categories:\")\n",
        "        for cat_id, pck in cat_pcks_sorted[-5:]:\n",
        "            print(f\"    Category {cat_id}: {pck:.2%}\")\n",
        "\n",
        "    print(\"â”€\" * 80)\n",
        "    print()\n",
        "else:\n",
        "    print(\"âš ï¸  Metrics file not found - evaluation may have failed\")\n",
        "    print(f\"   Expected: {metrics_file}\")\n",
        "    print()\n",
        "\n",
        "# Count visualization files\n",
        "vis_dir = os.path.join(EVAL_OUTPUT_DIR, \"visualizations\")\n",
        "if os.path.exists(vis_dir):\n",
        "    vis_files = glob.glob(os.path.join(vis_dir, \"*.png\"))\n",
        "    print(f\"ğŸ–¼ï¸  Visualizations: {len(vis_files)} files generated\")\n",
        "    print(f\"   Location: {vis_dir}\")\n",
        "    print()\n",
        "\n",
        "# Final summary\n",
        "print(\"=\" * 80)\n",
        "print(\"ğŸ“¥ ACCESS YOUR RESULTS\")\n",
        "print(\"=\" * 80)\n",
        "print()\n",
        "print(\"All results are saved in Google Drive and persist after Colab session ends!\")\n",
        "print()\n",
        "print(\"Google Drive path:\")\n",
        "print(f\"  MyDrive/cape_training_output/{os.path.basename(EVAL_OUTPUT_DIR)}/\")\n",
        "print()\n",
        "print(\"Contains:\")\n",
        "print(f\"  - metrics_test.json       : Full PCK metrics + per-category breakdown\")\n",
        "print(f\"  - visualizations/*.png    : Example predictions with GT comparison\")\n",
        "print()\n",
        "print(\"To download:\")\n",
        "print(f\"  1. Go to https://drive.google.com/drive/my-drive\")\n",
        "print(f\"  2. Navigate to cape_training_output/{os.path.basename(EVAL_OUTPUT_DIR)}/\")\n",
        "print(f\"  3. Download the entire folder or specific files\")\n",
        "print()\n",
        "print(\"=\" * 80)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
