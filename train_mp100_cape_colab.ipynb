{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MP-100 CAPE Training on Google Colab\n",
        "\n",
        "This notebook trains Category-Agnostic Pose Estimation (CAPE) on the MP-100 dataset using Google Colab's GPU.\n",
        "\n",
        "## Setup Instructions\n",
        "1. Enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU (T4 or better)\n",
        "2. Run all cells in order\n",
        "3. The notebook will:\n",
        "   - Clone code from GitHub\n",
        "   - Install dependencies\n",
        "   - Authenticate to GCP\n",
        "   - Mount GCS bucket with data\n",
        "   - Run training with \"tiny\" mode\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Check GPU Availability\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No GPU detected! Please enable GPU in Runtime > Change runtime type > GPU\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone repository from GitHub\n",
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "REPO_URL = \"https://github.com/nkkrnkl/category-agnostic-pose-estimation.git\"\n",
        "BRANCH = \"teo-branch-copy\"\n",
        "PROJECT_ROOT = \"/content/category-agnostic-pose-estimation\"\n",
        "\n",
        "# Remove existing directory if it exists\n",
        "if os.path.exists(PROJECT_ROOT):\n",
        "    print(f\"Removing existing directory: {PROJECT_ROOT}\")\n",
        "    !rm -rf {PROJECT_ROOT}\n",
        "\n",
        "# For private repositories, you need to authenticate\n",
        "# Option 1: Use Personal Access Token (recommended)\n",
        "# Get token from: https://github.com/settings/tokens\n",
        "# Create a token with 'repo' scope\n",
        "print(\"For private repositories, you need to authenticate.\")\n",
        "print(\"Option 1: Enter your GitHub Personal Access Token\")\n",
        "print(\"  (Get one from: https://github.com/settings/tokens)\")\n",
        "print(\"Option 2: Press Enter to try without token (will fail if repo is private)\")\n",
        "print()\n",
        "\n",
        "GITHUB_TOKEN = getpass(\"Enter GitHub Personal Access Token (or press Enter to skip): \")\n",
        "\n",
        "if GITHUB_TOKEN.strip():\n",
        "    # Use token in URL\n",
        "    # Format: https://TOKEN@github.com/username/repo.git\n",
        "    AUTH_REPO_URL = REPO_URL.replace(\"https://github.com/\", f\"https://{GITHUB_TOKEN}@github.com/\")\n",
        "    print(f\"Cloning repository from {REPO_URL} (branch: {BRANCH})...\")\n",
        "    !git clone -b {BRANCH} {AUTH_REPO_URL} {PROJECT_ROOT}\n",
        "else:\n",
        "    # Try without token (will work if repo is public)\n",
        "    print(f\"Cloning repository from {REPO_URL} (branch: {BRANCH})...\")\n",
        "    !git clone -b {BRANCH} {REPO_URL} {PROJECT_ROOT}\n",
        "\n",
        "# Verify clone\n",
        "if os.path.exists(PROJECT_ROOT) and os.path.exists(os.path.join(PROJECT_ROOT, \".git\")):\n",
        "    print(f\"‚úÖ Repository cloned successfully to {PROJECT_ROOT}\")\n",
        "    !cd {PROJECT_ROOT} && git branch\n",
        "else:\n",
        "    print(\"‚ùå Failed to clone repository\")\n",
        "    print(\"\\nIf the repository is private, you need to:\")\n",
        "    print(\"1. Create a Personal Access Token at: https://github.com/settings/tokens\")\n",
        "    print(\"2. Select 'repo' scope\")\n",
        "    print(\"3. Run this cell again and paste the token when prompted\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Install Requirements\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install additional dependencies needed for plot_utils and other utilities\n",
        "# (descartes, shapely, etc. - these are in requirements.txt but not requirements_cape.txt)\n",
        "print(\"Installing additional dependencies (descartes, shapely, etc.)...\")\n",
        "!pip install -q descartes shapely>=1.8.0\n",
        "print(\"‚úÖ Additional dependencies installed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install requirements\n",
        "import os\n",
        "\n",
        "PROJECT_ROOT = \"/content/category-agnostic-pose-estimation\"\n",
        "REQUIREMENTS_FILE = os.path.join(PROJECT_ROOT, \"requirements_cape.txt\")\n",
        "\n",
        "print(\"Installing requirements from requirements_cape.txt...\")\n",
        "!cd {PROJECT_ROOT} && pip install -q -r {REQUIREMENTS_FILE}\n",
        "\n",
        "# Install detectron2 for CUDA 11.8 (Colab typically has CUDA 11.8)\n",
        "print(\"\\nInstalling detectron2...\")\n",
        "!pip install -q 'git+https://github.com/facebookresearch/detectron2.git'\n",
        "\n",
        "print(\"‚úÖ All dependencies installed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Authenticate to GCP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Authenticate to GCP\n",
        "from google.colab import auth\n",
        "\n",
        "print(\"Authenticating to GCP...\")\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Set GCP project\n",
        "GCP_PROJECT = \"dl-category-agnostic-pose-est\"\n",
        "!gcloud config set project {GCP_PROJECT}\n",
        "\n",
        "print(f\"‚úÖ Authenticated to GCP project: {GCP_PROJECT}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Mount GCS Bucket\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify data access before training\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "PROJECT_ROOT = \"/content/category-agnostic-pose-estimation\"\n",
        "DATA_DIR = os.path.join(PROJECT_ROOT, \"data\")\n",
        "\n",
        "print(\"Verifying data access...\")\n",
        "print(f\"Data directory: {DATA_DIR}\")\n",
        "print(f\"Exists: {os.path.exists(DATA_DIR)}\")\n",
        "print(f\"Is symlink: {os.path.islink(DATA_DIR)}\")\n",
        "\n",
        "if os.path.exists(DATA_DIR):\n",
        "    # Check if we can list directories\n",
        "    try:\n",
        "        categories = [d for d in os.listdir(DATA_DIR) if os.path.isdir(os.path.join(DATA_DIR, d))]\n",
        "        print(f\"‚úÖ Found {len(categories)} category directories\")\n",
        "        if len(categories) > 0:\n",
        "            print(f\"   First 5 categories: {categories[:5]}\")\n",
        "            \n",
        "            # Try to access a file in the first category\n",
        "            first_cat = categories[0]\n",
        "            cat_dir = os.path.join(DATA_DIR, first_cat)\n",
        "            files = [f for f in os.listdir(cat_dir) if f.endswith(('.jpg', '.png', '.jpeg'))]\n",
        "            if len(files) > 0:\n",
        "                test_file = os.path.join(cat_dir, files[0])\n",
        "                print(f\"   Test file exists: {os.path.exists(test_file)}\")\n",
        "                print(f\"   Test file: {test_file}\")\n",
        "            else:\n",
        "                print(f\"   ‚ö†Ô∏è  No image files found in {first_cat}\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error accessing data directory: {e}\")\n",
        "        print(\"   This might indicate the GCS mount is not working properly\")\n",
        "else:\n",
        "    print(f\"‚ùå Data directory does not exist: {DATA_DIR}\")\n",
        "    print(\"   Please check:\")\n",
        "    print(\"   1. GCS bucket is mounted\")\n",
        "    print(\"   2. Data symlink is created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount GCS bucket using gcsfuse\n",
        "import os\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "PROJECT_ROOT = \"/content/category-agnostic-pose-estimation\"\n",
        "BUCKET_NAME = \"dl-category-agnostic-pose-mp100-data\"\n",
        "MOUNT_POINT = os.path.join(PROJECT_ROOT, \"Raster2Seq_internal-main\", \"data\")\n",
        "\n",
        "# Install gcsfuse from Google's official repository\n",
        "print(\"Installing gcsfuse...\")\n",
        "# Add Google's gcsfuse repository (updated method for newer Ubuntu versions)\n",
        "!export GCSFUSE_REPO=gcsfuse-`lsb_release -c -s` && \\\n",
        "echo \"deb http://packages.cloud.google.com/apt $GCSFUSE_REPO main\" | sudo tee /etc/apt/sources.list.d/gcsfuse.list && \\\n",
        "curl -fsSL https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo gpg --dearmor -o /usr/share/keyrings/cloud.google.gpg && \\\n",
        "echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] http://packages.cloud.google.com/apt $GCSFUSE_REPO main\" | sudo tee /etc/apt/sources.list.d/gcsfuse.list && \\\n",
        "sudo apt-get update && \\\n",
        "sudo apt-get install -y gcsfuse\n",
        "\n",
        "# Verify installation\n",
        "!which gcsfuse\n",
        "print(\"‚úÖ gcsfuse installed\")\n",
        "\n",
        "# Create mount point directory and parent directories\n",
        "print(f\"Creating mount point: {MOUNT_POINT}\")\n",
        "os.makedirs(os.path.dirname(MOUNT_POINT), exist_ok=True)\n",
        "os.makedirs(MOUNT_POINT, exist_ok=True)\n",
        "\n",
        "# Check if already mounted\n",
        "try:\n",
        "    result = subprocess.run(['mountpoint', '-q', MOUNT_POINT], capture_output=True)\n",
        "    if result.returncode == 0:\n",
        "        print(f\"‚úÖ Already mounted at {MOUNT_POINT}\")\n",
        "    else:\n",
        "        # Try to unmount if exists but not properly mounted\n",
        "        try:\n",
        "            subprocess.run(['fusermount', '-u', MOUNT_POINT], capture_output=True, timeout=5)\n",
        "        except:\n",
        "            try:\n",
        "                subprocess.run(['umount', MOUNT_POINT], capture_output=True, timeout=5)\n",
        "            except:\n",
        "                pass\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Mount the bucket\n",
        "print(f\"Mounting gs://{BUCKET_NAME} to {MOUNT_POINT}...\")\n",
        "print(\"This may take a moment...\")\n",
        "\n",
        "# Run gcsfuse in background\n",
        "# Note: In Colab, we need to run gcsfuse in background using shell &\n",
        "print(f\"Running: gcsfuse --implicit-dirs {BUCKET_NAME} {MOUNT_POINT}\")\n",
        "!nohup gcsfuse --implicit-dirs {BUCKET_NAME} {MOUNT_POINT} > /tmp/gcsfuse.log 2>&1 &\n",
        "\n",
        "# Wait a moment for mount to initialize\n",
        "print(\"Waiting for mount to initialize...\")\n",
        "time.sleep(8)  # Give it more time to mount\n",
        "\n",
        "# Check mount status\n",
        "print(\"\\nChecking mount status...\")\n",
        "# Check mount log for errors\n",
        "if os.path.exists(\"/tmp/gcsfuse.log\"):\n",
        "    with open(\"/tmp/gcsfuse.log\", \"r\") as f:\n",
        "        log_content = f.read()\n",
        "        if log_content:\n",
        "            print(\"Mount log:\")\n",
        "            print(log_content[-500:])  # Last 500 chars\n",
        "        else:\n",
        "            print(\"Mount log is empty (mount might still be initializing)\")\n",
        "\n",
        "# Also verify we can access the bucket directly with gsutil\n",
        "print(\"\\nVerifying bucket access with gsutil...\")\n",
        "!gsutil ls gs://{BUCKET_NAME}/ | head -10\n",
        "\n",
        "# Verify mount\n",
        "print(f\"\\nVerifying mount at: {MOUNT_POINT}\")\n",
        "print(f\"Path exists: {os.path.exists(MOUNT_POINT)}\")\n",
        "\n",
        "# Check if actually mounted using mountpoint command\n",
        "try:\n",
        "    result = subprocess.run(['mountpoint', '-q', MOUNT_POINT], capture_output=True)\n",
        "    is_mounted = (result.returncode == 0)\n",
        "    print(f\"Is mounted: {is_mounted}\")\n",
        "except:\n",
        "    # Fallback: check mount table\n",
        "    result = subprocess.run(['mount'], capture_output=True, text=True)\n",
        "    is_mounted = MOUNT_POINT in result.stdout\n",
        "    print(f\"Is mounted (from mount table): {is_mounted}\")\n",
        "\n",
        "if os.path.exists(MOUNT_POINT) and is_mounted:\n",
        "    try:\n",
        "        # Try to list contents\n",
        "        items = os.listdir(MOUNT_POINT)\n",
        "        if len(items) > 0:\n",
        "            print(f\"‚úÖ GCS bucket mounted successfully!\")\n",
        "            print(f\"Mount point: {MOUNT_POINT}\")\n",
        "            print(f\"Found {len(items)} items in bucket\")\n",
        "            # List a few items to verify\n",
        "            for item in items[:10]:\n",
        "                item_path = os.path.join(MOUNT_POINT, item)\n",
        "                item_type = \"directory\" if os.path.isdir(item_path) else \"file\"\n",
        "                print(f\"   - {item} ({item_type})\")\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è  Mount point exists but is empty (0 items)\")\n",
        "            print(f\"   This might indicate:\")\n",
        "            print(f\"   1. Bucket is empty\")\n",
        "            print(f\"   2. Mount didn't work correctly\")\n",
        "            print(f\"   3. Permission issues\")\n",
        "    except PermissionError as e:\n",
        "        print(f\"‚ö†Ô∏è  Permission error accessing mount: {e}\")\n",
        "        print(\"   Mount might still be initializing, wait a moment and try again\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Mount point exists but cannot list contents: {e}\")\n",
        "        print(\"   This might indicate a mount issue\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "elif os.path.exists(MOUNT_POINT) and not is_mounted:\n",
        "    print(f\"‚ö†Ô∏è  Directory exists but is not mounted\")\n",
        "    print(f\"   The directory exists but gcsfuse mount is not active\")\n",
        "    print(f\"   Trying to mount again...\")\n",
        "    # Try mounting again\n",
        "    !nohup gcsfuse --implicit-dirs {BUCKET_NAME} {MOUNT_POINT} > /tmp/gcsfuse.log 2>&1 &\n",
        "    time.sleep(5)\n",
        "    # Re-check\n",
        "    items = os.listdir(MOUNT_POINT) if os.path.exists(MOUNT_POINT) else []\n",
        "    if len(items) > 0:\n",
        "        print(f\"‚úÖ Mount successful after retry! Found {len(items)} items\")\n",
        "    else:\n",
        "        print(f\"‚ùå Mount still not working\")\n",
        "else:\n",
        "    print(\"‚ùå Failed to mount GCS bucket\")\n",
        "    print(f\"   Mount point: {MOUNT_POINT}\")\n",
        "    print(f\"   Check:\")\n",
        "    print(f\"   1. GCP authentication (run the GCP auth cell)\")\n",
        "    print(f\"   2. Bucket name is correct: {BUCKET_NAME}\")\n",
        "    print(f\"   3. You have read access to the bucket\")\n",
        "    print(f\"   4. Check mount log: /tmp/gcsfuse.log\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Create Data Symlink\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create symlink from data to mounted GCS bucket (as expected by START_TRAINING.sh)\n",
        "import os\n",
        "\n",
        "PROJECT_ROOT = \"/content/category-agnostic-pose-estimation\"\n",
        "MOUNTED_DATA = os.path.join(PROJECT_ROOT, \"Raster2Seq_internal-main\", \"data\")\n",
        "DATA_SYMLINK = os.path.join(PROJECT_ROOT, \"data\")\n",
        "\n",
        "print(f\"Checking mount point: {MOUNTED_DATA}\")\n",
        "print(f\"  Exists: {os.path.exists(MOUNTED_DATA)}\")\n",
        "if os.path.exists(MOUNTED_DATA):\n",
        "    print(f\"  Is directory: {os.path.isdir(MOUNTED_DATA)}\")\n",
        "    try:\n",
        "        items = os.listdir(MOUNTED_DATA)\n",
        "        print(f\"  Can list contents: Yes ({len(items)} items)\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Can list contents: No ({e})\")\n",
        "\n",
        "# Remove existing symlink or directory if it exists\n",
        "if os.path.exists(DATA_SYMLINK):\n",
        "    if os.path.islink(DATA_SYMLINK):\n",
        "        print(f\"Removing existing symlink: {DATA_SYMLINK}\")\n",
        "        os.unlink(DATA_SYMLINK)\n",
        "    elif os.path.isdir(DATA_SYMLINK):\n",
        "        print(f\"Warning: {DATA_SYMLINK} exists as a directory (not a symlink)\")\n",
        "        print(\"   Removing it to create symlink...\")\n",
        "        import shutil\n",
        "        shutil.rmtree(DATA_SYMLINK)\n",
        "    else:\n",
        "        print(f\"Warning: {DATA_SYMLINK} exists and is not a symlink or directory\")\n",
        "        os.remove(DATA_SYMLINK)\n",
        "\n",
        "# Create symlink\n",
        "if os.path.exists(MOUNTED_DATA) and os.path.isdir(MOUNTED_DATA):\n",
        "    try:\n",
        "        # Use absolute path for symlink target\n",
        "        MOUNTED_DATA_ABS = os.path.abspath(MOUNTED_DATA)\n",
        "        print(f\"\\nCreating symlink:\")\n",
        "        print(f\"  From: {DATA_SYMLINK}\")\n",
        "        print(f\"  To: {MOUNTED_DATA_ABS}\")\n",
        "        os.symlink(MOUNTED_DATA_ABS, DATA_SYMLINK)\n",
        "        print(f\"‚úÖ Created symlink: {DATA_SYMLINK} -> {MOUNTED_DATA_ABS}\")\n",
        "        \n",
        "        # Verify symlink\n",
        "        if os.path.exists(DATA_SYMLINK):\n",
        "            print(f\"‚úÖ Symlink verified: {DATA_SYMLINK}\")\n",
        "            print(f\"  Is symlink: {os.path.islink(DATA_SYMLINK)}\")\n",
        "            # Try to list contents through symlink\n",
        "            try:\n",
        "                items = os.listdir(DATA_SYMLINK)\n",
        "                print(f\"‚úÖ Can access {len(items)} items through symlink\")\n",
        "                print(f\"   First 5 items: {items[:5]}\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è  Symlink exists but cannot access contents: {e}\")\n",
        "        else:\n",
        "            print(f\"‚ùå Symlink creation failed - path does not exist after creation\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error creating symlink: {e}\")\n",
        "        print(f\"   Source: {MOUNTED_DATA}\")\n",
        "        print(f\"   Target: {DATA_SYMLINK}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "else:\n",
        "    print(f\"‚ùå Mounted data not found at {MOUNTED_DATA}\")\n",
        "    print(f\"   Please check that GCS bucket is mounted correctly\")\n",
        "    print(f\"   Run the mount cell above and check for errors\")\n",
        "    print(f\"   Mount point should exist and be accessible\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.1. Create Cleaned Annotations Symlink\n",
        "\n",
        "Create a symlink to access cleaned_annotations from the GCS bucket.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create symlink for cleaned_annotations from GCS bucket\n",
        "import os\n",
        "\n",
        "PROJECT_ROOT = \"/content/category-agnostic-pose-estimation\"\n",
        "MOUNTED_DATA = os.path.join(PROJECT_ROOT, \"Raster2Seq_internal-main\", \"data\")\n",
        "CLEANED_ANNOTATIONS_SOURCE = os.path.join(MOUNTED_DATA, \"cleaned_annotations\")\n",
        "CLEANED_ANNOTATIONS_SYMLINK = os.path.join(PROJECT_ROOT, \"cleaned_annotations\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"CREATING CLEANED_ANNOTATIONS SYMLINK\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Check if source exists\n",
        "if not os.path.exists(MOUNTED_DATA):\n",
        "    print(f\"‚ùå Mount point not found: {MOUNTED_DATA}\")\n",
        "    print(\"   Please run the 'Mount GCS Bucket' cell first.\")\n",
        "elif not os.path.exists(CLEANED_ANNOTATIONS_SOURCE):\n",
        "    print(f\"‚ö†Ô∏è  cleaned_annotations not found at: {CLEANED_ANNOTATIONS_SOURCE}\")\n",
        "    print(\"   Checking what's available in the mount...\")\n",
        "    try:\n",
        "        items = os.listdir(MOUNTED_DATA)\n",
        "        print(f\"   Found {len(items)} items in mount:\")\n",
        "        for item in sorted(items)[:10]:\n",
        "            item_path = os.path.join(MOUNTED_DATA, item)\n",
        "            item_type = \"directory\" if os.path.isdir(item_path) else \"file\"\n",
        "            print(f\"     - {item} ({item_type})\")\n",
        "        if len(items) > 10:\n",
        "            print(f\"     ... and {len(items) - 10} more\")\n",
        "    except Exception as e:\n",
        "        print(f\"   Error listing mount contents: {e}\")\n",
        "    print(\"\\n   Please ensure cleaned_annotations exists in the GCS bucket.\")\n",
        "else:\n",
        "    # Remove existing symlink or directory if it exists\n",
        "    if os.path.exists(CLEANED_ANNOTATIONS_SYMLINK):\n",
        "        if os.path.islink(CLEANED_ANNOTATIONS_SYMLINK):\n",
        "            print(f\"Removing existing symlink: {CLEANED_ANNOTATIONS_SYMLINK}\")\n",
        "            os.unlink(CLEANED_ANNOTATIONS_SYMLINK)\n",
        "        elif os.path.isdir(CLEANED_ANNOTATIONS_SYMLINK):\n",
        "            print(f\"Warning: {CLEANED_ANNOTATIONS_SYMLINK} exists as a directory (not a symlink)\")\n",
        "            print(\"   Removing it to create symlink...\")\n",
        "            import shutil\n",
        "            shutil.rmtree(CLEANED_ANNOTATIONS_SYMLINK)\n",
        "        else:\n",
        "            print(f\"Warning: {CLEANED_ANNOTATIONS_SYMLINK} exists and is not a symlink or directory\")\n",
        "            os.remove(CLEANED_ANNOTATIONS_SYMLINK)\n",
        "    \n",
        "    # Create symlink\n",
        "    try:\n",
        "        CLEANED_ANNOTATIONS_SOURCE_ABS = os.path.abspath(CLEANED_ANNOTATIONS_SOURCE)\n",
        "        print(f\"\\nCreating symlink:\")\n",
        "        print(f\"  From: {CLEANED_ANNOTATIONS_SYMLINK}\")\n",
        "        print(f\"  To: {CLEANED_ANNOTATIONS_SOURCE_ABS}\")\n",
        "        os.symlink(CLEANED_ANNOTATIONS_SOURCE_ABS, CLEANED_ANNOTATIONS_SYMLINK)\n",
        "        print(f\"‚úÖ Created symlink: {CLEANED_ANNOTATIONS_SYMLINK} -> {CLEANED_ANNOTATIONS_SOURCE_ABS}\")\n",
        "        \n",
        "        # Verify symlink\n",
        "        if os.path.exists(CLEANED_ANNOTATIONS_SYMLINK):\n",
        "            print(f\"‚úÖ Symlink verified: {CLEANED_ANNOTATIONS_SYMLINK}\")\n",
        "            print(f\"  Is symlink: {os.path.islink(CLEANED_ANNOTATIONS_SYMLINK)}\")\n",
        "            # Try to list contents through symlink\n",
        "            try:\n",
        "                items = os.listdir(CLEANED_ANNOTATIONS_SYMLINK)\n",
        "                print(f\"‚úÖ Can access {len(items)} annotation files through symlink\")\n",
        "                print(f\"   Files: {', '.join(sorted(items)[:5])}\")\n",
        "                if len(items) > 5:\n",
        "                    print(f\"   ... and {len(items) - 5} more files\")\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è  Symlink exists but cannot access contents: {e}\")\n",
        "        else:\n",
        "            print(f\"‚ùå Symlink creation failed - path does not exist after creation\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error creating symlink: {e}\")\n",
        "        print(f\"   Source: {CLEANED_ANNOTATIONS_SOURCE}\")\n",
        "        print(f\"   Target: {CLEANED_ANNOTATIONS_SYMLINK}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6.5. Clean Annotations (Optional but Recommended)\n",
        "\n",
        "This step removes entries for non-existent images from annotation files. \n",
        "**‚ö†Ô∏è WARNING: This can take 1-3 hours on mounted GCS data due to network latency.**\n",
        "\n",
        "**Options:**\n",
        "- Clean all annotation files (default)\n",
        "- Clean only a specific split (e.g., split 2) - much faster!\n",
        "  - Set `CLEAN_SPLIT = 2` in the code cell to clean only split 2 files\n",
        "\n",
        "You can skip this if you prefer to use `--skip_missing_at_runtime` during training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean annotation files by removing entries for non-existent images\n",
        "# This will permanently modify the annotation JSON files in the annotations/ folder\n",
        "# Backups are created automatically as *.json.backup\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "PROJECT_ROOT = \"/content/category-agnostic-pose-estimation\"\n",
        "CLEAN_SCRIPT = os.path.join(PROJECT_ROOT, \"clean_annotations.py\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"CLEANING ANNOTATION FILES\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\n‚ö†Ô∏è  WARNING: This process can take 1-3 hours on mounted GCS data.\")\n",
        "print(\"   The script needs to:\")\n",
        "print(\"   1. Scan all image files in the data directory (slow on GCS mount)\")\n",
        "print(\"   2. Check each annotation entry against existing files\")\n",
        "print(\"\\n   Progress will be shown below. You can monitor the output.\")\n",
        "print(\"   The script creates backups (*.json.backup) before modifying files.\")\n",
        "print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
        "\n",
        "# Verify data symlink exists\n",
        "DATA_SYMLINK = os.path.join(PROJECT_ROOT, \"data\")\n",
        "if not os.path.exists(DATA_SYMLINK):\n",
        "    print(f\"‚ùå Data symlink not found: {DATA_SYMLINK}\")\n",
        "    print(\"   Please run the 'Create Data Symlink' cell above first.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Verify clean_annotations.py exists\n",
        "if not os.path.exists(CLEAN_SCRIPT):\n",
        "    print(f\"‚ùå clean_annotations.py not found: {CLEAN_SCRIPT}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Check if annotations directory exists\n",
        "ANNOTATIONS_DIR = os.path.join(PROJECT_ROOT, \"annotations\")\n",
        "if not os.path.exists(ANNOTATIONS_DIR):\n",
        "    print(f\"‚ùå Annotations directory not found: {ANNOTATIONS_DIR}\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Count annotation files\n",
        "annotation_files = list(Path(ANNOTATIONS_DIR).glob(\"*.json\"))\n",
        "annotation_files = [f for f in annotation_files if not f.name.endswith(\".backup\")]\n",
        "print(f\"Found {len(annotation_files)} annotation file(s) in total:\")\n",
        "for f in annotation_files:\n",
        "    print(f\"  - {f.name}\")\n",
        "\n",
        "# Option to clean specific split (set to None to clean all files)\n",
        "# Examples: CLEAN_SPLIT = 2  (cleans only split 2 files)\n",
        "#           CLEAN_SPLIT = None  (cleans all files - default)\n",
        "CLEAN_SPLIT = None  # Change this to clean only a specific split (e.g., 2 for split2)\n",
        "\n",
        "# Build command arguments\n",
        "clean_args = [CLEAN_SCRIPT]\n",
        "if CLEAN_SPLIT is not None:\n",
        "    clean_args.extend([\"--split\", str(CLEAN_SPLIT)])\n",
        "    print(f\"\\n‚ö†Ô∏è  Will clean only split {CLEAN_SPLIT} files\")\n",
        "    print(\"   (Change CLEAN_SPLIT variable above to clean a different split or set to None for all)\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è  Will clean ALL annotation files\")\n",
        "    print(\"   (Set CLEAN_SPLIT variable above to clean only a specific split)\")\n",
        "\n",
        "# Ask for confirmation (optional - you can comment this out to auto-run)\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Starting annotation cleanup...\")\n",
        "print(\"=\" * 80 + \"\\n\")\n",
        "\n",
        "# Record start time\n",
        "start_time = time.time()\n",
        "\n",
        "# Run the cleanup script\n",
        "try:\n",
        "    import subprocess\n",
        "    \n",
        "    # Change to project directory and run the script\n",
        "    print(f\"Running: python {' '.join(clean_args)}\\n\")\n",
        "    process = subprocess.Popen(\n",
        "        [\"python\"] + clean_args,\n",
        "        cwd=PROJECT_ROOT,\n",
        "        stdout=subprocess.PIPE,\n",
        "        stderr=subprocess.STDOUT,\n",
        "        universal_newlines=True,\n",
        "        bufsize=1\n",
        "    )\n",
        "    \n",
        "    # Print output in real-time\n",
        "    for line in process.stdout:\n",
        "        print(line, end='')\n",
        "    \n",
        "    # Wait for process to complete\n",
        "    process.wait()\n",
        "    \n",
        "    if process.returncode != 0:\n",
        "        raise subprocess.CalledProcessError(process.returncode, CLEAN_SCRIPT)\n",
        "    \n",
        "    # Calculate elapsed time\n",
        "    elapsed_time = time.time() - start_time\n",
        "    hours = int(elapsed_time // 3600)\n",
        "    minutes = int((elapsed_time % 3600) // 60)\n",
        "    seconds = int(elapsed_time % 60)\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"‚úÖ Annotation cleanup completed!\")\n",
        "    print(f\"   Time taken: {hours}h {minutes}m {seconds}s\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"\\nNext steps:\")\n",
        "    print(\"  1. Check the cleanup report: annotation_cleanup_report.txt\")\n",
        "    print(\"  2. Backups are saved as *.json.backup in the annotations/ folder\")\n",
        "    print(\"  3. You can now run training without --skip_missing_at_runtime\")\n",
        "    print(\"     (or keep using it if you prefer)\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Error running clean_annotations.py: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    print(\"\\nYou can continue with training using --skip_missing_at_runtime instead.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 7. Run Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Run training using START_TRAINING.sh with \"tiny\" mode\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run START_TRAINING.sh with \"tiny\" mode\n",
        "import os\n",
        "\n",
        "PROJECT_ROOT = \"/content/category-agnostic-pose-estimation\"\n",
        "TRAINING_SCRIPT = os.path.join(PROJECT_ROOT, \"START_TRAINING.sh\")\n",
        "\n",
        "# Make script executable\n",
        "!chmod +x {TRAINING_SCRIPT}\n",
        "\n",
        "# Change to project directory and run training\n",
        "print(\"Starting training with 'tiny' mode...\")\n",
        "print(\"This will run 5 epochs with batch_size 8 (~30-60 min)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "!cd {PROJECT_ROOT} && bash {TRAINING_SCRIPT} tiny\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 8. Monitor Training (Optional)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Evaluate Model with PCK Metrics\n",
        "\n",
        "Evaluate a trained model checkpoint and compute PCK (Percentage of Correct Keypoints) metrics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate a trained model checkpoint and compute PCK metrics\n",
        "import os\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from torch.utils.data import DataLoader\n",
        "import sys\n",
        "\n",
        "# Add project to path\n",
        "PROJECT_ROOT = \"/content/category-agnostic-pose-estimation\"\n",
        "sys.path.insert(0, PROJECT_ROOT)\n",
        "\n",
        "from datasets import build_dataset\n",
        "from datasets.discrete_tokenizer import DiscreteTokenizerV2\n",
        "from engine import evaluate\n",
        "from models import build_model\n",
        "import util.misc as utils\n",
        "import argparse\n",
        "\n",
        "# Import the collate function from train_mp100_cape\n",
        "def trivial_batch_collator(batch):\n",
        "    \"\"\"Split batch records into batched_inputs and batched_extras\"\"\"\n",
        "    batch = [record for record in batch if record is not None]\n",
        "    if not batch:\n",
        "        return [], {}\n",
        "    batched_inputs = []\n",
        "    batched_extras_keys = None\n",
        "    batched_extras = {}\n",
        "    for record in batch:\n",
        "        input_dict = {\n",
        "            'image': record['image'],\n",
        "            'image_id': record['image_id'],\n",
        "            'height': record['height'],\n",
        "            'width': record['width'],\n",
        "            'instances': None,\n",
        "        }\n",
        "        batched_inputs.append(input_dict)\n",
        "        if 'seq_data' in record:\n",
        "            if batched_extras_keys is None:\n",
        "                batched_extras_keys = list(record['seq_data'].keys())\n",
        "                for key in batched_extras_keys:\n",
        "                    batched_extras[key] = []\n",
        "            for key in batched_extras_keys:\n",
        "                batched_extras[key].append(record['seq_data'][key])\n",
        "    for key in batched_extras:\n",
        "        batched_extras[key] = torch.stack(batched_extras[key])\n",
        "    return batched_inputs, batched_extras\n",
        "\n",
        "# Configuration\n",
        "CHECKPOINT_PATH = os.path.join(PROJECT_ROOT, \"output\", \"tiny_test\", \"tiny_test\", \"checkpoint.pth\")\n",
        "# Alternative: use a specific epoch checkpoint\n",
        "# CHECKPOINT_PATH = os.path.join(PROJECT_ROOT, \"output\", \"tiny_test\", \"tiny_test\", \"checkpoint0004.pth\")\n",
        "\n",
        "SPLIT = 2  # Which split to evaluate on (should match training split)\n",
        "IMAGE_SIZE = 256\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"EVALUATING MODEL WITH PCK METRICS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Check if checkpoint exists\n",
        "if not os.path.exists(CHECKPOINT_PATH):\n",
        "    print(f\"‚ùå Checkpoint not found: {CHECKPOINT_PATH}\")\n",
        "    print(\"\\nAvailable checkpoints:\")\n",
        "    output_dir = os.path.join(PROJECT_ROOT, \"output\", \"tiny_test\", \"tiny_test\")\n",
        "    if os.path.exists(output_dir):\n",
        "        checkpoints = list(Path(output_dir).glob(\"checkpoint*.pth\"))\n",
        "        for cp in sorted(checkpoints):\n",
        "            print(f\"  - {cp}\")\n",
        "    else:\n",
        "        print(f\"  Output directory not found: {output_dir}\")\n",
        "else:\n",
        "    print(f\"‚úÖ Loading checkpoint: {CHECKPOINT_PATH}\")\n",
        "    \n",
        "    # Get device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "    \n",
        "    # Create args namespace (minimal args needed for evaluation)\n",
        "    args = argparse.Namespace(\n",
        "        dataset_root=PROJECT_ROOT,\n",
        "        dataset_name='mp100',\n",
        "        mp100_split=SPLIT,\n",
        "        semantic_classes=49,\n",
        "        image_norm=True,\n",
        "        vocab_size=2000,\n",
        "        seq_len=200,\n",
        "        num_queries=200,\n",
        "        num_polys=1,\n",
        "        num_feature_levels=4,\n",
        "        aux_loss=True,\n",
        "        with_poly_refine=False,\n",
        "        masked_attn=False,\n",
        "        cls_loss_coef=2.0,\n",
        "        coords_loss_coef=5.0,\n",
        "        room_cls_loss_coef=0.5,\n",
        "        raster_loss_coef=0.0,\n",
        "        label_smoothing=0.0,\n",
        "        per_token_sem_loss=False,\n",
        "        dec_layer_type='v1',\n",
        "        dec_attn_concat_src=False,\n",
        "        dec_qkv_proj=True,\n",
        "        pre_decoder_pos_embed=False,\n",
        "        learnable_dec_pe=False,\n",
        "        add_cls_token=False,\n",
        "        backbone='resnet50',\n",
        "        dilation=False,\n",
        "        position_embedding='sine',\n",
        "        position_embedding_scale=6.283185307179586,\n",
        "        enc_layers=6,\n",
        "        dec_layers=6,\n",
        "        dim_feedforward=1024,\n",
        "        hidden_dim=256,\n",
        "        dropout=0.1,\n",
        "        nheads=8,\n",
        "        use_anchor=False,\n",
        "        image_size=IMAGE_SIZE,\n",
        "        device=str(device),\n",
        "        skip_missing_at_runtime=True,\n",
        "    )\n",
        "    \n",
        "    # Build tokenizer\n",
        "    import math\n",
        "    num_bins = int(math.sqrt(args.vocab_size))\n",
        "    print(f\"\\nBuilding tokenizer (vocab_size={args.vocab_size}, num_bins={num_bins}, seq_len={args.seq_len})...\")\n",
        "    tokenizer = DiscreteTokenizerV2(num_bins=num_bins, seq_len=args.seq_len, add_cls=False)\n",
        "    \n",
        "    # Build model\n",
        "    print(\"\\nBuilding model...\")\n",
        "    model, criterion = build_model(args, train=False, tokenizer=tokenizer)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    \n",
        "    # Load checkpoint\n",
        "    print(f\"\\nLoading checkpoint: {CHECKPOINT_PATH}\")\n",
        "    checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model'], strict=False)\n",
        "    if 'epoch' in checkpoint:\n",
        "        print(f\"Checkpoint epoch: {checkpoint['epoch']}\")\n",
        "    \n",
        "    # Build validation dataset\n",
        "    print(f\"\\nBuilding validation dataset (split {SPLIT})...\")\n",
        "    dataset_val = build_dataset(image_set='val', args=args)\n",
        "    print(f\"Val dataset: {len(dataset_val)} samples\")\n",
        "    \n",
        "    # Build dataloader\n",
        "    sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
        "    data_loader_val = DataLoader(\n",
        "        dataset_val, \n",
        "        BATCH_SIZE, \n",
        "        sampler=sampler_val,\n",
        "        drop_last=False, \n",
        "        collate_fn=trivial_batch_collator,\n",
        "        num_workers=2,\n",
        "        pin_memory=(device.type == 'cuda')\n",
        "    )\n",
        "    \n",
        "    # Run evaluation\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"Running evaluation...\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    test_stats = evaluate(\n",
        "        model, \n",
        "        criterion, \n",
        "        args.dataset_name, \n",
        "        data_loader_val, \n",
        "        device, \n",
        "        poly2seq=True\n",
        "    )\n",
        "    \n",
        "    # Display results\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"EVALUATION RESULTS\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"  Validation Loss:     {test_stats.get('loss', 0):.4f}\")\n",
        "    print(f\"    - Class Loss:      {test_stats.get('loss_ce', 0):.4f}\")\n",
        "    print(f\"    - Coords Loss:     {test_stats.get('loss_coords', 0):.4f}\")\n",
        "    \n",
        "    # Display PCK metrics\n",
        "    print(\"\\n  PCK Metrics (Percentage of Correct Keypoints):\")\n",
        "    pck_005 = test_stats.get('pck_0.05', None)\n",
        "    pck_01 = test_stats.get('pck_0.1', None)\n",
        "    pck_02 = test_stats.get('pck_0.2', None)\n",
        "    mean_pck = test_stats.get('mean_pck', None)\n",
        "    \n",
        "    if pck_005 is not None:\n",
        "        print(f\"    - PCK@0.05:        {pck_005*100:.2f}%\")\n",
        "    if pck_01 is not None:\n",
        "        print(f\"    - PCK@0.1:         {pck_01*100:.2f}%\")\n",
        "    if pck_02 is not None:\n",
        "        print(f\"    - PCK@0.2:         {pck_02*100:.2f}%\")\n",
        "    if mean_pck is not None:\n",
        "        print(f\"    - Mean PCK:         {mean_pck*100:.2f}%\")\n",
        "    \n",
        "    if all(v is None for v in [pck_005, pck_01, pck_02, mean_pck]):\n",
        "        print(\"    ‚ö†Ô∏è  PCK metrics not computed (may need to check evaluation code)\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"‚úÖ Evaluation complete!\")\n",
        "    print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check training logs\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "PROJECT_ROOT = \"/content/category-agnostic-pose-estimation\"\n",
        "OUTPUT_DIR = os.path.join(PROJECT_ROOT, \"output\", \"tiny_test\", \"tiny_test\")\n",
        "LOG_FILE = os.path.join(OUTPUT_DIR, \"log.txt\")\n",
        "\n",
        "if os.path.exists(LOG_FILE):\n",
        "    print(f\"Reading log file: {LOG_FILE}\")\n",
        "    with open(LOG_FILE, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        print(f\"Total log entries: {len(lines)}\")\n",
        "        if lines:\n",
        "            print(\"\\nLast 3 entries:\")\n",
        "            for line in lines[-3:]:\n",
        "                try:\n",
        "                    stats = json.loads(line.strip())\n",
        "                    print(f\"  Epoch {stats.get('epoch', 'N/A')}: \")\n",
        "                    print(f\"    Train Loss: {stats.get('train_loss', stats.get('loss', 'N/A'))}\")\n",
        "                    print(f\"    Val Loss: {stats.get('test_loss', 'N/A')}\")\n",
        "                except:\n",
        "                    pass\n",
        "else:\n",
        "    print(f\"Log file not found: {LOG_FILE}\")\n",
        "    print(\"\\nAvailable output directories:\")\n",
        "    output_base = os.path.join(PROJECT_ROOT, \"output\")\n",
        "    if os.path.exists(output_base):\n",
        "        for d in os.listdir(output_base):\n",
        "            print(f\"  - {os.path.join(output_base, d)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 9. Download Results (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download checkpoints and logs\n",
        "from google.colab import files\n",
        "from pathlib import Path\n",
        "import zipfile\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "PROJECT_ROOT = \"/content/category-agnostic-pose-estimation\"\n",
        "OUTPUT_BASE = os.path.join(PROJECT_ROOT, \"output\")\n",
        "\n",
        "# Find all checkpoints\n",
        "checkpoints = list(Path(OUTPUT_BASE).rglob(\"checkpoint*.pth\"))\n",
        "\n",
        "if checkpoints:\n",
        "    print(f\"Found {len(checkpoints)} checkpoint(s):\")\n",
        "    for cp in sorted(checkpoints):\n",
        "        size_mb = cp.stat().st_size / (1024 * 1024)\n",
        "        print(f\"  - {cp.name} ({size_mb:.2f} MB)\")\n",
        "    \n",
        "    # Create downloads directory in project root (better organization)\n",
        "    downloads_dir = os.path.join(PROJECT_ROOT, \"downloads\")\n",
        "    os.makedirs(downloads_dir, exist_ok=True)\n",
        "    \n",
        "    # Create zip with timestamp for easy identification\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    zip_filename = f\"checkpoints_{timestamp}.zip\"\n",
        "    zip_path = os.path.join(downloads_dir, zip_filename)\n",
        "    \n",
        "    print(f\"\\nCreating zip file: {zip_path}\")\n",
        "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        for cp in checkpoints:\n",
        "            # Preserve relative path structure\n",
        "            rel_path = os.path.relpath(cp, PROJECT_ROOT)\n",
        "            zipf.write(cp, rel_path)\n",
        "            print(f\"  Added: {rel_path}\")\n",
        "    \n",
        "    zip_size_mb = os.path.getsize(zip_path) / (1024 * 1024)\n",
        "    print(f\"\\n‚úÖ Zip file created: {zip_size_mb:.2f} MB\")\n",
        "    print(f\"Downloading {zip_filename}...\")\n",
        "    files.download(zip_path)\n",
        "    print(\"‚úÖ Download complete!\")\n",
        "    print(f\"\\nüí° Tip: Zip file also saved at: {zip_path}\")\n",
        "    print(\"  (You can download it again later if needed)\")\n",
        "else:\n",
        "    print(\"No checkpoints found yet.\")\n",
        "    print(f\"Output directory: {OUTPUT_BASE}\")\n",
        "    print(\"\\nüí° Checkpoints are saved during training. Run training first.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
