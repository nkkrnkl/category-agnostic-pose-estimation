# Default configuration for Raster2Seq CAPE training

# Data paths
data:
  image_root: 'data'
  annotation_train: 'data/cleaned_annotations/mp100_split1_train.json'
  annotation_val: 'data/cleaned_annotations/mp100_split1_val.json'
  annotation_test: 'data/cleaned_annotations/mp100_split1_test.json'
  image_size: 512

# Model architecture
model:
  hidden_dim: 256
  num_encoder_layers: 3
  num_decoder_layers: 6
  num_heads: 8
  dropout: 0.1
  max_keypoints: 100
  pretrained_resnet: true

# Training hyperparameters
training:
  batch_size: 64  # Increased from 4 for better GPU utilization
  num_epochs: 300
  num_epochs_preliminary: 5  # For quick testing
  num_episodes_per_epoch: 10000
  learning_rate: 0.0001
  weight_decay: 0.0001
  coord_loss_weight: 5.0

  # Data augmentation
  augment: true
  random_flip: true
  color_jitter: true

  # Optimizer
  optimizer: 'adamw'
  lr_scheduler: 'cosine'
  warmup_epochs: 10

  # Checkpointing
  save_interval: 10  # Save every N epochs
  eval_interval: 5   # Evaluate every N epochs

  # Logging
  log_interval: 100  # Log every N steps

# Evaluation
evaluation:
  pck_threshold: 0.2
  batch_size: 1

# Inference modes
inference:
  mode: 'oneshot'  # 'oneshot' or 'zeroshot'
  max_generation_steps: 500

# Paths
paths:
  checkpoint_dir: 'checkpoints'
  log_dir: 'logs'
  output_dir: 'outputs'

# Device
device: 'cuda'  # 'cuda' or 'cpu'
num_workers: 4

# Performance optimizations
compile_model: true  # Set to false to disable torch.compile if it's not helping

# Experiment tracking
experiment:
  name: 'raster2seq_cape_split1'
  seed: 42
